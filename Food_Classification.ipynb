{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Food Classification Using Deep Neural Networks and Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Splitting "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into training, validation, and test sets. Justify your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import shutil\n",
    "import os\n",
    "import random\n",
    "random.seed = 2019\n",
    "split = [600,800,1000]\n",
    "\n",
    "for food in ['falafel','apple_pie','donuts','french_fries','macarons','nachos','onion_rings','oysters','pizza', 'mussels','chicken_wings','chocolate_cake','fried_rice','greek_salad','hamburger','lasagna','poutine','spring_rolls','waffles','spaghetti_carbonara']:\n",
    "    \n",
    "    #create new empty folders\n",
    "    if not os.path.exists('train_more_food/' + food):\n",
    "        os.makedirs('train_more_food/' + food)\n",
    "    if not os.path.exists('val_more_food/' + food):\n",
    "        os.makedirs('val_more_food/' + food)\n",
    "    if not os.path.exists('test_more_food/' + food):\n",
    "        os.makedirs('test_more_food/' + food)\n",
    "    \n",
    "    #make a list of image file names, shuffle them\n",
    "    pictures = os.listdir(\"More Food/\"+ food)\n",
    "    pictures.sort()\n",
    "    random.shuffle(pictures)\n",
    "    \n",
    "    #copy images into folders\n",
    "    for pic in pictures[0:600]:\n",
    "        shutil.copy(\"More Food/\"+ food +\"/\"+ pic,\"train_more_food/\"+food+\"/\"+pic)\n",
    "        print(\"copy\",\"More Food/\"+ food +\"/\"+ pic,\"to\",\"train_more_food/\"+food+\"/\"+pic)\n",
    "        \n",
    "    for pic in pictures[600:800]:\n",
    "        shutil.copy(\"More Food/\"+ food +\"/\"+ pic,\"val_more_food/\"+food+\"/\"+pic)\n",
    "        print(\"copy\",\"More Food/\"+ food +\"/\"+ pic,\"to\",\"val_more_food/\"+food+\"/\"+pic)\n",
    "        \n",
    "    for pic in pictures[800:1000]:\n",
    "        shutil.copy(\"More Food/\"+ food +\"/\"+ pic,\"test_more_food/\"+food+\"/\"+pic)\n",
    "        print(\"copy\",\"More Food/\"+ food +\"/\"+ pic,\"to\",\"test_more_food/\"+food+\"/\"+pic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_loader(target_classes, batch_size):\n",
    "    \"\"\" Returns the indices for datapoints in the dataset that\n",
    "    belongs to the desired target classes, a subset of all possible classes.\n",
    "\n",
    "    Args:\n",
    "        dataset: Dataset object\n",
    "        classes: A list of strings denoting the name of each class\n",
    "        target_classes: A list of strings denoting the name of the desired\n",
    "                        classes. Should be a subset of the argument 'classes'\n",
    "    Returns:\n",
    "        indices: list of indices that have labels corresponding to one of the\n",
    "                 target classes\n",
    "    \"\"\"\n",
    "    classes = ('falafel','apple_pie','donuts','french_fries','macarons','nachos','onion_rings','oysters','pizza', 'mussels')\n",
    "\n",
    "\n",
    "    transform = transforms.Compose(\n",
    "    [transforms.Resize((224,224), interpolation=2), transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "\n",
    "    trainset = torchvision.datasets.ImageFolder(root = 'train',\n",
    "                                                 transform=transform)\n",
    " \n",
    "    train_loader = torch.utils.data.DataLoader(trainset, batch_size = batch_size,\n",
    "                                              num_workers = 1, shuffle = True)\n",
    "\n",
    "    \n",
    "    val_set = torchvision.datasets.ImageFolder(root='val',\n",
    "                                                 transform=transform)\n",
    "    val_loader = torch.utils.data.DataLoader(val_set, batch_size=batch_size,\n",
    "                                              num_workers=1, shuffle = True)\n",
    "\n",
    "    testset = torchvision.datasets.ImageFolder(root= 'test',\n",
    "                                            transform=transform)\n",
    "    \n",
    "    test_loader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                             num_workers=1, shuffle = True)\n",
    "\n",
    "    return train_loader, classes#, val_loader, test_loader, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ('falafel','apple_pie','donuts','french_fries','macarons','nachos','onion_rings','oysters','pizza', 'mussels')\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.Resize((224,224), interpolation=2), transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "\n",
    "trainset = torchvision.datasets.ImageFolder(root = 'train',\n",
    "                                             transform=transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(trainset, batch_size = 64,\n",
    "                                          num_workers = 1, shuffle = True)\n",
    "val_set = torchvision.datasets.ImageFolder(root='val',\n",
    "                                                 transform=transform)\n",
    "val_loader = torch.utils.data.DataLoader(val_set, batch_size=64,\n",
    "                                              num_workers=1, shuffle = True)\n",
    "\n",
    "testset = torchvision.datasets.ImageFolder(root= 'test',\n",
    "                                            transform=transform)\n",
    "    \n",
    "test_loader = torch.utils.data.DataLoader(testset, batch_size=64,\n",
    "                                             num_workers=1, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000\n"
     ]
    }
   ],
   "source": [
    "print(len(trainset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_train_data = []\n",
    "\n",
    "my_transform = transforms.Compose([\n",
    "    transforms.RandomRotation(25),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "for i in range(2):\n",
    "    mnist_new = torchvision.datasets.ImageFolder(root = 'train',\n",
    "                                             transform=my_transform)\n",
    "    for j, item in enumerate(mnist_new):\n",
    "        augmented_train_data.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000\n"
     ]
    }
   ],
   "source": [
    "print(len(mnist_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(trainset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len((trainset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'apple_pie': 0,\n",
       " 'donuts': 1,\n",
       " 'falafel': 2,\n",
       " 'french_fries': 3,\n",
       " 'macarons': 4,\n",
       " 'mussels': 5,\n",
       " 'nachos': 6,\n",
       " 'onion_rings': 7,\n",
       " 'oysters': 8,\n",
       " 'pizza': 9}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset.class_to_idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3, 224, 224])\n",
      "tensor([0, 1, 1, 6, 8, 6, 4, 5, 7, 2, 0, 1, 0, 9, 6, 7, 1, 3, 1, 0, 8, 7, 6, 9,\n",
      "        3, 9, 3, 9, 0, 5, 3, 0, 8, 1, 1, 3, 4, 1, 4, 0, 1, 7, 0, 2, 6, 6, 7, 2,\n",
      "        9, 7, 5, 7, 6, 0, 7, 4, 8, 3, 3, 9, 8, 7, 8, 0])\n",
      "torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "train_loader_iter = iter(train_loader)\n",
    "imgs, labels = next(train_loader_iter)\n",
    "print(imgs.shape)\n",
    "print(labels)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Convolutional Network - Baseline Model \n",
    "Build a convolutional neural network model that takes the (224x224 RGB) image as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    " \n",
    "class LargeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LargeNet, self).__init__()\n",
    "        self.name = \"large\"\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3, padding = 1) \n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding = 1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3, padding = 1)\n",
    "        \n",
    "        self.fc1 = nn.Linear(128 * 28 * 28, 128)\n",
    "        self.fc2 = nn.Linear(128, 9)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x = x.view(-1, 128 * 28 * 28)  \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x) \n",
    "        x = x.squeeze(1) # Flatten to [batch_size]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Training \n",
    "Train your network. Plot the training curve.\n",
    "\n",
    "Make sure that you are checkpointing frequently!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# convolutional neural network, \n",
    "class LargeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LargeNet, self).__init__()\n",
    "        self.name = \"large\"\n",
    "        self.conv1 = nn.Conv2d(3, 5, 3, padding = 1) \n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(5, 10, 3, padding = 1)\n",
    "        self.conv3 = nn.Conv2d(10, 15, 3, padding = 1)\n",
    "        self.fc1 = nn.Linear(15 * 28 * 28 , 64)\n",
    "        self.fc2 = nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        \n",
    "        x = x.view(-1, 15 * 28 * 28)  \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x) \n",
    "        x = x.squeeze(1) # Flatten to [batch_size]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_name(name, batch_size, learning_rate, epoch):\n",
    "    \"\"\" Generate a name for the model consisting of all the hyperparameter values\n",
    "\n",
    "    Args:\n",
    "        config: Configuration object containing the hyperparameters\n",
    "    Returns:\n",
    "        path: A string with the hyperparameter name and value concatenated\n",
    "    \"\"\"\n",
    "    path = \"model_{0}_bs{1}_lr{2}_epoch{3}\".format(name,\n",
    "                                                   batch_size,\n",
    "                                                   learning_rate,\n",
    "                                                   epoch)\n",
    "    return path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3, 224, 224])\n",
      "tensor([3, 5, 3, 7, 3, 6, 0, 9, 8, 9, 0, 8, 3, 6, 9, 9, 6, 3, 4, 8, 5, 3, 9, 8,\n",
      "        0, 6, 1, 4, 0, 5, 1, 5, 4, 7, 4, 0, 3, 0, 5, 5, 9, 3, 5, 8, 8, 6, 4, 9,\n",
      "        3, 2, 1, 0, 3, 2, 6, 1, 3, 1, 0, 3, 7, 0, 6, 9])\n",
      "torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "train_loader_iter = iter(train_loader)\n",
    "imgs, labels = next(train_loader_iter)\n",
    "print(imgs.shape)\n",
    "print(labels)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training - Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(net, loader, criterion):\n",
    "    \"\"\" Evaluate the network on the validation set.\n",
    "\n",
    "     Args:\n",
    "         net: PyTorch neural network object\n",
    "         loader: PyTorch data loader for the validation set\n",
    "         criterion: The loss function\n",
    "     Returns:\n",
    "         err: A scalar for the avg classification error over the validation set\n",
    "         loss: A scalar for the average loss function over the validation set\n",
    "     \"\"\"\n",
    "    total_loss = 0.0\n",
    "    total_err = 0.0\n",
    "    total_epoch = 0\n",
    "\n",
    "    for i, data in enumerate(loader, 0):\n",
    "        inputs, labels = data\n",
    "        \n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        pred = outputs.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        total_err += pred.ne(labels.view_as(pred)).sum().item()\n",
    "        total_loss += loss.item()\n",
    "        total_epoch += len(labels)\n",
    "\n",
    "    err = float(total_err) / total_epoch\n",
    "    loss = float(total_loss) / (i + 1)\n",
    "\n",
    "    return err, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, batch_size=1, learning_rate=0.0001, num_epochs=30):\n",
    "    transform = transforms.Compose(\n",
    "        [transforms.Resize((224,224)), transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "    trainset = torchvision.datasets.ImageFolder(root='train',\n",
    "                                                 transform=transform)\n",
    "    train_loader = torch.utils.data.DataLoader(trainset, batch_size=64,\n",
    "                                          num_workers=1, shuffle = True)\n",
    "    \n",
    "    train_loader_iter = iter(train_loader)\n",
    "    imgs, labels = next(train_loader_iter)\n",
    "    \n",
    "    \n",
    "    torch.manual_seed(1000)\n",
    "\n",
    "   \n",
    "  \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "    \n",
    "    train_err = np.zeros(num_epochs)\n",
    "    train_loss = np.zeros(num_epochs)\n",
    "    val_err = np.zeros(num_epochs)\n",
    "    val_loss = np.zeros(num_epochs)\n",
    "    \n",
    "    \n",
    "    iters, losses, train_acc, val_acc = [], [], [], []\n",
    "    start_time = time.time()\n",
    "    # training\n",
    "    n=0\n",
    "    for epoch in range(num_epochs):  # loop over the dataset multiple times \n",
    "      \n",
    "        total_train_loss = 0.0\n",
    "        total_train_err = 0.0\n",
    "\n",
    "        total_epoch = 0\n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "            \n",
    "            inputs, labels = data\n",
    "                      \n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(inputs)\n",
    "           \n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            iters.append(n)\n",
    "           \n",
    "            pred = outputs.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "            total_train_err += pred.ne(labels.view_as(pred)).sum().item()\n",
    "            total_train_loss += loss.item()\n",
    "            total_epoch += len(labels)\n",
    "           \n",
    "            losses.append(float(loss)/batch_size)  \n",
    "            n += 1# compute *average* loss\n",
    "        \n",
    "        train_err[epoch] = float(total_train_err) / total_epoch\n",
    "     \n",
    "        train_loss[epoch] = float(total_train_loss) / (i+1)\n",
    "        \n",
    "        val_err[epoch], val_loss[epoch] = evaluate(net, val_loader, criterion)\n",
    "    \n",
    "        print((\"Epoch {}: Train err: {}, Train loss: {}  |\" +\n",
    "               \"Validation err: {} , Validation loss:{} \").format(\n",
    "                   epoch + 1,\n",
    "            \n",
    "                   train_err[epoch],\n",
    "                   train_loss[epoch],\n",
    "                   val_err[epoch],\n",
    "                   val_loss[epoch]\n",
    "                        ))\n",
    "\n",
    "        # Save the current model (checkpoint) to a file\n",
    "        model_path = get_model_name(net.name, batch_size, learning_rate, epoch)\n",
    "        torch.save(net.state_dict(), model_path)\n",
    "\n",
    "    print('Finished Training')\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(\"Total time elapsed: {:.2f} seconds\".format(elapsed_time))\n",
    "\n",
    "    # Write the train/test loss/err into CSV file for plotting later\n",
    "    epochs = np.arange(1, num_epochs + 1)\n",
    "\n",
    "    np.savetxt(\"{}_train_err.csv\".format(model_path), train_err)\n",
    "    np.savetxt(\"{}_train_loss.csv\".format(model_path), train_loss)\n",
    "    np.savetxt(\"{}_val_err.csv\".format(model_path), val_err)\n",
    "    np.savetxt(\"{}_val_loss.csv\".format(model_path), val_loss)\n",
    "        \n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Curve\n",
    "import matplotlib.pyplot as plt\n",
    "def plot_training_curve(path):\n",
    "    \"\"\" Plots the training curve for a model run, given the csv files\n",
    "    containing the train/validation error/loss.\n",
    "\n",
    "    Args:\n",
    "        path: The base path of the csv files produced during training\n",
    "    \"\"\"\n",
    "\n",
    "    train_err = np.loadtxt(\"{}_train_err.csv\".format(path))\n",
    "    val_err = np.loadtxt(\"{}_val_err.csv\".format(path))\n",
    "    train_loss = np.loadtxt(\"{}_train_loss.csv\".format(path))\n",
    "    val_loss = np.loadtxt(\"{}_val_loss.csv\".format(path))\n",
    "    plt.title(\"Train vs Validation Error\")\n",
    "    plt.plot(range(1,16), train_err, label=\"Train\")\n",
    "    plt.plot(range(1,16), val_err, label=\"Validation\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Error\")\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n",
    "    plt.title(\"Train vs Validation Loss\")\n",
    "    plt.plot(range(1,16), train_loss, label=\"Train\")\n",
    "    plt.plot(range(1,16), val_loss, label=\"Validation\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Seach - Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ne3 = LargeNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train err: 0.8581666666666666, Train loss: 2.292994164405985  |Validation err: 0.8265 , Validation loss:2.2168687283992767 \n",
      "Epoch 2: Train err: 0.8133333333333334, Train loss: 2.1898040238847125  |Validation err: 0.8125 , Validation loss:2.1898394525051117 \n",
      "Epoch 3: Train err: 0.7908333333333334, Train loss: 2.150301992893219  |Validation err: 0.8325 , Validation loss:2.231725499033928 \n",
      "Epoch 4: Train err: 0.7218333333333333, Train loss: 2.024061899235908  |Validation err: 0.7735 , Validation loss:2.1689374707639217 \n",
      "Epoch 5: Train err: 0.6418333333333334, Train loss: 1.8138254690677562  |Validation err: 0.7865 , Validation loss:2.2966490015387535 \n",
      "Epoch 6: Train err: 0.5516666666666666, Train loss: 1.5930405474723655  |Validation err: 0.806 , Validation loss:2.587955057621002 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-4f2dc5f4ef1d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mne3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.01\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m15\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-11-0253fd649bf5>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(net, batch_size, learning_rate, num_epochs)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\aps3601\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    488\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 489\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    490\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-77d54b61f7c3>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\aps3601\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    488\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 489\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    490\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\aps3601\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    318\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    319\u001b[0m         return F.conv2d(input, self.weight, self.bias, self.stride,\n\u001b[1;32m--> 320\u001b[1;33m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[0;32m    321\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    322\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(ne3, batch_size = 64, learning_rate=0.01, num_epochs = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ne3 = LargeNet()\n",
    "train(ne3, batch_size = 64, learning_rate=0.1, num_epochs = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ne3 = LargeNet()\n",
    "train(ne3, batch_size = 32, learning_rate=0.001, num_epochs = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ne3 = LargeNet()\n",
    "train(ne3, batch_size = 32, learning_rate=0.01, num_epochs = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ne3 = LargeNet()\n",
    "train(ne3, batch_size = 32, learning_rate=0.1, num_epochs = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'model_large_bs64_lr0.001_epoch14'\n",
    "plot_training_curve(path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Classification Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.581625, 1.6759631481170654)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = LargeNet()\n",
    "model_path = get_model_name(net.name, batch_size=64, learning_rate=0.001, epoch=5)\n",
    "state = torch.load(model_path)\n",
    "net.load_state_dict(state)\n",
    "\n",
    "evaluate(net, test_loader, nn.CrossEntropyLoss())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-42-3ddd3359581b>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-42-3ddd3359581b>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    The test classification error was about 0.37.\u001b[0m\n\u001b[1;37m           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "The test classification error was about 0.37."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8000"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(testset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(model):\n",
    "    data = testset\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    for i, data in enumerate(test_loader, 0):\n",
    "        inputs, labels = data\n",
    "        outputs = net(inputs)\n",
    "        # We don't need to run F.softmax\n",
    "        pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        correct += pred.eq(labels.view_as(pred)).sum().item()\n",
    "        total += imgs.shape[0]\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = LargeNet()\n",
    "model_path = get_model_name(net.name, batch_size=64, learning_rate=0.001, epoch=5)\n",
    "state = torch.load(model_path)\n",
    "net.load_state_dict(state)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "conv2d(): argument 'input' (position 1) must be Tensor, not int",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-f1638336344e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mget_accuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-33-718f2e7d6d4c>\u001b[0m in \u001b[0;36mget_accuracy\u001b[1;34m(model)\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mcorrect\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mimgs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# We don't need to run F.softmax\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m         \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m# get the index of the max log-probability\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mcorrect\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mpred\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meq\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview_as\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\aps3601\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    488\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 489\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    490\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-27-77d54b61f7c3>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\aps3601\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    488\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 489\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    490\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\aps3601\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    318\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    319\u001b[0m         return F.conv2d(input, self.weight, self.bias, self.stride,\n\u001b[1;32m--> 320\u001b[1;33m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[0;32m    321\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    322\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: conv2d(): argument 'input' (position 1) must be Tensor, not int"
     ]
    }
   ],
   "source": [
    "get_accuracy(net)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Test accuracy was about 62 %."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Transfer Learning [16 pt]\n",
    "For many image classification tasks, it is generally not a good idea to train a very large Deep Neural Network model from scratch due to the enormous compute requirements and lack of sufficient amounts of training data. One of the better option is to try using an existing model that performs a similar task to the one you need to solve. This method of utilizing a pre-trained network for other similar tasks is broadly termed â€œTransfer Learningâ€. In this assignment, we will use Transfer Learning to extract features from the hand gesture images. Then, train a smaller network to use these features as input and classify the hand gestures.\n",
    "\n",
    "As you have learned from the CNN lecture, Convolution layers extract various features from the images which get utilized by the fully connected layers for correct classification. AlexNet architecture played a pivotal role in establishing Deep Neural Nets as a go-to tool for Image classification problems and we will use an imagenet pre-trained AlexNet model to extract features in this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "# Set up a transform that scales and crops an image so it has the dimensions\n",
    "# of the input layer of alexnet\n",
    "scale_crop = transforms.Compose([\n",
    "   transforms.Resize(256),\n",
    "   transforms.CenterCrop(224)\n",
    "])\n",
    "\n",
    "# The normalization that was applied to the data when alexnet was trained\n",
    "normalize = transforms.Normalize(\n",
    "   mean=[0.485, 0.456, 0.406],\n",
    "   std=[0.229, 0.224, 0.225]\n",
    ")\n",
    "\n",
    "# Put scaling, cropping, normalzing and converting from PIL image to pytorch into one package\n",
    "preprocess = transforms.Compose([\n",
    "   scale_crop,\n",
    "   transforms.ToTensor(),\n",
    "   normalize\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = torchvision.datasets.ImageFolder(root = 'train',\n",
    "                                             transform=preprocess)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(trainset, batch_size = 32,\n",
    "                                          num_workers = 1, shuffle = True)\n",
    "val_set = torchvision.datasets.ImageFolder(root='val',\n",
    "                                                 transform=preprocess)\n",
    "val_loader = torch.utils.data.DataLoader(val_set, batch_size=32,\n",
    "                                              num_workers=1, shuffle = True)\n",
    "\n",
    "testset = torchvision.datasets.ImageFolder(root= 'test',\n",
    "                                           transform=transform)\n",
    "    \n",
    "test_loader = torch.utils.data.DataLoader(testset, batch_size=32,\n",
    "                                             num_workers=1, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing relevant Libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.nn import functional as F\n",
    "import copy\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# Feature Extraction using AlexNet pretrained model\n",
    "\n",
    "class AlexNet(nn.Module):\n",
    "    '''\n",
    "    Class that loads AlexNet Feature Model ('Convolution layers') with imagenet trained weights\n",
    "    \n",
    "    input : image tensors with dimension Lx3x224x224\n",
    "    \n",
    "    output : feature tensor with dimension Lx256x6x6\n",
    "    \n",
    "    *L - Batch size\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    def load_weights(self):\n",
    "        an_builtin = torchvision.models.alexnet(pretrained=True) # Loads the pretrained model weights\n",
    "        \n",
    "        features_weight_i = [0, 3, 6, 8, 10]\n",
    "        for i in features_weight_i:\n",
    "            self.features[i].weight = an_builtin.features[i].weight\n",
    "            self.features[i].bias = an_builtin.features[i].bias\n",
    "\n",
    "    def __init__(self):\n",
    "        super(AlexNet, self).__init__()\n",
    "        self.name = \"AlexNet\"\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(256 * 6 * 6, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4096, 10),\n",
    "        )\n",
    "        self.load_weights() # Copies the weights to AlexNetFeatures model layers\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), 256 * 6 * 6)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "netalex = AlexNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train err: 0.3893333333333333, Train loss: 1.1690362076810066  |Validation err: 0.25 , Validation loss:0.7569273803383112 \n",
      "Epoch 2: Train err: 0.18666666666666668, Train loss: 0.5706664022613079  |Validation err: 0.245 , Validation loss:0.7428885139524937 \n",
      "Epoch 3: Train err: 0.11716666666666667, Train loss: 0.3517896554412994  |Validation err: 0.216 , Validation loss:0.6798681542277336 \n",
      "Epoch 4: Train err: 0.07, Train loss: 0.20826976659133078  |Validation err: 0.213 , Validation loss:0.6846165582537651 \n",
      "Epoch 5: Train err: 0.041833333333333333, Train loss: 0.1299964866026285  |Validation err: 0.2445 , Validation loss:0.8536502728238702 \n",
      "Epoch 6: Train err: 0.03383333333333333, Train loss: 0.09675123150202822  |Validation err: 0.2165 , Validation loss:0.8224746491760015 \n",
      "Epoch 7: Train err: 0.028166666666666666, Train loss: 0.09213120129672771  |Validation err: 0.213 , Validation loss:0.8018942242488265 \n",
      "Epoch 8: Train err: 0.024333333333333332, Train loss: 0.06864176876842976  |Validation err: 0.2125 , Validation loss:0.8322078147903085 \n",
      "Epoch 9: Train err: 0.018666666666666668, Train loss: 0.056135455077078115  |Validation err: 0.2105 , Validation loss:0.9495439641177654 \n",
      "Epoch 10: Train err: 0.018666666666666668, Train loss: 0.05672675628769905  |Validation err: 0.2145 , Validation loss:0.9743683924898505 \n",
      "Epoch 11: Train err: 0.017166666666666667, Train loss: 0.05312231284605854  |Validation err: 0.2175 , Validation loss:1.0827493602409959 \n",
      "Epoch 12: Train err: 0.01633333333333333, Train loss: 0.04881046985850689  |Validation err: 0.219 , Validation loss:1.0706124156713486 \n",
      "Epoch 13: Train err: 0.015, Train loss: 0.040426485676397665  |Validation err: 0.2295 , Validation loss:1.0808596089482307 \n",
      "Epoch 14: Train err: 0.018833333333333334, Train loss: 0.05670001538113711  |Validation err: 0.255 , Validation loss:1.222406093031168 \n",
      "Epoch 15: Train err: 0.011666666666666667, Train loss: 0.03741952117056923  |Validation err: 0.199 , Validation loss:0.9428147533908486 \n",
      "Finished Training\n",
      "Total time elapsed: 31872.83 seconds\n"
     ]
    }
   ],
   "source": [
    "train(netalex, batch_size = 32, learning_rate = 0.0001, num_epochs = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train err: 0.8636666666666667, Train loss: 6.696951688604152  |Validation err: 0.8235 , Validation loss:2.198826481425573 \n",
      "Epoch 2: Train err: 0.7846666666666666, Train loss: 2.0924417465291123  |Validation err: 0.7975 , Validation loss:2.510080871127901 \n",
      "Epoch 3: Train err: 0.7626666666666667, Train loss: 2.0622327657456094  |Validation err: 0.7625 , Validation loss:2.4051466385523477 \n",
      "Epoch 4: Train err: 0.751, Train loss: 2.019769406065028  |Validation err: 0.7625 , Validation loss:2.2512343421814935 \n",
      "Epoch 5: Train err: 0.7235, Train loss: 1.9731543495299968  |Validation err: 0.7465 , Validation loss:2.336838175380041 \n",
      "Epoch 6: Train err: 0.6973333333333334, Train loss: 1.9247440936717581  |Validation err: 0.7205 , Validation loss:2.2657625448136103 \n",
      "Epoch 7: Train err: 0.6743333333333333, Train loss: 1.8526354774515679  |Validation err: 0.6925 , Validation loss:2.011690378189087 \n",
      "Epoch 8: Train err: 0.6598333333333334, Train loss: 1.827751729082554  |Validation err: 0.7235 , Validation loss:2.3836690925416493 \n",
      "Epoch 9: Train err: 0.6433333333333333, Train loss: 1.7885482438067173  |Validation err: 0.719 , Validation loss:2.336203590271965 \n",
      "Epoch 10: Train err: 0.6193333333333333, Train loss: 1.743459827088295  |Validation err: 0.704 , Validation loss:2.3837664978844777 \n",
      "Epoch 11: Train err: 0.6101666666666666, Train loss: 1.6957772919472227  |Validation err: 0.7005 , Validation loss:2.4929724647885276 \n",
      "Epoch 12: Train err: 0.5885, Train loss: 1.6535703374984416  |Validation err: 0.645 , Validation loss:2.0353128645155163 \n",
      "Epoch 13: Train err: 0.5641666666666667, Train loss: 1.596399836083676  |Validation err: 0.6535 , Validation loss:2.2003054013327947 \n",
      "Epoch 14: Train err: 0.5616666666666666, Train loss: 1.578344371724636  |Validation err: 0.671 , Validation loss:2.3120113478766546 \n",
      "Epoch 15: Train err: 0.5436666666666666, Train loss: 1.5411840436306405  |Validation err: 0.6545 , Validation loss:2.978955346440512 \n",
      "Finished Training\n",
      "Total time elapsed: 57385.94 seconds\n"
     ]
    }
   ],
   "source": [
    "train(netalex, batch_size = 32, learning_rate = 0.001, num_epochs = 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Accuracy-Alex Net\n",
    "Report the test accuracy of your best model. How does the test accuracy compare to part 4(d)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracyt(model):\n",
    "    data = testset\n",
    "    total = 0\n",
    "    correct =0\n",
    "    for imgs, labels in torch.utils.data.DataLoader(data, batch_size=32):\n",
    "        #features = myfeature_model(imgs)\n",
    "        output = model(imgs) # We don't need to run F.softmax\n",
    "        pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        correct += pred.eq(labels.view_as(pred)).sum().item()\n",
    "        total += imgs.shape[0]\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = AlexNet()\n",
    "model_path = get_model_name(net.name, batch_size=32, learning_rate=0.0001, epoch=3)\n",
    "state = torch.load(model_path)\n",
    "net.load_state_dict(state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.794"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_accuracyt(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_accuracy(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_conv = torchvision.models.resnet18(pretrained = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_ftrs = model_conv.fc.in_features\n",
    "model_conv.fc = nn.Linear(num_ftrs, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1 weight\n",
      "bn1 weight\n",
      "bn1 bias\n",
      "layer1 0.conv1.weight\n",
      "layer1 0.bn1.weight\n",
      "layer1 0.bn1.bias\n",
      "layer1 0.conv2.weight\n",
      "layer1 0.bn2.weight\n",
      "layer1 0.bn2.bias\n",
      "layer1 1.conv1.weight\n",
      "layer1 1.bn1.weight\n",
      "layer1 1.bn1.bias\n",
      "layer1 1.conv2.weight\n",
      "layer1 1.bn2.weight\n",
      "layer1 1.bn2.bias\n",
      "layer2 0.conv1.weight\n",
      "layer2 0.bn1.weight\n",
      "layer2 0.bn1.bias\n",
      "layer2 0.conv2.weight\n",
      "layer2 0.bn2.weight\n",
      "layer2 0.bn2.bias\n",
      "layer2 0.downsample.0.weight\n",
      "layer2 0.downsample.1.weight\n",
      "layer2 0.downsample.1.bias\n",
      "layer2 1.conv1.weight\n",
      "layer2 1.bn1.weight\n",
      "layer2 1.bn1.bias\n",
      "layer2 1.conv2.weight\n",
      "layer2 1.bn2.weight\n",
      "layer2 1.bn2.bias\n",
      "layer3 0.conv1.weight\n",
      "layer3 0.bn1.weight\n",
      "layer3 0.bn1.bias\n",
      "layer3 0.conv2.weight\n",
      "layer3 0.bn2.weight\n",
      "layer3 0.bn2.bias\n",
      "layer3 0.downsample.0.weight\n",
      "layer3 0.downsample.1.weight\n",
      "layer3 0.downsample.1.bias\n",
      "layer3 1.conv1.weight\n",
      "layer3 1.bn1.weight\n",
      "layer3 1.bn1.bias\n",
      "layer3 1.conv2.weight\n",
      "layer3 1.bn2.weight\n",
      "layer3 1.bn2.bias\n",
      "layer4 0.conv1.weight\n",
      "layer4 0.bn1.weight\n",
      "layer4 0.bn1.bias\n",
      "layer4 0.conv2.weight\n",
      "layer4 0.bn2.weight\n",
      "layer4 0.bn2.bias\n",
      "layer4 0.downsample.0.weight\n",
      "layer4 0.downsample.1.weight\n",
      "layer4 0.downsample.1.bias\n",
      "layer4 1.conv1.weight\n",
      "layer4 1.bn1.weight\n",
      "layer4 1.bn1.bias\n",
      "layer4 1.conv2.weight\n",
      "layer4 1.bn2.weight\n",
      "layer4 1.bn2.bias\n",
      "fc weight\n",
      "fc bias\n"
     ]
    }
   ],
   "source": [
    "for name, child in model_conv.named_children():\n",
    "    for name2, params in child.named_parameters():\n",
    "        print(name, name2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AvgPool2d(kernel_size=7, stride=1, padding=0)\n",
       "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
