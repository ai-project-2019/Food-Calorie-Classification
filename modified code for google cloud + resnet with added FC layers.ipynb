{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hV2YkkE9T9g3"
   },
   "source": [
    " # Food Classification Using Deep Neural Networks and Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JNNjPabdT9g6"
   },
   "source": [
    "# 1. Data Splitting "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ksf8gEyBT9g7"
   },
   "source": [
    "Split the data into training, validation, and test sets. Justify your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-LdKamKAT9g9"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "V1UNELltT9hB",
    "outputId": "83284a9d-ed33-4606-c910-47ac0fe37d71"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device cuda\n"
     ]
    }
   ],
   "source": [
    "#move to gpu if possible\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "print(\"using device\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7PGF9YYJT9hF"
   },
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KeHo3BdtT9hG"
   },
   "outputs": [],
   "source": [
    "def get_data_loader(target_classes, batch_size):\n",
    "    \"\"\" Returns the indices for datapoints in the dataset that\n",
    "    belongs to the desired target classes, a subset of all possible classes.\n",
    "\n",
    "    Args:\n",
    "        dataset: Dataset object\n",
    "        classes: A list of strings denoting the name of each class\n",
    "        target_classes: A list of strings denoting the name of the desired\n",
    "                        classes. Should be a subset of the argument 'classes'\n",
    "    Returns:\n",
    "        indices: list of indices that have labels corresponding to one of the\n",
    "                 target classes\n",
    "    \"\"\"\n",
    "    classes = ('falafel','apple_pie','donuts','french_fries','macarons','nachos','onion_rings','oysters','pizza', 'mussels','chicken_wings','chocolate_cake','fried_rice','greek_salad','hamburger','lasagna','poutine','spring_rolls','waffles','spaghetti_carbonara')\n",
    "\n",
    "\n",
    "    from torchvision import transforms\n",
    "\n",
    "    # Set up a transform that scales and crops an image so it has the dimensions\n",
    "    # of the input layer of alexnet\n",
    "    scale_crop = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224)\n",
    "    ])\n",
    "  \n",
    "    # The normalization that was applied to the data when alexnet was trained\n",
    "    normalize = transforms.Normalize(\n",
    "      mean=[0.485, 0.456, 0.406],\n",
    "      std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "\n",
    "    # Put scaling, cropping, normalzing and converting from PIL image to pytorch into one package\n",
    "    preprocess = transforms.Compose([\n",
    "      scale_crop,\n",
    "      transforms.ToTensor(),\n",
    "      normalize\n",
    "    ])\n",
    "    trainset = torchvision.datasets.ImageFolder(root = 'train_more_food',\n",
    "                                                 transform=preprocess)\n",
    " \n",
    "    train_loader = torch.utils.data.DataLoader(trainset, batch_size = batch_size,\n",
    "                                              num_workers = 1, shuffle = True)\n",
    "\n",
    "    \n",
    "    val_set = torchvision.datasets.ImageFolder(root='val_more_food',\n",
    "                                                 transform=preprocess)\n",
    "    val_loader = torch.utils.data.DataLoader(val_set, batch_size=batch_size,\n",
    "                                              num_workers=1, shuffle = True)\n",
    "\n",
    "    testset = torchvision.datasets.ImageFolder(root= 'test_more_food',\n",
    "                                            transform=preprocess)\n",
    "    \n",
    "    test_loader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                             num_workers=1, shuffle = True)\n",
    "\n",
    "    return train_loader, val_loader, test_loader, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T34FJAqKVYc1"
   },
   "outputs": [],
   "source": [
    "    from torchvision import transforms\n",
    "\n",
    "    # Set up a transform that scales and crops an image so it has the dimensions\n",
    "    # of the input layer of alexnet\n",
    "    scale_crop = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224)\n",
    "    ])\n",
    "  \n",
    "    # The normalization that was applied to the data when alexnet was trained\n",
    "    normalize = transforms.Normalize(\n",
    "      mean=[0.485, 0.456, 0.406],\n",
    "      std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "\n",
    "    # Put scaling, cropping, normalzing and converting from PIL image to pytorch into one package\n",
    "    preprocess = transforms.Compose([\n",
    "      scale_crop,\n",
    "      transforms.ToTensor(),\n",
    "      normalize\n",
    "    ])\n",
    "    trainset = torchvision.datasets.ImageFolder(root = 'ig/train_more_food',\n",
    "                                                 transform=preprocess)\n",
    " \n",
    "    train_loader = torch.utils.data.DataLoader(trainset, batch_size = 128,\n",
    "                                              num_workers = 1, shuffle = True)\n",
    "\n",
    "    \n",
    "    val_set = torchvision.datasets.ImageFolder(root='ig/val_more_food',\n",
    "                                                 transform=preprocess)\n",
    "    val_loader = torch.utils.data.DataLoader(val_set, batch_size=128,\n",
    "                                              num_workers=1, shuffle = True)\n",
    "\n",
    "    testset = torchvision.datasets.ImageFolder(root= 'ig/test_more_food',\n",
    "                                            transform=preprocess)\n",
    "    \n",
    "    test_loader = torch.utils.data.DataLoader(testset, batch_size=128,\n",
    "                                             num_workers=1, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 385
    },
    "colab_type": "code",
    "id": "UI7rliTUT9hO",
    "outputId": "e6726e88-78ff-497f-ac19-3fe1060ebcfd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'apple_pie': 0,\n",
       " 'chicken_wings': 1,\n",
       " 'chocolate_cake': 2,\n",
       " 'donuts': 3,\n",
       " 'falafel': 4,\n",
       " 'french_fries': 5,\n",
       " 'fried_rice': 6,\n",
       " 'greek_salad': 7,\n",
       " 'hamburger': 8,\n",
       " 'lasagna': 9,\n",
       " 'macarons': 10,\n",
       " 'mussels': 11,\n",
       " 'nachos': 12,\n",
       " 'onion_rings': 13,\n",
       " 'oysters': 14,\n",
       " 'pizza': 15,\n",
       " 'poutine': 16,\n",
       " 'spaghetti_carbonara': 17,\n",
       " 'spring_rolls': 18,\n",
       " 'waffles': 19}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset.class_to_idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(net, loader, criterion):\n",
    "    \"\"\" Evaluate the network on the validation set.\n",
    "\n",
    "     Args:\n",
    "         net: PyTorch neural network object\n",
    "         loader: PyTorch data loader for the validation set\n",
    "         criterion: The loss function\n",
    "     Returns:\n",
    "         err: A scalar for the avg classification error over the validation set\n",
    "         loss: A scalar for the average loss function over the validation set\n",
    "     \"\"\"\n",
    "    total_loss = 0.0\n",
    "    total_err = 0.0\n",
    "    total_epoch = 0\n",
    "\n",
    "    for i, data in enumerate(loader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        pred = outputs.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        total_err += pred.ne(labels.view_as(pred)).sum().item()\n",
    "        total_loss += loss.item()\n",
    "        total_epoch += len(labels)\n",
    "\n",
    "    err = float(total_err) / total_epoch\n",
    "    loss = float(total_loss) / (i + 1)\n",
    "\n",
    "    return err, loss\n",
    "\n",
    "def get_model_name(name, batch_size, learning_rate, epoch):\n",
    "    \"\"\" Generate a name for the model consisting of all the hyperparameter values\n",
    "\n",
    "    Args:\n",
    "        config: Configuration object containing the hyperparameters\n",
    "    Returns:\n",
    "        path: A string with the hyperparameter name and value concatenated\n",
    "    \"\"\"\n",
    "    path = \"model_{0}_bs{1}_lr{2}_epoch{3}\".format(name,\n",
    "                                                   batch_size,\n",
    "                                                   learning_rate,\n",
    "                                                   epoch)\n",
    "    return path\n",
    "\n",
    "def trainAlexNet(net, batch_size=1, learning_rate=0.0001, num_epochs=30):\n",
    "   \n",
    "    \n",
    "    from torchvision import transforms\n",
    "\n",
    "    # Set up a transform that scales and crops an image so it has the dimensions\n",
    "    # of the input layer of alexnet\n",
    "    scale_crop = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224)\n",
    "    ])\n",
    "  \n",
    "    # The normalization that was applied to the data when alexnet was trained\n",
    "    normalize = transforms.Normalize(\n",
    "      mean=[0.485, 0.456, 0.406],\n",
    "      std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "\n",
    "    # Put scaling, cropping, normalzing and converting from PIL image to pytorch into one package\n",
    "    preprocess = transforms.Compose([\n",
    "      scale_crop,\n",
    "      transforms.ToTensor(),\n",
    "      normalize\n",
    "    ])\n",
    "    #trainset = torchvision.datasets.ImageFolder(augmented_train_data,\n",
    "      #                                           transform=preprocess)\n",
    " \n",
    "    train_loader = torch.utils.data.DataLoader(train_with_aug, batch_size = batch_size,\n",
    "                                              num_workers = 1, shuffle = True)\n",
    "\n",
    "    \n",
    "    val_set = torchvision.datasets.ImageFolder(root='ig/val_more_food',\n",
    "                                                 transform=preprocess)\n",
    "    val_loader = torch.utils.data.DataLoader(val_set, batch_size=batch_size,\n",
    "                                              num_workers=1, shuffle = True)\n",
    "\n",
    "    testset = torchvision.datasets.ImageFolder(root= 'ig/test_more_food',\n",
    "                                            transform=preprocess)\n",
    "    \n",
    "    test_loader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                             num_workers=1, shuffle = True)\n",
    "    \n",
    "    \n",
    "    torch.manual_seed(1000)\n",
    "\n",
    "   \n",
    "  \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "    \n",
    "    train_err = np.zeros(num_epochs)\n",
    "    train_loss = np.zeros(num_epochs)\n",
    "    val_err = np.zeros(num_epochs)\n",
    "    val_loss = np.zeros(num_epochs)\n",
    "    \n",
    "    \n",
    "    iters, losses, train_acc, val_acc = [], [], [], []\n",
    "    start_time = time.time()\n",
    "    # training\n",
    "    n=0\n",
    "    for epoch in range(num_epochs):  # loop over the dataset multiple times \n",
    "      \n",
    "        total_train_loss = 0.0\n",
    "        total_train_err = 0.0\n",
    "\n",
    "        total_epoch = 0\n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "           \n",
    "            inputs, labels = data\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "                      \n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(inputs)\n",
    "           \n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            iters.append(n)\n",
    "           \n",
    "            pred = outputs.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "            total_train_err += pred.ne(labels.view_as(pred)).sum().item()\n",
    "            total_train_loss += loss.item()\n",
    "            total_epoch += len(labels)\n",
    "           \n",
    "            losses.append(float(loss)/batch_size)  \n",
    "            n += 1# compute *average* loss\n",
    "        \n",
    "        train_err[epoch] = float(total_train_err) / total_epoch\n",
    "     \n",
    "        train_loss[epoch] = float(total_train_loss) / (i+1)\n",
    "        \n",
    "        val_err[epoch], val_loss[epoch] = evaluate(net, val_loader, criterion)\n",
    "    \n",
    "        print((\"Epoch {}: Train err: {}, Train loss: {}  |\" +\n",
    "               \"Validation err: {} , Validation loss:{} \").format(\n",
    "                   epoch + 1,\n",
    "            \n",
    "                   train_err[epoch],\n",
    "                   train_loss[epoch],\n",
    "                   val_err[epoch],\n",
    "                   val_loss[epoch]\n",
    "                        ))\n",
    "\n",
    "        # Save the current model (checkpoint) to a file\n",
    "        model_path = get_model_name(\"RESNET\", batch_size, learning_rate, epoch)\n",
    "        torch.save(net.state_dict(), model_path)\n",
    "        \n",
    "\n",
    "    print('Finished Training')\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(\"Total time elapsed: {:.2f} seconds\".format(elapsed_time))\n",
    "\n",
    "    # Write the train/test loss/err into CSV file for plotting later\n",
    "    epochs = np.arange(1, num_epochs + 1)\n",
    "\n",
    "    np.savetxt(\"{}_train_err.csv\".format(model_path), train_err)\n",
    "    np.savetxt(\"{}_train_loss.csv\".format(model_path), train_loss)\n",
    "    np.savetxt(\"{}_val_err.csv\".format(model_path), val_err)\n",
    "    np.savetxt(\"{}_val_loss.csv\".format(model_path), val_loss)\n",
    "    \n",
    "# Importing relevant Libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.nn import functional as F\n",
    "import copy\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# Feature Extraction using AlexNet pretrained model\n",
    "\n",
    "class AlexNetDA(nn.Module):\n",
    "    '''\n",
    "    Class that loads AlexNet Feature Model ('Convolution layers') with imagenet trained weights\n",
    "    \n",
    "    input : image tensors with dimension Lx3x224x224\n",
    "    \n",
    "    output : feature tensor with dimension Lx256x6x6\n",
    "    \n",
    "    *L - Batch size\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    def load_weights(self):\n",
    "        an_builtin = torchvision.models.alexnet(pretrained=True) # Loads the pretrained model weights\n",
    "        \n",
    "        features_weight_i = [0, 3, 6, 8, 10]\n",
    "        for i in features_weight_i:\n",
    "            self.features[i].weight = an_builtin.features[i].weight\n",
    "            self.features[i].bias = an_builtin.features[i].bias\n",
    "\n",
    "    def __init__(self):\n",
    "        super(AlexNetDA, self).__init__()\n",
    "        self.name = \"AlexNetDA\"\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(256 * 6 * 6, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4096, 20),\n",
    "        )\n",
    "        self.load_weights() # Copies the weights to AlexNetFeatures model layers\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), 256 * 6 * 6)\n",
    "        \n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1]\n",
      "Epoch 1: Train err: 0.3785833333333333, Train loss: 1.2762629676659902  |Validation err: 0.27075 , Validation loss:0.911521057844162 \n",
      "Epoch 2: Train err: 0.23333333333333334, Train loss: 0.7891806231737137  |Validation err: 0.266 , Validation loss:0.876939565896988 \n",
      "Epoch 3: Train err: 0.17825, Train loss: 0.6175516118804614  |Validation err: 0.25725 , Validation loss:0.8859039123058319 \n",
      "Epoch 4: Train err: 0.145375, Train loss: 0.5075232909321785  |Validation err: 0.247 , Validation loss:0.8823808536529542 \n",
      "Epoch 5: Train err: 0.12995833333333334, Train loss: 0.4593242875734965  |Validation err: 0.24275 , Validation loss:0.8407653880119323 \n",
      "Epoch 6: Train err: 0.12283333333333334, Train loss: 0.4233424866100152  |Validation err: 0.24 , Validation loss:0.8589568246603012 \n",
      "Epoch 7: Train err: 0.111125, Train loss: 0.3873286395271619  |Validation err: 0.231 , Validation loss:0.8205231423377991 \n",
      "Epoch 8: Train err: 0.10595833333333333, Train loss: 0.37267463718851407  |Validation err: 0.23675 , Validation loss:0.8415715628862381 \n",
      "Epoch 9: Train err: 0.10258333333333333, Train loss: 0.350224021623532  |Validation err: 0.25025 , Validation loss:0.9162530190944672 \n",
      "Epoch 10: Train err: 0.096875, Train loss: 0.33500382669766743  |Validation err: 0.2305 , Validation loss:0.850329534292221 \n",
      "Epoch 11: Train err: 0.096375, Train loss: 0.3281121488809586  |Validation err: 0.22475 , Validation loss:0.8144587199687958 \n",
      "Epoch 12: Train err: 0.09170833333333334, Train loss: 0.30604588728149734  |Validation err: 0.2265 , Validation loss:0.822745183467865 \n",
      "Epoch 13: Train err: 0.08995833333333333, Train loss: 0.30806731458504993  |Validation err: 0.21675 , Validation loss:0.8153034501075744 \n",
      "Epoch 14: Train err: 0.087125, Train loss: 0.29713665596644084  |Validation err: 0.22825 , Validation loss:0.8790553650856018 \n",
      "Epoch 15: Train err: 0.08470833333333333, Train loss: 0.28836332307259244  |Validation err: 0.22175 , Validation loss:0.853460380077362 \n",
      "Epoch 16: Train err: 0.080625, Train loss: 0.26972453513741496  |Validation err: 0.2285 , Validation loss:0.8674105463027955 \n",
      "Epoch 17: Train err: 0.07875, Train loss: 0.2618773516714573  |Validation err: 0.21925 , Validation loss:0.8335937231779098 \n",
      "Epoch 18: Train err: 0.076125, Train loss: 0.25414751523733137  |Validation err: 0.214 , Validation loss:0.8265973905324936 \n",
      "Epoch 19: Train err: 0.07608333333333334, Train loss: 0.25540541353821755  |Validation err: 0.21725 , Validation loss:0.835578681230545 \n",
      "Epoch 20: Train err: 0.07341666666666667, Train loss: 0.2486385410030683  |Validation err: 0.2155 , Validation loss:0.8181992528438569 \n",
      "Finished Training\n",
      "Total time elapsed: 3327.20 seconds\n"
     ]
    }
   ],
   "source": [
    "netalex = AlexNetDA()\n",
    "#netalex.to(device)\n",
    "device_ids = [i for i in range(torch.cuda.device_count())]\n",
    "print(device_ids)\n",
    "model = nn.DataParallel(netalex, [0,1])\n",
    "model = model.to(device)\n",
    "trainAlexNet(model, batch_size = 32, learning_rate = 0.00001, num_epochs = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1]\n",
      "Epoch 1: Train err: 0.9529166666666666, Train loss: 3.0282405437307154  |Validation err: 0.95 , Validation loss:2.9957598969340324 \n",
      "Epoch 2: Train err: 0.9520833333333333, Train loss: 2.9959838529850575  |Validation err: 0.95 , Validation loss:2.9957447201013565 \n",
      "Epoch 3: Train err: 0.9522083333333333, Train loss: 2.9959474954199283  |Validation err: 0.95 , Validation loss:2.9957177191972733 \n",
      "Epoch 4: Train err: 0.95125, Train loss: 2.995912674893724  |Validation err: 0.95 , Validation loss:2.995738409459591 \n",
      "Epoch 5: Train err: 0.9526666666666667, Train loss: 2.9958997825358775  |Validation err: 0.95 , Validation loss:2.995729185640812 \n",
      "Epoch 6: Train err: 0.952875, Train loss: 2.9959137744091926  |Validation err: 0.95 , Validation loss:2.9957431107759476 \n",
      "Epoch 7: Train err: 0.9545, Train loss: 2.9959080650451337  |Validation err: 0.95 , Validation loss:2.9957460686564445 \n",
      "Epoch 8: Train err: 0.9534166666666667, Train loss: 2.9959142956327884  |Validation err: 0.95 , Validation loss:2.9957370162010193 \n",
      "Epoch 9: Train err: 0.9514166666666667, Train loss: 2.995897501072985  |Validation err: 0.95 , Validation loss:2.995716631412506 \n",
      "Epoch 10: Train err: 0.952375, Train loss: 3.117789681921614  |Validation err: 0.95 , Validation loss:2.9957195594906807 \n",
      "Epoch 11: Train err: 0.9541666666666667, Train loss: 2.995880236016943  |Validation err: 0.95 , Validation loss:2.9957253709435463 \n",
      "Epoch 12: Train err: 0.9546666666666667, Train loss: 2.9958998535541776  |Validation err: 0.95 , Validation loss:2.9957297667860985 \n",
      "Epoch 13: Train err: 0.9538333333333333, Train loss: 2.995883917554896  |Validation err: 0.95 , Validation loss:2.995710089802742 \n",
      "Epoch 14: Train err: 0.95075, Train loss: 9.46661350828536  |Validation err: 0.92775 , Validation loss:2.97464257478714 \n",
      "Epoch 15: Train err: 0.8976666666666666, Train loss: 2.863273445596086  |Validation err: 0.86425 , Validation loss:2.7517398223280907 \n",
      "Epoch 16: Train err: 0.8499583333333334, Train loss: 2.7223843551696616  |Validation err: 0.83725 , Validation loss:2.6489566639065742 \n",
      "Epoch 17: Train err: 0.8217916666666667, Train loss: 2.630472239027632  |Validation err: 0.799 , Validation loss:2.5575881749391556 \n",
      "Epoch 18: Train err: 0.7905, Train loss: 2.5388146131596665  |Validation err: 0.75525 , Validation loss:2.4440276697278023 \n",
      "Epoch 19: Train err: 0.7609583333333333, Train loss: 2.441180188605126  |Validation err: 0.742 , Validation loss:2.400281585752964 \n",
      "Epoch 20: Train err: 0.7255833333333334, Train loss: 2.3242090014701193  |Validation err: 0.736 , Validation loss:2.358641490340233 \n",
      "Finished Training\n",
      "Total time elapsed: 3281.66 seconds\n"
     ]
    }
   ],
   "source": [
    "netalex = AlexNetDA()\n",
    "#netalex.to(device)\n",
    "device_ids = [i for i in range(torch.cuda.device_count())]\n",
    "print(device_ids)\n",
    "model = nn.DataParallel(netalex, [0,1])\n",
    "model = model.to(device)\n",
    "trainAlexNet(model, batch_size = 128, learning_rate = 0.001, num_epochs = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1]\n",
      "Epoch 1: Train err: 0.3459583333333333, Train loss: 1.1686042778669519  |Validation err: 0.262 , Validation loss:0.8850515745580196 \n",
      "Epoch 2: Train err: 0.19420833333333334, Train loss: 0.6483240877377227  |Validation err: 0.264 , Validation loss:0.9105774108320475 \n",
      "Epoch 3: Train err: 0.14154166666666668, Train loss: 0.48045425735255504  |Validation err: 0.25025 , Validation loss:0.9179508239030838 \n",
      "Epoch 4: Train err: 0.12366666666666666, Train loss: 0.4107015075043161  |Validation err: 0.23875 , Validation loss:0.899783294647932 \n",
      "Epoch 5: Train err: 0.11058333333333334, Train loss: 0.3735055167307245  |Validation err: 0.2395 , Validation loss:0.8464423958212137 \n",
      "Epoch 6: Train err: 0.105625, Train loss: 0.3566283643087174  |Validation err: 0.23 , Validation loss:0.8648484852164984 \n",
      "Epoch 7: Train err: 0.09529166666666666, Train loss: 0.3241417207457918  |Validation err: 0.21925 , Validation loss:0.8630065470933914 \n",
      "Epoch 8: Train err: 0.09245833333333334, Train loss: 0.3026661897434833  |Validation err: 0.22 , Validation loss:0.8617054084315896 \n",
      "Epoch 9: Train err: 0.08829166666666667, Train loss: 0.29323605551047527  |Validation err: 0.2265 , Validation loss:0.8847997784614563 \n",
      "Epoch 10: Train err: 0.08025, Train loss: 0.27602829506739657  |Validation err: 0.20825 , Validation loss:0.8778632339090109 \n",
      "Epoch 11: Train err: 0.08141666666666666, Train loss: 0.27066299652165554  |Validation err: 0.22325 , Validation loss:0.8908063229173422 \n",
      "Epoch 12: Train err: 0.07833333333333334, Train loss: 0.2597108618059057  |Validation err: 0.21525 , Validation loss:0.872648224234581 \n",
      "Epoch 13: Train err: 0.07495833333333334, Train loss: 0.2538850649081646  |Validation err: 0.22275 , Validation loss:0.9393356032669544 \n",
      "Epoch 14: Train err: 0.07591666666666666, Train loss: 0.25068001480812724  |Validation err: 0.22475 , Validation loss:0.892106469720602 \n",
      "Epoch 15: Train err: 0.0725, Train loss: 0.24348490150209437  |Validation err: 0.217 , Validation loss:0.9210092909634113 \n",
      "Epoch 16: Train err: 0.06970833333333333, Train loss: 0.22990489869992783  |Validation err: 0.225 , Validation loss:0.9422846790403128 \n",
      "Epoch 17: Train err: 0.07025, Train loss: 0.23144503205301278  |Validation err: 0.21425 , Validation loss:0.8840447124093771 \n",
      "Epoch 18: Train err: 0.06716666666666667, Train loss: 0.21685299265416377  |Validation err: 0.22175 , Validation loss:0.9254564587026834 \n",
      "Epoch 19: Train err: 0.06554166666666666, Train loss: 0.22019056317971109  |Validation err: 0.22025 , Validation loss:0.9482823014259338 \n",
      "Epoch 20: Train err: 0.06483333333333334, Train loss: 0.21756212266677238  |Validation err: 0.219 , Validation loss:0.920773446559906 \n",
      "Finished Training\n",
      "Total time elapsed: 3297.08 seconds\n"
     ]
    }
   ],
   "source": [
    "netalex = AlexNetDA()\n",
    "#netalex.to(device)\n",
    "device_ids = [i for i in range(torch.cuda.device_count())]\n",
    "print(device_ids)\n",
    "model = nn.DataParallel(netalex, [0,1])\n",
    "model = model.to(device)\n",
    "trainAlexNet(model, batch_size = 128, learning_rate = 0.0001, num_epochs = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1]\n",
      "Epoch 1: Train err: 0.34579166666666666, Train loss: 1.1604937566121418  |Validation err: 0.2565 , Validation loss:0.8742000753917392 \n",
      "Epoch 2: Train err: 0.18991666666666668, Train loss: 0.6340108564694722  |Validation err: 0.24075 , Validation loss:0.8327392888447595 \n",
      "Epoch 3: Train err: 0.14370833333333333, Train loss: 0.48019088558355966  |Validation err: 0.25725 , Validation loss:0.9308844161412072 \n",
      "Epoch 4: Train err: 0.12458333333333334, Train loss: 0.4142082628806432  |Validation err: 0.251 , Validation loss:0.9071697062916226 \n",
      "Epoch 5: Train err: 0.11254166666666666, Train loss: 0.3800031027793884  |Validation err: 0.227 , Validation loss:0.826458605035903 \n",
      "Epoch 6: Train err: 0.10445833333333333, Train loss: 0.3532179073095322  |Validation err: 0.22975 , Validation loss:0.8759564076151166 \n",
      "Epoch 7: Train err: 0.09841666666666667, Train loss: 0.3292626137733459  |Validation err: 0.2265 , Validation loss:0.8822438172877781 \n",
      "Epoch 8: Train err: 0.09316666666666666, Train loss: 0.3172169351577759  |Validation err: 0.23075 , Validation loss:0.9225967521705325 \n",
      "Epoch 9: Train err: 0.09170833333333334, Train loss: 0.30927836141983667  |Validation err: 0.23875 , Validation loss:0.9542203625989338 \n",
      "Epoch 10: Train err: 0.08495833333333333, Train loss: 0.2910161684155464  |Validation err: 0.222 , Validation loss:0.9153256936678811 \n",
      "Epoch 11: Train err: 0.08345833333333333, Train loss: 0.27869398734966916  |Validation err: 0.22325 , Validation loss:0.8892526442096347 \n",
      "Epoch 12: Train err: 0.08020833333333334, Train loss: 0.2683330999016762  |Validation err: 0.22925 , Validation loss:0.9270260352937002 \n",
      "Epoch 13: Train err: 0.07525, Train loss: 0.25883692502975464  |Validation err: 0.2285 , Validation loss:0.9563346651811448 \n",
      "Epoch 14: Train err: 0.07483333333333334, Train loss: 0.2550550986230373  |Validation err: 0.2385 , Validation loss:1.050756851832072 \n",
      "Epoch 15: Train err: 0.07833333333333334, Train loss: 0.26007755162318547  |Validation err: 0.2295 , Validation loss:0.9665638813896785 \n",
      "Epoch 16: Train err: 0.07345833333333333, Train loss: 0.24457388366262117  |Validation err: 0.2285 , Validation loss:0.9980237465056162 \n",
      "Epoch 17: Train err: 0.07008333333333333, Train loss: 0.2366062280535698  |Validation err: 0.2215 , Validation loss:0.9707204207541451 \n",
      "Epoch 18: Train err: 0.07020833333333333, Train loss: 0.23039325246214867  |Validation err: 0.23725 , Validation loss:1.052653395940387 \n",
      "Epoch 19: Train err: 0.06858333333333333, Train loss: 0.2281274621983369  |Validation err: 0.22475 , Validation loss:0.9962880223516434 \n",
      "Epoch 20: Train err: 0.0685, Train loss: 0.23210998085141182  |Validation err: 0.21925 , Validation loss:0.9770093929199946 \n",
      "Finished Training\n",
      "Total time elapsed: 3276.36 seconds\n"
     ]
    }
   ],
   "source": [
    "netalex = AlexNetDA()\n",
    "#netalex.to(device)\n",
    "device_ids = [i for i in range(torch.cuda.device_count())]\n",
    "print(device_ids)\n",
    "model = nn.DataParallel(netalex, [0,1])\n",
    "model = model.to(device)\n",
    "trainAlexNet(model, batch_size = 64, learning_rate = 0.0001, num_epochs = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1]\n",
      "Epoch 1: Train err: 0.3983333333333333, Train loss: 1.3596372736295064  |Validation err: 0.28725 , Validation loss:0.9695532984203763 \n",
      "Epoch 2: Train err: 0.254875, Train loss: 0.8571073141098022  |Validation err: 0.27175 , Validation loss:0.9154061495311676 \n",
      "Epoch 3: Train err: 0.19854166666666667, Train loss: 0.6813607420921326  |Validation err: 0.27325 , Validation loss:0.9202023233686175 \n",
      "Epoch 4: Train err: 0.16004166666666667, Train loss: 0.5658844091892242  |Validation err: 0.2485 , Validation loss:0.8817730810907152 \n",
      "Epoch 5: Train err: 0.13945833333333332, Train loss: 0.49756006654103596  |Validation err: 0.24875 , Validation loss:0.8621167857495565 \n",
      "Epoch 6: Train err: 0.1295, Train loss: 0.4589031814734141  |Validation err: 0.2465 , Validation loss:0.8545234070883857 \n",
      "Epoch 7: Train err: 0.116625, Train loss: 0.4210182774861654  |Validation err: 0.235 , Validation loss:0.8286143531875004 \n",
      "Epoch 8: Train err: 0.11529166666666667, Train loss: 0.4024809059302012  |Validation err: 0.2355 , Validation loss:0.8318452645861913 \n",
      "Epoch 9: Train err: 0.10895833333333334, Train loss: 0.3794403776327769  |Validation err: 0.253 , Validation loss:0.8691639422424255 \n",
      "Epoch 10: Train err: 0.10504166666666667, Train loss: 0.36478926249345145  |Validation err: 0.2295 , Validation loss:0.8502230086023845 \n",
      "Epoch 11: Train err: 0.102, Train loss: 0.35079467153549193  |Validation err: 0.232 , Validation loss:0.8234854406780667 \n",
      "Epoch 12: Train err: 0.096875, Train loss: 0.33147177801529565  |Validation err: 0.22625 , Validation loss:0.82876346177525 \n",
      "Epoch 13: Train err: 0.095875, Train loss: 0.3326659680604935  |Validation err: 0.2265 , Validation loss:0.8226853542857699 \n",
      "Epoch 14: Train err: 0.0945, Train loss: 0.3239301692843437  |Validation err: 0.22725 , Validation loss:0.8515664897267781 \n",
      "Epoch 15: Train err: 0.09375, Train loss: 0.3176662739912669  |Validation err: 0.2285 , Validation loss:0.8432586954699622 \n",
      "Epoch 16: Train err: 0.08516666666666667, Train loss: 0.29335708830753965  |Validation err: 0.22775 , Validation loss:0.85029246220513 \n",
      "Epoch 17: Train err: 0.08629166666666667, Train loss: 0.28686433200041456  |Validation err: 0.22725 , Validation loss:0.836278343957568 \n",
      "Epoch 18: Train err: 0.08275, Train loss: 0.2792638356486956  |Validation err: 0.21925 , Validation loss:0.8399270441797044 \n",
      "Epoch 19: Train err: 0.08229166666666667, Train loss: 0.2814482977191607  |Validation err: 0.22275 , Validation loss:0.8203783243421524 \n",
      "Epoch 20: Train err: 0.07991666666666666, Train loss: 0.2760302592118581  |Validation err: 0.222 , Validation loss:0.8160380281153179 \n",
      "Finished Training\n",
      "Total time elapsed: 3277.68 seconds\n"
     ]
    }
   ],
   "source": [
    "netalex = AlexNetDA()\n",
    "#netalex.to(device)\n",
    "device_ids = [i for i in range(torch.cuda.device_count())]\n",
    "print(device_ids)\n",
    "model = nn.DataParallel(netalex, [0,1])\n",
    "model = model.to(device)\n",
    "trainAlexNet(model, batch_size = 64, learning_rate = 0.00001, num_epochs = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xAECXnrlT9hV"
   },
   "source": [
    "# 2. Convolutional Network - Baseline Model \n",
    "Build a convolutional neural network model that takes the (224x224 RGB) image as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bungE51hT9hW"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    " \n",
    "class LargeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LargeNet, self).__init__()\n",
    "        self.name = \"large\"\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3, padding = 1) \n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding = 1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3, padding = 1)\n",
    "        \n",
    "        self.fc1 = nn.Linear(128 * 28 * 28, 128)\n",
    "        self.fc2 = nn.Linear(128, 9)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x = x.view(-1, 128 * 28 * 28)  \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x) \n",
    "        x = x.squeeze(1) # Flatten to [batch_size]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IDPxQg20T9ha"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# convolutional neural network, \n",
    "class LargeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LargeNet, self).__init__()\n",
    "        self.name = \"large\"\n",
    "        self.conv1 = nn.Conv2d(3, 5, 3, padding = 1) \n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(5, 10, 3, padding = 1)\n",
    "        self.conv3 = nn.Conv2d(10, 15, 3, padding = 1)\n",
    "        self.fc1 = nn.Linear(15 * 28 * 28 , 64)\n",
    "        self.fc2 = nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        \n",
    "        x = x.view(-1, 15 * 28 * 28)  \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x) \n",
    "        x = x.squeeze(1) # Flatten to [batch_size]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L5sdpmLhT9hZ"
   },
   "source": [
    "# 3. Training \n",
    "Train your network. Plot the training curve.\n",
    "\n",
    "Make sure that you are checkpointing frequently!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mcNH2X3ET9hd"
   },
   "outputs": [],
   "source": [
    "def get_model_name(name, batch_size, learning_rate, epoch):\n",
    "    \"\"\" Generate a name for the model consisting of all the hyperparameter values\n",
    "\n",
    "    Args:\n",
    "        config: Configuration object containing the hyperparameters\n",
    "    Returns:\n",
    "        path: A string with the hyperparameter name and value concatenated\n",
    "    \"\"\"\n",
    "    path = \"model_{0}_bs{1}_lr{2}_epoch{3}\".format(name,\n",
    "                                                   batch_size,\n",
    "                                                   learning_rate,\n",
    "                                                   epoch)\n",
    "    return path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JokNNQZRT9hl"
   },
   "outputs": [],
   "source": [
    "def evaluate(net, loader, criterion):\n",
    "    \"\"\" Evaluate the network on the validation set.\n",
    "\n",
    "     Args:\n",
    "         net: PyTorch neural network object\n",
    "         loader: PyTorch data loader for the validation set\n",
    "         criterion: The loss function\n",
    "     Returns:\n",
    "         err: A scalar for the avg classification error over the validation set\n",
    "         loss: A scalar for the average loss function over the validation set\n",
    "     \"\"\"\n",
    "    total_loss = 0.0\n",
    "    total_err = 0.0\n",
    "    total_epoch = 0\n",
    "\n",
    "    for i, data in enumerate(loader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        pred = outputs.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        total_err += pred.ne(labels.view_as(pred)).sum().item()\n",
    "        total_loss += loss.item()\n",
    "        total_epoch += len(labels)\n",
    "\n",
    "    err = float(total_err) / total_epoch\n",
    "    loss = float(total_loss) / (i + 1)\n",
    "\n",
    "    return err, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ELngXxjuT9hn"
   },
   "outputs": [],
   "source": [
    "def trainResNet(net, batch_size=1, learning_rate=0.0001, num_epochs=30):\n",
    "   \n",
    "    \n",
    "    from torchvision import transforms\n",
    "\n",
    "    # Set up a transform that scales and crops an image so it has the dimensions\n",
    "    # of the input layer of alexnet\n",
    "    scale_crop = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224)\n",
    "    ])\n",
    "  \n",
    "    # The normalization that was applied to the data when alexnet was trained\n",
    "    normalize = transforms.Normalize(\n",
    "      mean=[0.485, 0.456, 0.406],\n",
    "      std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "\n",
    "    # Put scaling, cropping, normalzing and converting from PIL image to pytorch into one package\n",
    "    preprocess = transforms.Compose([\n",
    "      scale_crop,\n",
    "      transforms.ToTensor(),\n",
    "      normalize\n",
    "    ])\n",
    "    trainset = torchvision.datasets.ImageFolder(root = 'ig/train_more_food',\n",
    "                                                 transform=preprocess)\n",
    " \n",
    "    train_loader = torch.utils.data.DataLoader(trainset, batch_size = batch_size,\n",
    "                                              num_workers = 1, shuffle = True)\n",
    "\n",
    "    \n",
    "    val_set = torchvision.datasets.ImageFolder(root='ig/val_more_food',\n",
    "                                                 transform=preprocess)\n",
    "    val_loader = torch.utils.data.DataLoader(val_set, batch_size=batch_size,\n",
    "                                              num_workers=1, shuffle = True)\n",
    "\n",
    "    testset = torchvision.datasets.ImageFolder(root= 'ig/test_more_food',\n",
    "                                            transform=preprocess)\n",
    "    \n",
    "    test_loader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                             num_workers=1, shuffle = True)\n",
    "    \n",
    "    \n",
    "    torch.manual_seed(1000)\n",
    "\n",
    "   \n",
    "  \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(net.fc.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "    #optimizer_conv = optim.SGD(list(filter(lambda p: p.requires_grad, model_conv.parameters())), lr=0.001, momentum=0.9)\n",
    "    train_err = np.zeros(num_epochs)\n",
    "    train_loss = np.zeros(num_epochs)\n",
    "    val_err = np.zeros(num_epochs)\n",
    "    val_loss = np.zeros(num_epochs)\n",
    "    \n",
    "    \n",
    "    iters, losses, train_acc, val_acc = [], [], [], []\n",
    "    start_time = time.time()\n",
    "    # training\n",
    "    n=0\n",
    "    for epoch in range(num_epochs):  # loop over the dataset multiple times \n",
    "      \n",
    "        total_train_loss = 0.0\n",
    "        total_train_err = 0.0\n",
    "\n",
    "        total_epoch = 0\n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "           \n",
    "            inputs, labels = data\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "                      \n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(inputs)\n",
    "           \n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            iters.append(n)\n",
    "           \n",
    "            pred = outputs.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "            total_train_err += pred.ne(labels.view_as(pred)).sum().item()\n",
    "            total_train_loss += loss.item()\n",
    "            total_epoch += len(labels)\n",
    "           \n",
    "            losses.append(float(loss)/batch_size)  \n",
    "            n += 1# compute *average* loss\n",
    "        \n",
    "        train_err[epoch] = float(total_train_err) / total_epoch\n",
    "     \n",
    "        train_loss[epoch] = float(total_train_loss) / (i+1)\n",
    "        \n",
    "        val_err[epoch], val_loss[epoch] = evaluate(net, val_loader, criterion)\n",
    "    \n",
    "        print((\"Epoch {}: Train err: {}, Train loss: {}  |\" +\n",
    "               \"Validation err: {} , Validation loss:{} \").format(\n",
    "                   epoch + 1,\n",
    "            \n",
    "                   train_err[epoch],\n",
    "                   train_loss[epoch],\n",
    "                   val_err[epoch],\n",
    "                   val_loss[epoch]\n",
    "                        ))\n",
    "\n",
    "        # Save the current model (checkpoint) to a file\n",
    "        model_path = get_model_name(\"RESNET\", batch_size, learning_rate, epoch)\n",
    "        torch.save(net.state_dict(), model_path)\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "\n",
    "        print(\"Total time elapsed: {:.2f} seconds\".format(elapsed_time))\n",
    "\n",
    "    print('Finished Training')\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(\"Total time elapsed: {:.2f} seconds\".format(elapsed_time))\n",
    "\n",
    "    # Write the train/test loss/err into CSV file for plotting later\n",
    "    epochs = np.arange(1, num_epochs + 1)\n",
    "\n",
    "    np.savetxt(\"{}_train_err.csv\".format(model_path), train_err)\n",
    "    np.savetxt(\"{}_train_loss.csv\".format(model_path), train_loss)\n",
    "    np.savetxt(\"{}_val_err.csv\".format(model_path), val_err)\n",
    "    np.savetxt(\"{}_val_loss.csv\".format(model_path), val_loss)\n",
    "        \n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K5TqoTanVvB4"
   },
   "outputs": [],
   "source": [
    "def trainBaseline(net, batch_size=1, learning_rate=0.0001, num_epochs=30):\n",
    "    from torchvision import transforms\n",
    "\n",
    "    # Set up a transform that scales and crops an image so it has the dimensions\n",
    "    # of the input layer of alexnet\n",
    "    scale_crop = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224)\n",
    "    ])\n",
    "  \n",
    "    # The normalization that was applied to the data when alexnet was trained\n",
    "    normalize = transforms.Normalize(\n",
    "      mean=[0.485, 0.456, 0.406],\n",
    "      std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "\n",
    "    # Put scaling, cropping, normalzing and converting from PIL image to pytorch into one package\n",
    "    preprocess = transforms.Compose([\n",
    "      scale_crop,\n",
    "      transforms.ToTensor(),\n",
    "      normalize\n",
    "    ])\n",
    "    trainset = torchvision.datasets.ImageFolder(root = 'train_more_food',\n",
    "                                                 transform=preprocess)\n",
    " \n",
    "    train_loader = torch.utils.data.DataLoader(trainset, batch_size = batch_size,\n",
    "                                              num_workers = 1, shuffle = True)\n",
    "\n",
    "    \n",
    "    val_set = torchvision.datasets.ImageFolder(root='val_more_food',\n",
    "                                                 transform=preprocess)\n",
    "    val_loader = torch.utils.data.DataLoader(val_set, batch_size=batch_size,\n",
    "                                              num_workers=1, shuffle = True)\n",
    "\n",
    "    testset = torchvision.datasets.ImageFolder(root= 'test_more_food',\n",
    "                                            transform=preprocess)\n",
    "    \n",
    "    test_loader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                             num_workers=1, shuffle = True)\n",
    "    \n",
    "    torch.manual_seed(1000)\n",
    "\n",
    "   \n",
    "  \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "    \n",
    "    train_err = np.zeros(num_epochs)\n",
    "    train_loss = np.zeros(num_epochs)\n",
    "    val_err = np.zeros(num_epochs)\n",
    "    val_loss = np.zeros(num_epochs)\n",
    "    \n",
    "    \n",
    "    iters, losses, train_acc, val_acc = [], [], [], []\n",
    "    start_time = time.time()\n",
    "    # training\n",
    "    n=0\n",
    "    for epoch in range(num_epochs):  # loop over the dataset multiple times \n",
    "      \n",
    "        total_train_loss = 0.0\n",
    "        total_train_err = 0.0\n",
    "\n",
    "        total_epoch = 0\n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "           \n",
    "            inputs, labels = data\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "                      \n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(inputs)\n",
    "           \n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            iters.append(n)\n",
    "           \n",
    "            pred = outputs.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "            total_train_err += pred.ne(labels.view_as(pred)).sum().item()\n",
    "            total_train_loss += loss.item()\n",
    "            total_epoch += len(labels)\n",
    "           \n",
    "            losses.append(float(loss)/batch_size)  \n",
    "            n += 1# compute *average* loss\n",
    "        \n",
    "        train_err[epoch] = float(total_train_err) / total_epoch\n",
    "     \n",
    "        train_loss[epoch] = float(total_train_loss) / (i+1)\n",
    "        \n",
    "        val_err[epoch], val_loss[epoch] = evaluate(net, val_loader, criterion)\n",
    "    \n",
    "        print((\"Epoch {}: Train err: {}, Train loss: {}  |\" +\n",
    "               \"Validation err: {} , Validation loss:{} \").format(\n",
    "                   epoch + 1,\n",
    "            \n",
    "                   train_err[epoch],\n",
    "                   train_loss[epoch],\n",
    "                   val_err[epoch],\n",
    "                   val_loss[epoch]\n",
    "                        ))\n",
    "        \n",
    "        # Save the current model (checkpoint) to a file\n",
    "        model_path = get_model_name(net.name, batch_size, learning_rate, epoch)\n",
    "        torch.save(net.state_dict(), model_path)\n",
    "        print(\"Total time elapsed: {:.1f} seconds\".format(elapsed_time))\n",
    "\n",
    "    print('Finished Training')\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(\"Total time elapsed: {:.2f} seconds\".format(elapsed_time))\n",
    "\n",
    "    # Write the train/test loss/err into CSV file for plotting later\n",
    "    epochs = np.arange(1, num_epochs + 1)\n",
    "\n",
    "    np.savetxt(\"{}_train_err.csv\".format(model_path), train_err)\n",
    "    np.savetxt(\"{}_train_loss.csv\".format(model_path), train_loss)\n",
    "    np.savetxt(\"{}_val_err.csv\".format(model_path), val_err)\n",
    "    np.savetxt(\"{}_val_loss.csv\".format(model_path), val_loss)\n",
    "        \n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JFA7StZPT9hq"
   },
   "outputs": [],
   "source": [
    "# Training Curve\n",
    "import matplotlib.pyplot as plt\n",
    "def plot_training_curve(path):\n",
    "    \"\"\" Plots the training curve for a model run, given the csv files\n",
    "    containing the train/validation error/loss.\n",
    "\n",
    "    Args:\n",
    "        path: The base path of the csv files produced during training\n",
    "    \"\"\"\n",
    "\n",
    "    train_err = np.loadtxt(\"{}_train_err.csv\".format(path))\n",
    "    val_err = np.loadtxt(\"{}_val_err.csv\".format(path))\n",
    "    train_loss = np.loadtxt(\"{}_train_loss.csv\".format(path))\n",
    "    val_loss = np.loadtxt(\"{}_val_loss.csv\".format(path))\n",
    "    plt.title(\"Train vs Validation Error\")\n",
    "    plt.plot(range(1,16), train_err, label=\"Train\")\n",
    "    plt.plot(range(1,16), val_err, label=\"Validation\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Error\")\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n",
    "    plt.title(\"Train vs Validation Loss\")\n",
    "    plt.plot(range(1,16), train_loss, label=\"Train\")\n",
    "    plt.plot(range(1,16), val_loss, label=\"Validation\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0d68hPcAT9ii"
   },
   "source": [
    "# Test Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5bEOwrIiT9i5"
   },
   "outputs": [],
   "source": [
    "# Importing relevant Libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.nn import functional as F\n",
    "import copy\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# Feature Extraction using AlexNet pretrained model\n",
    "\n",
    "class AlexNet(nn.Module):\n",
    "    '''\n",
    "    Class that loads AlexNet Feature Model ('Convolution layers') with imagenet trained weights\n",
    "    \n",
    "    input : image tensors with dimension Lx3x224x224\n",
    "    \n",
    "    output : feature tensor with dimension Lx256x6x6\n",
    "    \n",
    "    *L - Batch size\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    def load_weights(self):\n",
    "        an_builtin = torchvision.models.alexnet(pretrained=True) # Loads the pretrained model weights\n",
    "        \n",
    "        features_weight_i = [0, 3, 6, 8, 10]\n",
    "        for i in features_weight_i:\n",
    "            self.features[i].weight = an_builtin.features[i].weight\n",
    "            self.features[i].bias = an_builtin.features[i].bias\n",
    "\n",
    "    def __init__(self):\n",
    "        super(AlexNet, self).__init__()\n",
    "        self.name = \"AlexNet\"\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(256 * 6 * 6, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4096, 20),\n",
    "        )\n",
    "        self.load_weights() # Copies the weights to AlexNetFeatures model layers\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), 256 * 6 * 6)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainAlexNet(net, batch_size=1, learning_rate=0.0001, num_epochs=30):\n",
    "   \n",
    "    \n",
    "    from torchvision import transforms\n",
    "\n",
    "    # Set up a transform that scales and crops an image so it has the dimensions\n",
    "    # of the input layer of alexnet\n",
    "    scale_crop = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224)\n",
    "    ])\n",
    "  \n",
    "    # The normalization that was applied to the data when alexnet was trained\n",
    "    normalize = transforms.Normalize(\n",
    "      mean=[0.485, 0.456, 0.406],\n",
    "      std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "\n",
    "    # Put scaling, cropping, normalzing and converting from PIL image to pytorch into one package\n",
    "    preprocess = transforms.Compose([\n",
    "      scale_crop,\n",
    "      transforms.ToTensor(),\n",
    "      normalize\n",
    "    ])\n",
    "    trainset = torchvision.datasets.ImageFolder(root = 'ig/train_more_food',\n",
    "                                                 transform=preprocess)\n",
    " \n",
    "    train_loader = torch.utils.data.DataLoader(trainset, batch_size = batch_size,\n",
    "                                              num_workers = 1, shuffle = True)\n",
    "\n",
    "    \n",
    "    val_set = torchvision.datasets.ImageFolder(root='ig/val_more_food',\n",
    "                                                 transform=preprocess)\n",
    "    val_loader = torch.utils.data.DataLoader(val_set, batch_size=batch_size,\n",
    "                                              num_workers=1, shuffle = True)\n",
    "\n",
    "    testset = torchvision.datasets.ImageFolder(root= 'ig/test_more_food',\n",
    "                                            transform=preprocess)\n",
    "    \n",
    "    test_loader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                             num_workers=1, shuffle = True)\n",
    "    \n",
    "    \n",
    "    torch.manual_seed(1000)\n",
    "\n",
    "   \n",
    "  \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "    \n",
    "    train_err = np.zeros(num_epochs)\n",
    "    train_loss = np.zeros(num_epochs)\n",
    "    val_err = np.zeros(num_epochs)\n",
    "    val_loss = np.zeros(num_epochs)\n",
    "    \n",
    "    \n",
    "    iters, losses, train_acc, val_acc = [], [], [], []\n",
    "    start_time = time.time()\n",
    "    # training\n",
    "    n=0\n",
    "    for epoch in range(num_epochs):  # loop over the dataset multiple times \n",
    "      \n",
    "        total_train_loss = 0.0\n",
    "        total_train_err = 0.0\n",
    "\n",
    "        total_epoch = 0\n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "           \n",
    "            inputs, labels = data\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "                      \n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(inputs)\n",
    "           \n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            iters.append(n)\n",
    "            pred = outputs.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "            total_train_err += pred.ne(labels.view_as(pred)).sum().item()\n",
    "            total_train_loss += loss.item()\n",
    "            total_epoch += len(labels)\n",
    "           \n",
    "            losses.append(float(loss)/batch_size)  \n",
    "            n += 1# compute *average* loss\n",
    "        \n",
    "        train_err[epoch] = float(total_train_err) / total_epoch\n",
    "     \n",
    "        train_loss[epoch] = float(total_train_loss) / (i+1)\n",
    "        \n",
    "        val_err[epoch], val_loss[epoch] = evaluate(net, val_loader, criterion)\n",
    "    \n",
    "        print((\"Epoch {}: Train err: {}, Train loss: {}  |\" +\n",
    "               \"Validation err: {} , Validation loss:{} \").format(\n",
    "                   epoch + 1,\n",
    "            \n",
    "                   train_err[epoch],\n",
    "                   train_loss[epoch],\n",
    "                   val_err[epoch],\n",
    "                   val_loss[epoch]\n",
    "                        ))\n",
    "\n",
    "        # Save the current model (checkpoint) to a file\n",
    "        model_path = get_model_name(\"RESNET\", batch_size, learning_rate, epoch)\n",
    "        torch.save(net.state_dict(), model_path)\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "\n",
    "        print(\"Total time elapsed: {:.2f} seconds\".format(elapsed_time))\n",
    "\n",
    "    print('Finished Training')\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(\"Total time elapsed: {:.2f} seconds\".format(elapsed_time))\n",
    "\n",
    "    # Write the train/test loss/err into CSV file for plotting later\n",
    "    epochs = np.arange(1, num_epochs + 1)\n",
    "\n",
    "    np.savetxt(\"{}_train_err.csv\".format(model_path), train_err)\n",
    "    np.savetxt(\"{}_train_loss.csv\".format(model_path), train_loss)\n",
    "    np.savetxt(\"{}_val_err.csv\".format(model_path), val_err)\n",
    "    np.savetxt(\"{}_val_loss.csv\".format(model_path), val_loss)\n",
    "        \n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "91DGE92aT9i9"
   },
   "outputs": [],
   "source": [
    "model = AlexNet()\n",
    "#netalex.to(device)\n",
    "\n",
    "#model = nn.DataParallel(netalex, [0,1])\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 592
    },
    "colab_type": "code",
    "id": "5U2LXww4T9jA",
    "outputId": "33e1ba85-ef10-448c-8d87-1670ff684a64"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train err: 0.648, Train loss: 2.1855580090208258  |Validation err: 0.4725 , Validation loss:1.5331826739841037 \n",
      "Total time elapsed: 91.61 seconds\n",
      "Epoch 2: Train err: 0.39416666666666667, Train loss: 1.3047558525775342  |Validation err: 0.391 , Validation loss:1.2812358386932858 \n",
      "Total time elapsed: 182.76 seconds\n"
     ]
    }
   ],
   "source": [
    "trainAlexNet(model, batch_size = 64, learning_rate = 0.00001, num_epochs = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Jvj-kk8BT9jC",
    "outputId": "dfb24e14-78fb-496e-b89b-302f736437d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train err: 0.8636666666666667, Train loss: 6.696951688604152  |Validation err: 0.8235 , Validation loss:2.198826481425573 \n",
      "Epoch 2: Train err: 0.7846666666666666, Train loss: 2.0924417465291123  |Validation err: 0.7975 , Validation loss:2.510080871127901 \n",
      "Epoch 3: Train err: 0.7626666666666667, Train loss: 2.0622327657456094  |Validation err: 0.7625 , Validation loss:2.4051466385523477 \n",
      "Epoch 4: Train err: 0.751, Train loss: 2.019769406065028  |Validation err: 0.7625 , Validation loss:2.2512343421814935 \n",
      "Epoch 5: Train err: 0.7235, Train loss: 1.9731543495299968  |Validation err: 0.7465 , Validation loss:2.336838175380041 \n",
      "Epoch 6: Train err: 0.6973333333333334, Train loss: 1.9247440936717581  |Validation err: 0.7205 , Validation loss:2.2657625448136103 \n",
      "Epoch 7: Train err: 0.6743333333333333, Train loss: 1.8526354774515679  |Validation err: 0.6925 , Validation loss:2.011690378189087 \n",
      "Epoch 8: Train err: 0.6598333333333334, Train loss: 1.827751729082554  |Validation err: 0.7235 , Validation loss:2.3836690925416493 \n",
      "Epoch 9: Train err: 0.6433333333333333, Train loss: 1.7885482438067173  |Validation err: 0.719 , Validation loss:2.336203590271965 \n",
      "Epoch 10: Train err: 0.6193333333333333, Train loss: 1.743459827088295  |Validation err: 0.704 , Validation loss:2.3837664978844777 \n",
      "Epoch 11: Train err: 0.6101666666666666, Train loss: 1.6957772919472227  |Validation err: 0.7005 , Validation loss:2.4929724647885276 \n",
      "Epoch 12: Train err: 0.5885, Train loss: 1.6535703374984416  |Validation err: 0.645 , Validation loss:2.0353128645155163 \n",
      "Epoch 13: Train err: 0.5641666666666667, Train loss: 1.596399836083676  |Validation err: 0.6535 , Validation loss:2.2003054013327947 \n",
      "Epoch 14: Train err: 0.5616666666666666, Train loss: 1.578344371724636  |Validation err: 0.671 , Validation loss:2.3120113478766546 \n",
      "Epoch 15: Train err: 0.5436666666666666, Train loss: 1.5411840436306405  |Validation err: 0.6545 , Validation loss:2.978955346440512 \n",
      "Finished Training\n",
      "Total time elapsed: 57385.94 seconds\n"
     ]
    }
   ],
   "source": [
    "train(netalex, batch_size = 32, learning_rate = 0.001, num_epochs = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "colab_type": "code",
    "id": "DdWaCx5WwSTD",
    "outputId": "9f208fba-93f3-480e-b9ef-a90624ef5d72"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train err: 0.3865, Train loss: 1.1424561700922378  |Validation err: 0.2865671641791045 , Validation loss:0.9071825702512075 \n",
      "Epoch 2: Train err: 0.197, Train loss: 0.6006449099550856  |Validation err: 0.3686567164179104 , Validation loss:1.1458514368250257 \n",
      "Epoch 3: Train err: 0.11216666666666666, Train loss: 0.35121220500862343  |Validation err: 0.30895522388059704 , Validation loss:1.091131821038231 \n",
      "Epoch 4: Train err: 0.07083333333333333, Train loss: 0.21470273650707083  |Validation err: 0.309452736318408 , Validation loss:1.3678297010206042 \n",
      "Epoch 5: Train err: 0.0425, Train loss: 0.1261425785958133  |Validation err: 0.31840796019900497 , Validation loss:1.5226205663075523 \n",
      "Epoch 6: Train err: 0.03866666666666667, Train loss: 0.11121613052772715  |Validation err: 0.32487562189054725 , Validation loss:1.636424587359504 \n",
      "Epoch 7: Train err: 0.024333333333333332, Train loss: 0.06661355166517674  |Validation err: 0.3238805970149254 , Validation loss:1.791774418618944 \n",
      "Epoch 8: Train err: 0.018833333333333334, Train loss: 0.06231192800592869  |Validation err: 0.27611940298507465 , Validation loss:1.5329324697691298 \n",
      "Epoch 9: Train err: 0.021666666666666667, Train loss: 0.05928395157481762  |Validation err: 0.36069651741293535 , Validation loss:2.203834565622466 \n",
      "Epoch 10: Train err: 0.021166666666666667, Train loss: 0.06568412256843233  |Validation err: 0.27412935323383086 , Validation loss:1.44787502312471 \n",
      "Epoch 11: Train err: 0.0225, Train loss: 0.06399956492191934  |Validation err: 0.26318407960199003 , Validation loss:1.444032204056543 \n",
      "Epoch 12: Train err: 0.012833333333333334, Train loss: 0.03390684236395866  |Validation err: 0.2870646766169154 , Validation loss:1.8885924461342039 \n",
      "Epoch 13: Train err: 0.0205, Train loss: 0.0589857573145406  |Validation err: 0.3482587064676617 , Validation loss:2.278676712560275 \n",
      "Epoch 14: Train err: 0.015, Train loss: 0.045627714085214315  |Validation err: 0.29701492537313434 , Validation loss:1.6853867343493871 \n",
      "Epoch 15: Train err: 0.008666666666666666, Train loss: 0.027287409938079245  |Validation err: 0.33134328358208953 , Validation loss:2.273909136889473 \n",
      "Epoch 16: Train err: 0.017333333333333333, Train loss: 0.047755037672183615  |Validation err: 0.31791044776119404 , Validation loss:1.919470149846304 \n",
      "Epoch 17: Train err: 0.013833333333333333, Train loss: 0.045271610246693836  |Validation err: 0.2796019900497512 , Validation loss:1.6636495147905652 \n",
      "Epoch 18: Train err: 0.007166666666666667, Train loss: 0.025918921138694945  |Validation err: 0.354726368159204 , Validation loss:2.498535850691417 \n",
      "Epoch 19: Train err: 0.0095, Train loss: 0.030174928472099946  |Validation err: 0.2845771144278607 , Validation loss:1.7138834666638147 \n",
      "Epoch 20: Train err: 0.007, Train loss: 0.01943804448510104  |Validation err: 0.3009950248756219 , Validation loss:2.257768635948499 \n",
      "Finished Training\n",
      "Total time elapsed: 2251.39 seconds\n"
     ]
    }
   ],
   "source": [
    "netalex = AlexNet()\n",
    "netalex.to(device)\n",
    "train(netalex, batch_size = 16, learning_rate = 0.0001, num_epochs = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "colab_type": "code",
    "id": "IMPgfAFwBDaV",
    "outputId": "35024ada-7cfa-4673-fee2-36236a811045"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/alexnet-owt-4df8aa71.pth\" to /root/.torch/models/alexnet-owt-4df8aa71.pth\n",
      "244418560it [00:04, 51536996.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train err: 0.8931666666666667, Train loss: 2.3129352813071393  |Validation err: 0.900497512437811 , Validation loss:2.3032867908477783 \n",
      "Epoch 2: Train err: 0.878, Train loss: 2.2840839675132263  |Validation err: 0.8766169154228856 , Validation loss:2.2816719819629 \n",
      "Epoch 3: Train err: 0.8566666666666667, Train loss: 2.2538155393397554  |Validation err: 0.8671641791044776 , Validation loss:2.270226516420879 \n",
      "Epoch 4: Train err: 0.8348333333333333, Train loss: 2.218134271337631  |Validation err: 0.836318407960199 , Validation loss:2.219892628609188 \n",
      "Epoch 5: Train err: 0.8061666666666667, Train loss: 2.1489719976770116  |Validation err: 0.8238805970149253 , Validation loss:2.668194231532869 \n",
      "Epoch 6: Train err: 0.7178333333333333, Train loss: 1.9751741493001898  |Validation err: 0.7079601990049751 , Validation loss:2.018897395285349 \n",
      "Epoch 7: Train err: 0.601, Train loss: 1.6993810927614252  |Validation err: 0.6975124378109453 , Validation loss:2.4539715827457487 \n",
      "Epoch 8: Train err: 0.5066666666666667, Train loss: 1.4586954433867272  |Validation err: 0.6059701492537314 , Validation loss:2.1201243608716935 \n",
      "Epoch 9: Train err: 0.39266666666666666, Train loss: 1.163744403326765  |Validation err: 0.5736318407960199 , Validation loss:2.1874502178222412 \n",
      "Epoch 10: Train err: 0.30533333333333335, Train loss: 0.9140308575427278  |Validation err: 0.5373134328358209 , Validation loss:1.9353769364811124 \n",
      "Epoch 11: Train err: 0.24533333333333332, Train loss: 0.7295319923061006  |Validation err: 0.5109452736318408 , Validation loss:2.230195787217882 \n",
      "Epoch 12: Train err: 0.19833333333333333, Train loss: 0.6054298928443421  |Validation err: 0.5447761194029851 , Validation loss:2.7820765820760576 \n",
      "Epoch 13: Train err: 0.141, Train loss: 0.41309425948148076  |Validation err: 0.563681592039801 , Validation loss:2.841241520548624 \n",
      "Epoch 14: Train err: 0.10166666666666667, Train loss: 0.3064268806830366  |Validation err: 0.49850746268656715 , Validation loss:2.3925273740102373 \n",
      "Epoch 15: Train err: 0.09583333333333334, Train loss: 0.28892292637140193  |Validation err: 0.5054726368159204 , Validation loss:2.4206373142817665 \n",
      "Epoch 16: Train err: 0.09133333333333334, Train loss: 0.2855696577895829  |Validation err: 0.554726368159204 , Validation loss:3.927305338874696 \n",
      "Epoch 17: Train err: 0.06366666666666666, Train loss: 0.20071197994687456  |Validation err: 0.5293532338308458 , Validation loss:3.3920124390768627 \n",
      "Epoch 18: Train err: 0.06116666666666667, Train loss: 0.1806436878173275  |Validation err: 0.4766169154228856 , Validation loss:2.89788341900659 \n",
      "Epoch 19: Train err: 0.05616666666666666, Train loss: 0.17415978815010252  |Validation err: 0.4925373134328358 , Validation loss:3.1044607276008245 \n",
      "Epoch 20: Train err: 0.052, Train loss: 0.15994947593896947  |Validation err: 0.4701492537313433 , Validation loss:3.193839967250824 \n",
      "Finished Training\n",
      "Total time elapsed: 3364.05 seconds\n"
     ]
    }
   ],
   "source": [
    "netalex = AlexNet()\n",
    "netalex.to(device)\n",
    "train(netalex, batch_size = 32, learning_rate = 0.0005, num_epochs = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "colab_type": "code",
    "id": "jFCR_AXLR4fZ",
    "outputId": "67884d5a-cb82-42c9-82bb-93cd33019d65"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train err: 0.8835, Train loss: 2.320931708559077  |Validation err: 0.8791044776119403 , Validation loss:2.2924960075862826 \n",
      "Epoch 2: Train err: 0.8613333333333333, Train loss: 2.2638895029717303  |Validation err: 0.8771144278606965 , Validation loss:2.285294078645252 \n",
      "Epoch 3: Train err: 0.828, Train loss: 2.179639473874518  |Validation err: 0.818407960199005 , Validation loss:2.2222018563558184 \n",
      "Epoch 4: Train err: 0.7263333333333334, Train loss: 1.9586204531344962  |Validation err: 0.6567164179104478 , Validation loss:1.811238551896716 \n",
      "Epoch 5: Train err: 0.6023333333333334, Train loss: 1.652762827721048  |Validation err: 0.6343283582089553 , Validation loss:1.8037584414557806 \n",
      "Epoch 6: Train err: 0.473, Train loss: 1.3626352485190047  |Validation err: 0.508955223880597 , Validation loss:1.6825155871255058 \n",
      "Epoch 7: Train err: 0.3635, Train loss: 1.0854070890457073  |Validation err: 0.5084577114427861 , Validation loss:1.517351762642936 \n",
      "Epoch 8: Train err: 0.30366666666666664, Train loss: 0.9126868083121928  |Validation err: 0.5348258706467661 , Validation loss:1.8191457002881974 \n",
      "Epoch 9: Train err: 0.24583333333333332, Train loss: 0.7387305396668454  |Validation err: 0.5069651741293533 , Validation loss:1.7135927942064073 \n",
      "Epoch 10: Train err: 0.20366666666666666, Train loss: 0.6041051482266568  |Validation err: 0.4412935323383085 , Validation loss:1.665384529128907 \n",
      "Epoch 11: Train err: 0.156, Train loss: 0.47282060790569225  |Validation err: 0.4417910447761194 , Validation loss:1.9173926029886519 \n",
      "Epoch 12: Train err: 0.11416666666666667, Train loss: 0.3487686586189777  |Validation err: 0.4621890547263682 , Validation loss:1.9606360367366247 \n",
      "Epoch 13: Train err: 0.09033333333333333, Train loss: 0.2803966020967098  |Validation err: 0.49402985074626865 , Validation loss:2.516180313768841 \n",
      "Epoch 14: Train err: 0.08266666666666667, Train loss: 0.25980742299493326  |Validation err: 0.46616915422885574 , Validation loss:2.1155306233300104 \n",
      "Epoch 15: Train err: 0.07216666666666667, Train loss: 0.22145704477232822  |Validation err: 0.4616915422885572 , Validation loss:2.203220488533141 \n",
      "Epoch 16: Train err: 0.057166666666666664, Train loss: 0.17086679072297634  |Validation err: 0.47512437810945274 , Validation loss:3.607003338753231 \n",
      "Epoch 17: Train err: 0.07183333333333333, Train loss: 0.23357124115399858  |Validation err: 0.42338308457711443 , Validation loss:1.8753071361117892 \n",
      "Epoch 18: Train err: 0.05516666666666667, Train loss: 0.16776240161283218  |Validation err: 0.4691542288557214 , Validation loss:2.6052251410862755 \n",
      "Epoch 19: Train err: 0.041666666666666664, Train loss: 0.12279086260798763  |Validation err: 0.4577114427860697 , Validation loss:3.5813440917030213 \n",
      "Epoch 20: Train err: 0.025666666666666667, Train loss: 0.07824255193167544  |Validation err: 0.4462686567164179 , Validation loss:3.038092255592346 \n",
      "Finished Training\n",
      "Total time elapsed: 2039.96 seconds\n"
     ]
    }
   ],
   "source": [
    "netalex = AlexNet()\n",
    "netalex.to(device)\n",
    "train(netalex, batch_size = 64, learning_rate = 0.0005, num_epochs = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "colab_type": "code",
    "id": "igsqV0AeSBu5",
    "outputId": "a6465f56-c80c-4c0a-8a83-1d9828c9dfbd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train err: 0.4196666666666667, Train loss: 1.2426844055348254  |Validation err: 0.2990049751243781 , Validation loss:0.920150447459448 \n",
      "Epoch 2: Train err: 0.219, Train loss: 0.6540741188094971  |Validation err: 0.3567164179104478 , Validation loss:1.1191261514784798 \n",
      "Epoch 3: Train err: 0.156, Train loss: 0.473246028765719  |Validation err: 0.2860696517412935 , Validation loss:0.926016146228427 \n",
      "Epoch 4: Train err: 0.10583333333333333, Train loss: 0.3268153307602761  |Validation err: 0.3119402985074627 , Validation loss:1.1126495798428853 \n",
      "Epoch 5: Train err: 0.06833333333333333, Train loss: 0.21071445062122446  |Validation err: 0.31144278606965176 , Validation loss:1.370934506257375 \n",
      "Epoch 6: Train err: 0.0505, Train loss: 0.15983863476108998  |Validation err: 0.2716417910447761 , Validation loss:1.151049964957767 \n",
      "Epoch 7: Train err: 0.031, Train loss: 0.09941372552767713  |Validation err: 0.2796019900497512 , Validation loss:1.213928859385233 \n",
      "Epoch 8: Train err: 0.028166666666666666, Train loss: 0.08551554068764473  |Validation err: 0.27761194029850744 , Validation loss:1.2897675836843157 \n",
      "Epoch 9: Train err: 0.02266666666666667, Train loss: 0.06321876139399853  |Validation err: 0.2840796019900497 , Validation loss:1.3508467986470176 \n",
      "Epoch 10: Train err: 0.015, Train loss: 0.04599705211659695  |Validation err: 0.30895522388059704 , Validation loss:1.6540270286893088 \n",
      "Epoch 11: Train err: 0.019666666666666666, Train loss: 0.05216296407532819  |Validation err: 0.2766169154228856 , Validation loss:1.5321917420341855 \n",
      "Epoch 12: Train err: 0.0115, Train loss: 0.036615117116177334  |Validation err: 0.3288557213930348 , Validation loss:1.9861294542040144 \n",
      "Epoch 13: Train err: 0.008666666666666666, Train loss: 0.03117739159534586  |Validation err: 0.2945273631840796 , Validation loss:1.6232731451117803 \n",
      "Epoch 14: Train err: 0.012833333333333334, Train loss: 0.039541525716715036  |Validation err: 0.27412935323383086 , Validation loss:1.5511488337365409 \n",
      "Epoch 15: Train err: 0.012666666666666666, Train loss: 0.038029459721230445  |Validation err: 0.34129353233830845 , Validation loss:2.2117845444452193 \n",
      "Epoch 16: Train err: 0.0095, Train loss: 0.0312893685211051  |Validation err: 0.2865671641791045 , Validation loss:1.7389576340478563 \n",
      "Epoch 17: Train err: 0.0065, Train loss: 0.021654769476067197  |Validation err: 0.2915422885572139 , Validation loss:1.929986574347057 \n",
      "Epoch 18: Train err: 0.011166666666666667, Train loss: 0.03174256542548934  |Validation err: 0.2721393034825871 , Validation loss:1.5763731438016135 \n",
      "Epoch 19: Train err: 0.012166666666666666, Train loss: 0.035264190776828436  |Validation err: 0.30646766169154227 , Validation loss:1.904491576883528 \n",
      "Epoch 20: Train err: 0.006333333333333333, Train loss: 0.01975536217140232  |Validation err: 0.3044776119402985 , Validation loss:2.049412723571535 \n",
      "Finished Training\n",
      "Total time elapsed: 2066.40 seconds\n"
     ]
    }
   ],
   "source": [
    "netalex = AlexNet()\n",
    "netalex.to(device)\n",
    "train(netalex, batch_size = 32, learning_rate = 0.00005, num_epochs = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "colab_type": "code",
    "id": "ES9xsuzFniNJ",
    "outputId": "6c896984-9dc6-48b1-a15c-8e26e522e925"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train err: 0.41883333333333334, Train loss: 1.2419434785842896  |Validation err: 0.3034825870646766 , Validation loss:0.9177590764704204 \n",
      "Epoch 2: Train err: 0.22016666666666668, Train loss: 0.657989907454937  |Validation err: 0.35124378109452736 , Validation loss:1.1080554543979584 \n",
      "Epoch 3: Train err: 0.15733333333333333, Train loss: 0.47340591885942096  |Validation err: 0.291044776119403 , Validation loss:0.9185986149878729 \n",
      "Epoch 4: Train err: 0.103, Train loss: 0.32332505919831867  |Validation err: 0.31044776119402984 , Validation loss:1.113845521022403 \n",
      "Epoch 5: Train err: 0.0685, Train loss: 0.21185640697466565  |Validation err: 0.3054726368159204 , Validation loss:1.3256247895104545 \n",
      "Epoch 6: Train err: 0.051, Train loss: 0.15742263427757203  |Validation err: 0.2845771144278607 , Validation loss:1.2137668506493644 \n",
      "Epoch 7: Train err: 0.029833333333333333, Train loss: 0.09628963585388153  |Validation err: 0.2860696517412935 , Validation loss:1.2755623246942247 \n",
      "Epoch 8: Train err: 0.03283333333333333, Train loss: 0.09294345099082653  |Validation err: 0.27810945273631843 , Validation loss:1.2632609028664847 \n",
      "Epoch 9: Train err: 0.0265, Train loss: 0.0734409973659414  |Validation err: 0.2880597014925373 , Validation loss:1.3908424524087755 \n",
      "Epoch 10: Train err: 0.009, Train loss: 0.03698920553669016  |Validation err: 0.30199004975124377 , Validation loss:1.5953040969750238 \n",
      "Epoch 11: Train err: 0.015, Train loss: 0.04418945296647701  |Validation err: 0.29850746268656714 , Validation loss:1.7095538945425124 \n",
      "Epoch 12: Train err: 0.01, Train loss: 0.031203043845264202  |Validation err: 0.32238805970149254 , Validation loss:1.9997996843996502 \n",
      "Epoch 13: Train err: 0.012, Train loss: 0.03817845912689858  |Validation err: 0.33930348258706466 , Validation loss:2.1710558495824297 \n",
      "Epoch 14: Train err: 0.009833333333333333, Train loss: 0.030824665668757356  |Validation err: 0.2716417910447761 , Validation loss:1.5995147076864091 \n",
      "Epoch 15: Train err: 0.013166666666666667, Train loss: 0.03718106028564433  |Validation err: 0.34378109452736316 , Validation loss:1.9989472873627194 \n",
      "Epoch 16: Train err: 0.0095, Train loss: 0.027039019603955935  |Validation err: 0.28955223880597014 , Validation loss:1.81055862562997 \n",
      "Epoch 17: Train err: 0.008833333333333334, Train loss: 0.03018204118818679  |Validation err: 0.3288557213930348 , Validation loss:2.166573155493963 \n",
      "Epoch 18: Train err: 0.014833333333333334, Train loss: 0.04339582761105625  |Validation err: 0.32686567164179103 , Validation loss:1.9392749922616142 \n",
      "Epoch 19: Train err: 0.009333333333333334, Train loss: 0.03032981389340885  |Validation err: 0.3492537313432836 , Validation loss:2.3371407768082997 \n",
      "Epoch 20: Train err: 0.007333333333333333, Train loss: 0.02515120333694714  |Validation err: 0.2810945273631841 , Validation loss:1.5031843038778456 \n",
      "Finished Training\n",
      "Total time elapsed: 2069.56 seconds\n"
     ]
    }
   ],
   "source": [
    "netalex = AlexNet()\n",
    "netalex.to(device)\n",
    "train(netalex, batch_size = 64, learning_rate = 0.00005, num_epochs = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "colab_type": "code",
    "id": "OH8PelmRB4TP",
    "outputId": "afef8229-e002-4b05-bdd2-c4a79148fb62"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train err: 0.5936666666666667, Train loss: 1.809698923471126  |Validation err: 0.41691542288557215 , Validation loss:1.226113047864702 \n",
      "Epoch 2: Train err: 0.3368333333333333, Train loss: 1.013751449103051  |Validation err: 0.3497512437810945 , Validation loss:1.0828766529522245 \n",
      "Epoch 3: Train err: 0.26616666666666666, Train loss: 0.8230058829835121  |Validation err: 0.345273631840796 , Validation loss:1.0834999680519104 \n",
      "Epoch 4: Train err: 0.23733333333333334, Train loss: 0.7154985222410648  |Validation err: 0.3338308457711443 , Validation loss:1.059084074837821 \n",
      "Epoch 5: Train err: 0.207, Train loss: 0.6225019794829348  |Validation err: 0.3174129353233831 , Validation loss:1.0316127169699896 \n",
      "Epoch 6: Train err: 0.18166666666666667, Train loss: 0.5609518210304544  |Validation err: 0.3074626865671642 , Validation loss:1.0131385222313896 \n",
      "Epoch 7: Train err: 0.17233333333333334, Train loss: 0.5021994854224489  |Validation err: 0.29701492537313434 , Validation loss:0.9666019645002153 \n",
      "Epoch 8: Train err: 0.14466666666666667, Train loss: 0.4493910616382639  |Validation err: 0.2935323383084577 , Validation loss:0.9925519947021727 \n",
      "Epoch 9: Train err: 0.128, Train loss: 0.40545543362485603  |Validation err: 0.30696517412935326 , Validation loss:1.1310163159219047 \n",
      "Epoch 10: Train err: 0.12433333333333334, Train loss: 0.3651692119050533  |Validation err: 0.2955223880597015 , Validation loss:1.065392796009306 \n",
      "Epoch 11: Train err: 0.10516666666666667, Train loss: 0.3297262526255973  |Validation err: 0.30149253731343284 , Validation loss:1.114138564420125 \n",
      "Epoch 12: Train err: 0.098, Train loss: 0.2951604720759899  |Validation err: 0.31840796019900497 , Validation loss:1.209966254612756 \n",
      "Epoch 13: Train err: 0.086, Train loss: 0.2583678243483635  |Validation err: 0.30398009950248756 , Validation loss:1.230717172698369 \n",
      "Epoch 14: Train err: 0.0775, Train loss: 0.22981409752305518  |Validation err: 0.30597014925373134 , Validation loss:1.2703235977225833 \n",
      "Epoch 15: Train err: 0.06383333333333334, Train loss: 0.20499081925508825  |Validation err: 0.2975124378109453 , Validation loss:1.1992925736639235 \n",
      "Epoch 16: Train err: 0.054, Train loss: 0.17403551841035803  |Validation err: 0.28756218905472636 , Validation loss:1.1508500017817058 \n",
      "Epoch 17: Train err: 0.0535, Train loss: 0.16133089545876422  |Validation err: 0.2835820895522388 , Validation loss:1.247922380765279 \n",
      "Epoch 18: Train err: 0.042833333333333334, Train loss: 0.13750680924413053  |Validation err: 0.29203980099502486 , Validation loss:1.3094454923319438 \n",
      "Epoch 19: Train err: 0.042333333333333334, Train loss: 0.13140984917891788  |Validation err: 0.3044776119402985 , Validation loss:1.4734993463470822 \n",
      "Epoch 20: Train err: 0.03716666666666667, Train loss: 0.1164837407859716  |Validation err: 0.2885572139303483 , Validation loss:1.3569671776559618 \n",
      "Finished Training\n",
      "Total time elapsed: 2193.26 seconds\n"
     ]
    }
   ],
   "source": [
    "netalex2 = AlexNet()\n",
    "netalex2.to(device)\n",
    "train(netalex2, batch_size = 32, learning_rate = 0.00001, num_epochs = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "colab_type": "code",
    "id": "WG-dQfq230r5",
    "outputId": "e826b442-2310-47bf-8409-84a331c46eb7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train err: 0.7033333333333334, Train loss: 2.0962643889670676  |Validation err: 0.5358208955223881 , Validation loss:1.6299511988957722 \n",
      "Epoch 2: Train err: 0.4156666666666667, Train loss: 1.342432473568206  |Validation err: 0.4223880597014925 , Validation loss:1.2120010058085124 \n",
      "Epoch 3: Train err: 0.33916666666666667, Train loss: 1.0346959842012284  |Validation err: 0.3925373134328358 , Validation loss:1.2089395097323827 \n",
      "Epoch 4: Train err: 0.29483333333333334, Train loss: 0.9154414857955689  |Validation err: 0.354228855721393 , Validation loss:1.0923599704863534 \n",
      "Epoch 5: Train err: 0.27, Train loss: 0.8197406967903705  |Validation err: 0.33880597014925373 , Validation loss:1.0811677045292325 \n",
      "Epoch 6: Train err: 0.25066666666666665, Train loss: 0.767062412931564  |Validation err: 0.3497512437810945 , Validation loss:1.0846690213869488 \n",
      "Epoch 7: Train err: 0.23166666666666666, Train loss: 0.7002718274263625  |Validation err: 0.336318407960199 , Validation loss:1.087264839618925 \n",
      "Epoch 8: Train err: 0.21816666666666668, Train loss: 0.6669354302451965  |Validation err: 0.32338308457711445 , Validation loss:1.0256766227502672 \n",
      "Epoch 9: Train err: 0.20433333333333334, Train loss: 0.6224860631405039  |Validation err: 0.3144278606965174 , Validation loss:1.0188905284518288 \n",
      "Epoch 10: Train err: 0.19016666666666668, Train loss: 0.581495648368876  |Validation err: 0.2980099502487562 , Validation loss:1.0008825848972986 \n",
      "Epoch 11: Train err: 0.17283333333333334, Train loss: 0.5359968175279334  |Validation err: 0.3054726368159204 , Validation loss:1.0438260795578125 \n",
      "Epoch 12: Train err: 0.171, Train loss: 0.5187038519281022  |Validation err: 0.32686567164179103 , Validation loss:1.0790726050497994 \n",
      "Epoch 13: Train err: 0.15483333333333332, Train loss: 0.47921800074425147  |Validation err: 0.30895522388059704 , Validation loss:1.0721513822911277 \n",
      "Epoch 14: Train err: 0.15016666666666667, Train loss: 0.457094687889231  |Validation err: 0.3194029850746269 , Validation loss:1.1331724041984195 \n",
      "Epoch 15: Train err: 0.1435, Train loss: 0.4359391599576524  |Validation err: 0.3049751243781095 , Validation loss:1.0948609227225894 \n",
      "Epoch 16: Train err: 0.13083333333333333, Train loss: 0.40745716113993463  |Validation err: 0.32487562189054725 , Validation loss:1.1847838466129605 \n",
      "Epoch 17: Train err: 0.126, Train loss: 0.37976542891974147  |Validation err: 0.29253731343283584 , Validation loss:1.0505517811056166 \n",
      "Epoch 18: Train err: 0.11983333333333333, Train loss: 0.36004361320049205  |Validation err: 0.30597014925373134 , Validation loss:1.134976575298915 \n",
      "Epoch 19: Train err: 0.11016666666666666, Train loss: 0.33718803461561814  |Validation err: 0.2845771144278607 , Validation loss:1.0858088755418385 \n",
      "Epoch 20: Train err: 0.1015, Train loss: 0.31902012355784154  |Validation err: 0.2840796019900497 , Validation loss:1.0709192128408522 \n",
      "Finished Training\n",
      "Total time elapsed: 2064.58 seconds\n"
     ]
    }
   ],
   "source": [
    "netalex2 = AlexNet()\n",
    "netalex2.to(device)\n",
    "train(netalex2, batch_size = 32, learning_rate = 0.000005, num_epochs = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TohhgGDfT9jL"
   },
   "source": [
    "# Test Accuracy-Alex Net\n",
    "Report the test accuracy of your best model. How does the test accuracy compare to part 4(d)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fX3IqNMNT9jM"
   },
   "outputs": [],
   "source": [
    "def get_accuracy(model):\n",
    "    data = testset\n",
    "    total = 0\n",
    "    correct =0\n",
    "    for imgs, labels in torch.utils.data.DataLoader(data, batch_size=32):\n",
    "        features = myfeature_model(imgs)\n",
    "        output = net(features) # We don't need to run F.softmax\n",
    "        pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        correct += pred.eq(labels.view_as(pred)).sum().item()\n",
    "        total += imgs.shape[0]\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X8h6ndabT9jY"
   },
   "outputs": [],
   "source": [
    "get_accuracy(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p7NvjfYEiXIa"
   },
   "source": [
    "# ResNet18 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FREEZE LAYERS + LEARNING RATE SCHEDULER**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainResNetnew(net, batch_size=1, learning_rate=0.0001, num_epochs=30):\n",
    "   \n",
    "    \n",
    "    from torchvision import transforms\n",
    "    from torch.optim import lr_scheduler\n",
    "    # Set up a transform that scales and crops an image so it has the dimensions\n",
    "    # of the input layer of alexnet\n",
    "    scale_crop = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224)\n",
    "    ])\n",
    "  \n",
    "    # The normalization that was applied to the data when alexnet was trained\n",
    "    normalize = transforms.Normalize(\n",
    "      mean=[0.485, 0.456, 0.406],\n",
    "      std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "\n",
    "    # Put scaling, cropping, normalzing and converting from PIL image to pytorch into one package\n",
    "    preprocess = transforms.Compose([\n",
    "      scale_crop,\n",
    "      transforms.ToTensor(),\n",
    "      normalize\n",
    "    ])\n",
    "    trainset = torchvision.datasets.ImageFolder(root = 'ig/train_more_food',\n",
    "                                                 transform=preprocess)\n",
    " \n",
    "    train_loader = torch.utils.data.DataLoader(trainset, batch_size = batch_size,\n",
    "                                              num_workers = 1, shuffle = True)\n",
    "\n",
    "    \n",
    "    val_set = torchvision.datasets.ImageFolder(root='ig/val_more_food',\n",
    "                                                 transform=preprocess)\n",
    "    val_loader = torch.utils.data.DataLoader(val_set, batch_size=batch_size,\n",
    "                                              num_workers=1, shuffle = True)\n",
    "\n",
    "    testset = torchvision.datasets.ImageFolder(root= 'ig/test_more_food',\n",
    "                                            transform=preprocess)\n",
    "    \n",
    "    test_loader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                             num_workers=1, shuffle = True)\n",
    "    \n",
    "    \n",
    "    torch.manual_seed(1000)\n",
    "\n",
    "    optimizer = optim.SGD(list(filter(lambda p: p.requires_grad, model_conv.parameters())), lr=0.001, momentum=0.9)\n",
    "    print(\"[Creating Learning rate scheduler...]\")\n",
    "    exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    #optimizer = torch.optim.Adam(net.fc.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "   \n",
    "    train_err = np.zeros(num_epochs)\n",
    "    train_loss = np.zeros(num_epochs)\n",
    "    val_err = np.zeros(num_epochs)\n",
    "    val_loss = np.zeros(num_epochs)\n",
    "    \n",
    "    \n",
    "    iters, losses, train_acc, val_acc = [], [], [], []\n",
    "    start_time = time.time()\n",
    "    # training\n",
    "    n=0\n",
    "    for epoch in range(num_epochs):  # loop over the dataset multiple times \n",
    "        exp_lr_scheduler.step()\n",
    "        total_train_loss = 0.0\n",
    "        total_train_err = 0.0\n",
    "\n",
    "        total_epoch = 0\n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "            net.train() #######################################################\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "                      \n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(inputs)\n",
    "           \n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            iters.append(n)\n",
    "           \n",
    "            pred = outputs.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "            total_train_err += pred.ne(labels.view_as(pred)).sum().item()\n",
    "            total_train_loss += loss.item()\n",
    "            total_epoch += len(labels)\n",
    "           \n",
    "            losses.append(float(loss)/batch_size)  \n",
    "            n += 1# compute *average* loss\n",
    "        \n",
    "        train_err[epoch] = float(total_train_err) / total_epoch\n",
    "     \n",
    "        train_loss[epoch] = float(total_train_loss) / (i+1)\n",
    "        net.eval() ###############################################3\n",
    "        val_err[epoch], val_loss[epoch] = evaluate(net, val_loader, criterion)\n",
    "    \n",
    "        print((\"Epoch {}: Train err: {}, Train loss: {}  |\" +\n",
    "               \"Validation err: {} , Validation loss:{} \").format(\n",
    "                   epoch + 1,\n",
    "            \n",
    "                   train_err[epoch],\n",
    "                   train_loss[epoch],\n",
    "                   val_err[epoch],\n",
    "                   val_loss[epoch]\n",
    "                        ))\n",
    "\n",
    "        # Save the current model (checkpoint) to a file\n",
    "        model_path = get_model_name(\"RESNETNEW\", batch_size, learning_rate, epoch)\n",
    "        torch.save(net.state_dict(), model_path)\n",
    "\n",
    "    print('Finished Training')\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(\"Total time elapsed: {:.2f} seconds\".format(elapsed_time))\n",
    "\n",
    "    # Write the train/test loss/err into CSV file for plotting later\n",
    "    epochs = np.arange(1, num_epochs + 1)\n",
    "\n",
    "    np.savetxt(\"{}_train_err.csv\".format(model_path), train_err)\n",
    "    np.savetxt(\"{}_train_loss.csv\".format(model_path), train_loss)\n",
    "    np.savetxt(\"{}_val_err.csv\".format(model_path), val_err)\n",
    "    np.savetxt(\"{}_val_loss.csv\".format(model_path), val_loss)\n",
    "        \n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "wWgZQAM2h7-Q",
    "outputId": "09325493-2f64-4f2c-8ba9-9480c2e8e1d8"
   },
   "outputs": [],
   "source": [
    "model_conv = torchvision.models.resnet18(pretrained=True).to(device)\n",
    "#num_ftrs = model_conv.fc.in_features\n",
    "#model_conv.fc = nn.Linear(num_ftrs, 20).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Code from: https://medium.com/@14prakash/almost-any-image-classification-problem-using-pytorch-i-am-in-love-with-pytorch-26c7aa979ec4\n",
    "\n",
    "## Lets freeze the first few layers. This is done in two stages \n",
    "# Stage-1 Freezing all the layers \n",
    "from torch.optim import lr_scheduler\n",
    "for i, param in model_conv.named_parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Since imagenet as 1000 classes , We need to change our last layer according to the number of classes we have,\n",
    "num_ftrs = model_conv.fc.in_features\n",
    "model_conv.fc = nn.Linear(num_ftrs, 20)\n",
    "\n",
    "# Stage-2 , Freeze all the layers till \"Conv2d_4a_3*3\"\n",
    "ct = []\n",
    "for name, child in model_conv.named_children():\n",
    "    if \"Conv2d_4a_3x3\" in ct:\n",
    "        for params in child.parameters():\n",
    "            params.requires_grad = True\n",
    "    ct.append(name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_conv = nn.DataParallel(model_conv, device_ids=[0, 1])\n",
    "model_conv = model_conv.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 586
    },
    "colab_type": "code",
    "id": "fV5PK6rcina-",
    "outputId": "ae098329-77d9-4416-d49c-17201e3d34dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Creating Learning rate scheduler...]\n",
      "Epoch 1: Train err: 0.72425, Train loss: 2.63157956143643  |Validation err: 0.4785 , Validation loss:2.113164186477661 \n",
      "Epoch 2: Train err: 0.39775, Train loss: 1.8542349668259317  |Validation err: 0.34575 , Validation loss:1.5982104651629925 \n",
      "Epoch 3: Train err: 0.31666666666666665, Train loss: 1.4835851306610919  |Validation err: 0.30375 , Validation loss:1.3485260382294655 \n",
      "Epoch 4: Train err: 0.2925833333333333, Train loss: 1.2892775002946244  |Validation err: 0.28275 , Validation loss:1.2002365235239267 \n",
      "Epoch 5: Train err: 0.2773333333333333, Train loss: 1.16473421898294  |Validation err: 0.27475 , Validation loss:1.1140891183167696 \n",
      "Epoch 6: Train err: 0.2669166666666667, Train loss: 1.0889027828865863  |Validation err: 0.2605 , Validation loss:1.0525336470454931 \n",
      "Epoch 7: Train err: 0.25658333333333333, Train loss: 1.027977055691658  |Validation err: 0.2575 , Validation loss:1.0065363999456167 \n",
      "Epoch 8: Train err: 0.25183333333333335, Train loss: 0.9937765921684022  |Validation err: 0.25475 , Validation loss:1.0014377851039171 \n",
      "Epoch 9: Train err: 0.25075, Train loss: 0.9924637053875213  |Validation err: 0.254 , Validation loss:0.996724933385849 \n",
      "Epoch 10: Train err: 0.24691666666666667, Train loss: 0.9821116943308648  |Validation err: 0.25425 , Validation loss:0.9944707341492176 \n",
      "Epoch 11: Train err: 0.24583333333333332, Train loss: 0.9759104404043644  |Validation err: 0.251 , Validation loss:0.9811059515923262 \n",
      "Epoch 12: Train err: 0.2475, Train loss: 0.9733499986060122  |Validation err: 0.248 , Validation loss:0.9701064340770245 \n",
      "Epoch 13: Train err: 0.24358333333333335, Train loss: 0.9693082945143923  |Validation err: 0.251 , Validation loss:0.982301902025938 \n",
      "Epoch 14: Train err: 0.247, Train loss: 0.9687015084510154  |Validation err: 0.251 , Validation loss:0.9749817457050085 \n",
      "Epoch 15: Train err: 0.24725, Train loss: 0.9671331802581219  |Validation err: 0.2485 , Validation loss:0.9725197907537222 \n",
      "Epoch 16: Train err: 0.24591666666666667, Train loss: 0.9630723361005175  |Validation err: 0.251 , Validation loss:0.9761803951114416 \n",
      "Epoch 17: Train err: 0.24375, Train loss: 0.9642134194678449  |Validation err: 0.24975 , Validation loss:0.9689796753227711 \n",
      "Epoch 18: Train err: 0.2455, Train loss: 0.9653581482298831  |Validation err: 0.24825 , Validation loss:0.9680598899722099 \n",
      "Epoch 19: Train err: 0.2475, Train loss: 0.9639272138159326  |Validation err: 0.25 , Validation loss:0.9639149364084005 \n",
      "Epoch 20: Train err: 0.24708333333333332, Train loss: 0.9641256497261372  |Validation err: 0.25 , Validation loss:0.9780917558819056 \n",
      "Epoch 21: Train err: 0.24391666666666667, Train loss: 0.9602176521686797  |Validation err: 0.2505 , Validation loss:0.9785208720713854 \n",
      "Epoch 22: Train err: 0.24541666666666667, Train loss: 0.965952271476705  |Validation err: 0.24825 , Validation loss:0.9695867244154215 \n",
      "Epoch 23: Train err: 0.248, Train loss: 0.9645650399492142  |Validation err: 0.2495 , Validation loss:0.9730235412716866 \n",
      "Epoch 24: Train err: 0.2435, Train loss: 0.9633595949791848  |Validation err: 0.24975 , Validation loss:0.9790357239544392 \n",
      "Epoch 25: Train err: 0.24333333333333335, Train loss: 0.9586784763539091  |Validation err: 0.25 , Validation loss:0.9753099400550127 \n",
      "Epoch 26: Train err: 0.24408333333333335, Train loss: 0.959161621458987  |Validation err: 0.2475 , Validation loss:0.9779691006988287 \n",
      "Epoch 27: Train err: 0.248, Train loss: 0.9686663664401846  |Validation err: 0.2485 , Validation loss:0.9701114576309919 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-a781fde51375>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainResNetnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_conv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-33-bddfe4603577>\u001b[0m in \u001b[0;36mtrainResNetnew\u001b[0;34m(net, batch_size, learning_rate, num_epochs)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mtotal_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#######################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    629\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatches_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 631\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    632\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatches_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrcvd_idx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_batch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    608\u001b[0m             \u001b[0;31m# need to call `.task_done()` because we don't use `.join()`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    611\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mblock\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rlock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/multiprocessing/connection.py\u001b[0m in \u001b[0;36mrecv_bytes\u001b[0;34m(self, maxlength)\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmaxlength\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmaxlength\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"negative maxlength\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaxlength\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbuf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bad_message_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_recv_bytes\u001b[0;34m(self, maxsize)\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_recv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m         \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstruct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"!i\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmaxsize\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msize\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mmaxsize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_recv\u001b[0;34m(self, size, read)\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0mremaining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m             \u001b[0mchunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremaining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m             \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainResNetnew(model_conv, batch_size=128, learning_rate=0.001, num_epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Creating Learning rate scheduler...]\n",
      "Epoch 1: Train err: 0.24083333333333334, Train loss: 0.9401577977423973  |Validation err: 0.248 , Validation loss:0.9585955888032913 \n",
      "Epoch 2: Train err: 0.23983333333333334, Train loss: 0.9259341042092506  |Validation err: 0.24675 , Validation loss:0.948080588132143 \n",
      "Epoch 3: Train err: 0.236, Train loss: 0.9099200900564802  |Validation err: 0.24375 , Validation loss:0.9350448958575726 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _DataLoaderIter.__del__ at 0x7fc32bb45840>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 717, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/opt/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 713, in _shutdown_workers\n",
      "    w.join()\n",
      "  File \"/opt/anaconda3/lib/python3.7/multiprocessing/process.py\", line 138, in join\n",
      "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "AssertionError: can only join a child process\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Train err: 0.23308333333333334, Train loss: 0.8963869556467584  |Validation err: 0.2415 , Validation loss:0.9245823845267296 \n",
      "Epoch 5: Train err: 0.23125, Train loss: 0.8810925458339934  |Validation err: 0.23825 , Validation loss:0.9046325832605362 \n",
      "Epoch 6: Train err: 0.23141666666666666, Train loss: 0.8730097996427658  |Validation err: 0.24025 , Validation loss:0.8998244181275368 \n",
      "Epoch 7: Train err: 0.22866666666666666, Train loss: 0.8603315289984358  |Validation err: 0.23825 , Validation loss:0.8881871588528156 \n",
      "Epoch 8: Train err: 0.22683333333333333, Train loss: 0.8527368877796416  |Validation err: 0.2405 , Validation loss:0.8935535699129105 \n",
      "Epoch 9: Train err: 0.22858333333333333, Train loss: 0.8506573387917052  |Validation err: 0.23975 , Validation loss:0.8898581750690937 \n",
      "Epoch 10: Train err: 0.22366666666666668, Train loss: 0.8467783661598854  |Validation err: 0.239 , Validation loss:0.890851940959692 \n",
      "Epoch 11: Train err: 0.22508333333333333, Train loss: 0.845298077197785  |Validation err: 0.23725 , Validation loss:0.8835626058280468 \n",
      "Epoch 12: Train err: 0.22391666666666668, Train loss: 0.8452716228809762  |Validation err: 0.2385 , Validation loss:0.885352149605751 \n",
      "Epoch 13: Train err: 0.22466666666666665, Train loss: 0.8445823598415294  |Validation err: 0.23625 , Validation loss:0.8855694569647312 \n",
      "Epoch 14: Train err: 0.22466666666666665, Train loss: 0.8445712797185208  |Validation err: 0.23725 , Validation loss:0.8836778029799461 \n",
      "Epoch 15: Train err: 0.2255, Train loss: 0.8444899548875525  |Validation err: 0.2365 , Validation loss:0.8849632665514946 \n",
      "Epoch 16: Train err: 0.2245, Train loss: 0.8423867162237776  |Validation err: 0.2375 , Validation loss:0.8827205561101437 \n",
      "Epoch 17: Train err: 0.2225, Train loss: 0.8431620293475212  |Validation err: 0.238 , Validation loss:0.8837558813393116 \n",
      "Epoch 18: Train err: 0.22466666666666665, Train loss: 0.8433513882312369  |Validation err: 0.23625 , Validation loss:0.8814907968044281 \n",
      "Epoch 19: Train err: 0.223, Train loss: 0.842460036277771  |Validation err: 0.2375 , Validation loss:0.8832484781742096 \n",
      "Epoch 20: Train err: 0.224, Train loss: 0.842889850443982  |Validation err: 0.237 , Validation loss:0.88451037555933 \n",
      "Finished Training\n",
      "Total time elapsed: 1820.26 seconds\n"
     ]
    }
   ],
   "source": [
    "trainResNetnew(model_conv, batch_size=256, learning_rate=0.001, num_epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Creating Learning rate scheduler...]\n",
      "Epoch 1: Train err: 0.5885833333333333, Train loss: 2.2864720542380153  |Validation err: 0.366 , Validation loss:1.6232504598678104 \n",
      "Epoch 2: Train err: 0.32075, Train loss: 1.431323174466478  |Validation err: 0.2905 , Validation loss:1.2172306416526673 \n",
      "Epoch 3: Train err: 0.2813333333333333, Train loss: 1.1648825787483377  |Validation err: 0.27125 , Validation loss:1.0705701481728327 \n",
      "Epoch 4: Train err: 0.2638333333333333, Train loss: 1.0400146585195622  |Validation err: 0.25775 , Validation loss:0.9707410874820891 \n",
      "Epoch 5: Train err: 0.25508333333333333, Train loss: 0.9670079410710233  |Validation err: 0.25325 , Validation loss:0.9218486169027904 \n",
      "Epoch 6: Train err: 0.24841666666666667, Train loss: 0.9164995123097237  |Validation err: 0.24425 , Validation loss:0.8839426296097892 \n",
      "Epoch 7: Train err: 0.23775, Train loss: 0.8794192056706611  |Validation err: 0.236 , Validation loss:0.8593225971100822 \n",
      "Epoch 8: Train err: 0.2315, Train loss: 0.85029595519634  |Validation err: 0.23275 , Validation loss:0.8487321441135709 \n",
      "Epoch 9: Train err: 0.23258333333333334, Train loss: 0.8512308895587921  |Validation err: 0.2335 , Validation loss:0.8400020703436837 \n",
      "Epoch 10: Train err: 0.23, Train loss: 0.8399463333982102  |Validation err: 0.23325 , Validation loss:0.8440377371651786 \n",
      "Epoch 11: Train err: 0.22958333333333333, Train loss: 0.8367525066467042  |Validation err: 0.23525 , Validation loss:0.839092232878246 \n",
      "Epoch 12: Train err: 0.22925, Train loss: 0.8346811165200904  |Validation err: 0.23325 , Validation loss:0.8295537517184303 \n",
      "Epoch 13: Train err: 0.22933333333333333, Train loss: 0.8336964829805049  |Validation err: 0.2315 , Validation loss:0.8375229362457518 \n",
      "Epoch 14: Train err: 0.22858333333333333, Train loss: 0.8303432708725016  |Validation err: 0.23575 , Validation loss:0.8359855791879078 \n",
      "Epoch 15: Train err: 0.22841666666666666, Train loss: 0.8295551516274189  |Validation err: 0.229 , Validation loss:0.8316764197652302 \n",
      "Epoch 16: Train err: 0.229, Train loss: 0.8291266573236343  |Validation err: 0.23425 , Validation loss:0.8339086496640765 \n",
      "Epoch 17: Train err: 0.227, Train loss: 0.8261897210428055  |Validation err: 0.23125 , Validation loss:0.8271350141555543 \n",
      "Epoch 18: Train err: 0.23208333333333334, Train loss: 0.8327415544301906  |Validation err: 0.23225 , Validation loss:0.8326959591063242 \n",
      "Epoch 19: Train err: 0.22633333333333333, Train loss: 0.8261309630693273  |Validation err: 0.23175 , Validation loss:0.8244339057377407 \n",
      "Epoch 20: Train err: 0.23008333333333333, Train loss: 0.8298359972999451  |Validation err: 0.231 , Validation loss:0.8294174604945712 \n",
      "Finished Training\n",
      "Total time elapsed: 1821.74 seconds\n"
     ]
    }
   ],
   "source": [
    "trainResNetnew(model_conv, batch_size=64, learning_rate=0.0001, num_epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Creating Learning rate scheduler...]\n",
      "Epoch 1: Train err: 0.2515, Train loss: 0.9755658475642509  |Validation err: 0.25525 , Validation loss:0.965672891587019 \n",
      "Epoch 2: Train err: 0.2455, Train loss: 0.9412747589831657  |Validation err: 0.24825 , Validation loss:0.9333129059523344 \n",
      "Epoch 3: Train err: 0.23891666666666667, Train loss: 0.9105711896368798  |Validation err: 0.24675 , Validation loss:0.9135573450475931 \n",
      "Epoch 4: Train err: 0.237, Train loss: 0.8877841589298654  |Validation err: 0.2435 , Validation loss:0.8881979025900364 \n",
      "Epoch 5: Train err: 0.23366666666666666, Train loss: 0.8654963558024549  |Validation err: 0.24325 , Validation loss:0.8797290846705437 \n",
      "Epoch 6: Train err: 0.23075, Train loss: 0.8493758674631727  |Validation err: 0.238 , Validation loss:0.8633231837302446 \n",
      "Epoch 7: Train err: 0.22625, Train loss: 0.8316304531503231  |Validation err: 0.23675 , Validation loss:0.8556522950530052 \n",
      "Epoch 8: Train err: 0.22516666666666665, Train loss: 0.8182203776024758  |Validation err: 0.235 , Validation loss:0.8519888743758202 \n",
      "Epoch 9: Train err: 0.22275, Train loss: 0.8199808857542403  |Validation err: 0.2385 , Validation loss:0.8468113485723734 \n",
      "Epoch 10: Train err: 0.21933333333333332, Train loss: 0.8119321002605113  |Validation err: 0.2345 , Validation loss:0.8498424272984266 \n",
      "Epoch 11: Train err: 0.22, Train loss: 0.8082782727606753  |Validation err: 0.23575 , Validation loss:0.8406600430607796 \n",
      "Epoch 12: Train err: 0.22075, Train loss: 0.8078025687248149  |Validation err: 0.2315 , Validation loss:0.831810349598527 \n",
      "Epoch 13: Train err: 0.21958333333333332, Train loss: 0.8076870707755394  |Validation err: 0.23425 , Validation loss:0.8438062369823456 \n",
      "Epoch 14: Train err: 0.22391666666666668, Train loss: 0.808705376817825  |Validation err: 0.23425 , Validation loss:0.8422526940703392 \n",
      "Epoch 15: Train err: 0.22175, Train loss: 0.8102312829900296  |Validation err: 0.233 , Validation loss:0.8380700424313545 \n",
      "Epoch 16: Train err: 0.22033333333333333, Train loss: 0.8043526334965483  |Validation err: 0.23475 , Validation loss:0.8430392164736986 \n",
      "Epoch 17: Train err: 0.21925, Train loss: 0.8065475288857805  |Validation err: 0.23575 , Validation loss:0.8357130363583565 \n",
      "Epoch 18: Train err: 0.22241666666666668, Train loss: 0.8074847351997456  |Validation err: 0.23125 , Validation loss:0.837080679833889 \n",
      "Epoch 19: Train err: 0.22325, Train loss: 0.8071311724946854  |Validation err: 0.237 , Validation loss:0.8321656808257103 \n",
      "Epoch 20: Train err: 0.2205, Train loss: 0.8076210281950362  |Validation err: 0.238 , Validation loss:0.8459990918636322 \n",
      "Finished Training\n",
      "Total time elapsed: 1823.41 seconds\n"
     ]
    }
   ],
   "source": [
    "trainResNetnew(model_conv, batch_size=128, learning_rate=0.001, num_epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "lj6o3_D7Npxf",
    "outputId": "cadc65a8-1bd3-4594-c312-1e8ee8268ba1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.243, 0.8824597001075745)"
      ]
     },
     "execution_count": 42,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_conv = torchvision.models.resnet18(pretrained=True).to(device)\n",
    "num_ftrs = model_conv.fc.in_features\n",
    "model_conv.fc = nn.Linear(num_ftrs, 20).to(device)\n",
    "model_conv.to(device)\n",
    "model_path = get_model_name(\"RESNET\", batch_size=128, learning_rate=0.0001, epoch=29)\n",
    "state = torch.load(model_path)\n",
    "model_conv.load_state_dict(state)\n",
    "evaluate(model_conv, test_loader, nn.CrossEntropyLoss())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 230
    },
    "colab_type": "code",
    "id": "ycOz0-YPn3TI",
    "outputId": "3b955d99-fb22-48fe-c441-184dff52f865"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train err: 0.39908485856905157, Train loss: 1.536391649474489  |Validation err: 0.2715 , Validation loss:0.9300655294209719 \n",
      "Epoch 2: Train err: 0.24242928452579035, Train loss: 0.7993874010887552  |Validation err: 0.24175 , Validation loss:0.8212614338845015 \n",
      "Epoch 3: Train err: 0.21647254575707156, Train loss: 0.7118265093006986  |Validation err: 0.242 , Validation loss:0.8290862459689379 \n",
      "Epoch 4: Train err: 0.21264559068219635, Train loss: 0.6827567175347754  |Validation err: 0.25575 , Validation loss:0.8893328811973333 \n",
      "Epoch 5: Train err: 0.2103161397670549, Train loss: 0.6960205194797922  |Validation err: 0.241 , Validation loss:0.8453835360705853 \n",
      "Epoch 6: Train err: 0.19767054908485857, Train loss: 0.6480449022764855  |Validation err: 0.2475 , Validation loss:0.8726602476090193 \n",
      "Epoch 7: Train err: 0.19176372712146422, Train loss: 0.6257810846288153  |Validation err: 0.259 , Validation loss:0.9814035221934319 \n",
      "Epoch 8: Train err: 0.18885191347753744, Train loss: 0.6206630043526913  |Validation err: 0.243 , Validation loss:0.8749541193246841 \n",
      "Epoch 9: Train err: 0.1768718801996672, Train loss: 0.5881926455396287  |Validation err: 0.239 , Validation loss:0.8732951749116182 \n",
      "Epoch 10: Train err: 0.1793677204658902, Train loss: 0.5867180282131155  |Validation err: 0.26525 , Validation loss:1.0533912349492311 \n",
      "Epoch 11: Train err: 0.17587354409317804, Train loss: 0.563836913476599  |Validation err: 0.2595 , Validation loss:1.0680122543126345 \n",
      "Epoch 12: Train err: 0.18153078202995007, Train loss: 0.5940985708160603  |Validation err: 0.24925 , Validation loss:0.9659781493246555 \n"
     ]
    }
   ],
   "source": [
    "model_conv = torchvision.models.resnet18(pretrained=True).to(device)\n",
    "num_ftrs = model_conv.fc.in_features\n",
    "model_conv.fc = nn.Linear(num_ftrs, 20).to(device)\n",
    "trainResNet(model_conv, batch_size=128, learning_rate=0.01, num_epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 230
    },
    "colab_type": "code",
    "id": "ycOz0-YPn3TI",
    "outputId": "3b955d99-fb22-48fe-c441-184dff52f865"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train err: 0.4190833333333333, Train loss: 1.6637391020008858  |Validation err: 0.28175 , Validation loss:1.1033667969325232 \n",
      "Epoch 2: Train err: 0.2614166666666667, Train loss: 0.9669207914712581  |Validation err: 0.25175 , Validation loss:0.8986421322065686 \n",
      "Epoch 3: Train err: 0.23383333333333334, Train loss: 0.8246071782517941  |Validation err: 0.2365 , Validation loss:0.8361362767597985 \n",
      "Epoch 4: Train err: 0.21791666666666668, Train loss: 0.752167339654679  |Validation err: 0.242 , Validation loss:0.8110082717168898 \n",
      "Epoch 5: Train err: 0.20683333333333334, Train loss: 0.7075566733454136  |Validation err: 0.23975 , Validation loss:0.7911841055703541 \n",
      "Epoch 6: Train err: 0.20108333333333334, Train loss: 0.680377427250781  |Validation err: 0.23325 , Validation loss:0.787389695171326 \n",
      "Epoch 7: Train err: 0.1935, Train loss: 0.6540768019696499  |Validation err: 0.22575 , Validation loss:0.7578528816737826 \n",
      "Epoch 8: Train err: 0.18633333333333332, Train loss: 0.6344646047404472  |Validation err: 0.22625 , Validation loss:0.7607730466222006 \n",
      "Epoch 9: Train err: 0.18625, Train loss: 0.6172115642656671  |Validation err: 0.22875 , Validation loss:0.7515671385659112 \n",
      "Epoch 10: Train err: 0.17666666666666667, Train loss: 0.5999604707702677  |Validation err: 0.22625 , Validation loss:0.7496110366450416 \n",
      "Epoch 11: Train err: 0.175, Train loss: 0.5836255458441186  |Validation err: 0.22675 , Validation loss:0.7446708896803478 \n",
      "Epoch 12: Train err: 0.17175, Train loss: 0.5731915807787408  |Validation err: 0.231 , Validation loss:0.7533167425602202 \n",
      "Epoch 13: Train err: 0.16783333333333333, Train loss: 0.5636755936640374  |Validation err: 0.22675 , Validation loss:0.7599666175388154 \n",
      "Epoch 14: Train err: 0.16958333333333334, Train loss: 0.5609238653423938  |Validation err: 0.224 , Validation loss:0.7480276633822729 \n",
      "Epoch 15: Train err: 0.17075, Train loss: 0.5628072232642072  |Validation err: 0.2195 , Validation loss:0.7400279608037736 \n",
      "Epoch 16: Train err: 0.16516666666666666, Train loss: 0.5374924833787248  |Validation err: 0.2295 , Validation loss:0.7667611331220657 \n",
      "Epoch 17: Train err: 0.16175, Train loss: 0.5321995818868597  |Validation err: 0.231 , Validation loss:0.7455425172571152 \n",
      "Epoch 18: Train err: 0.15983333333333333, Train loss: 0.5241882661555676  |Validation err: 0.22925 , Validation loss:0.7537411966967205 \n",
      "Epoch 19: Train err: 0.158, Train loss: 0.5175215743799159  |Validation err: 0.2255 , Validation loss:0.7506114627633776 \n",
      "Epoch 20: Train err: 0.15666666666666668, Train loss: 0.5115207038978313  |Validation err: 0.22475 , Validation loss:0.7442641721831428 \n",
      "Epoch 21: Train err: 0.15066666666666667, Train loss: 0.5047578549289957  |Validation err: 0.23075 , Validation loss:0.7671441194557008 \n",
      "Epoch 22: Train err: 0.15508333333333332, Train loss: 0.508582315546401  |Validation err: 0.23175 , Validation loss:0.7713146663847423 \n",
      "Epoch 23: Train err: 0.154, Train loss: 0.500089723537577  |Validation err: 0.2325 , Validation loss:0.7774875608701555 \n",
      "Epoch 24: Train err: 0.14916666666666667, Train loss: 0.49119967380736734  |Validation err: 0.22825 , Validation loss:0.7723233468002744 \n",
      "Epoch 25: Train err: 0.14541666666666667, Train loss: 0.48503459736387783  |Validation err: 0.2315 , Validation loss:0.768277569895699 \n",
      "Epoch 26: Train err: 0.14683333333333334, Train loss: 0.4820100331401571  |Validation err: 0.2315 , Validation loss:0.7933282104749528 \n",
      "Epoch 27: Train err: 0.15158333333333332, Train loss: 0.48929703734973645  |Validation err: 0.23125 , Validation loss:0.7749622284419952 \n",
      "Epoch 28: Train err: 0.14991666666666667, Train loss: 0.47956927730999094  |Validation err: 0.2355 , Validation loss:0.7830503871516575 \n",
      "Epoch 29: Train err: 0.14616666666666667, Train loss: 0.4728436824330624  |Validation err: 0.23125 , Validation loss:0.7790059612856971 \n",
      "Epoch 30: Train err: 0.147, Train loss: 0.4692534268536466  |Validation err: 0.2305 , Validation loss:0.7738765832923707 \n",
      "Epoch 31: Train err: 0.14241666666666666, Train loss: 0.4598484624256479  |Validation err: 0.23575 , Validation loss:0.7886187244975378 \n",
      "Epoch 32: Train err: 0.13983333333333334, Train loss: 0.46043727879828594  |Validation err: 0.2365 , Validation loss:0.7883847833625854 \n",
      "Epoch 33: Train err: 0.14141666666666666, Train loss: 0.46281934854515056  |Validation err: 0.243 , Validation loss:0.8020245461236863 \n",
      "Epoch 34: Train err: 0.14508333333333334, Train loss: 0.4644502119973619  |Validation err: 0.23425 , Validation loss:0.7845551740555536 \n",
      "Epoch 35: Train err: 0.14191666666666666, Train loss: 0.4547491229753545  |Validation err: 0.24375 , Validation loss:0.8164614382244292 \n",
      "Epoch 36: Train err: 0.14133333333333334, Train loss: 0.4513488840708073  |Validation err: 0.2375 , Validation loss:0.7992059332983834 \n",
      "Epoch 37: Train err: 0.14008333333333334, Train loss: 0.4483450251690885  |Validation err: 0.2405 , Validation loss:0.809395801926416 \n",
      "Epoch 38: Train err: 0.14258333333333334, Train loss: 0.44777561914413533  |Validation err: 0.2365 , Validation loss:0.803691237691849 \n",
      "Epoch 39: Train err: 0.1355, Train loss: 0.4416322729530487  |Validation err: 0.23375 , Validation loss:0.7872483020737058 \n",
      "Epoch 40: Train err: 0.13908333333333334, Train loss: 0.4416027956820549  |Validation err: 0.2405 , Validation loss:0.8139584594302707 \n",
      "Epoch 41: Train err: 0.13491666666666666, Train loss: 0.43479231023725046  |Validation err: 0.2345 , Validation loss:0.8067412698079669 \n",
      "Epoch 42: Train err: 0.13558333333333333, Train loss: 0.4365786036595385  |Validation err: 0.23175 , Validation loss:0.8085806781337375 \n",
      "Epoch 43: Train err: 0.13191666666666665, Train loss: 0.42644023566328465  |Validation err: 0.2385 , Validation loss:0.8021517003339435 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.7/multiprocessing/queues.py\", line 242, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/opt/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/opt/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/opt/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-3358b393ffad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mnum_ftrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_conv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel_conv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_ftrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtrainResNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_conv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-54-41cb88300213>\u001b[0m in \u001b[0;36mtrainResNet\u001b[0;34m(net, batch_size, learning_rate, num_epochs)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# get the index of the max log-probability\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m             \u001b[0mtotal_train_err\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mne\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m             \u001b[0mtotal_train_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0mtotal_epoch\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_conv = torchvision.models.resnet18(pretrained=True).to(device)\n",
    "num_ftrs = model_conv.fc.in_features\n",
    "model_conv.fc = nn.Linear(num_ftrs, 20).to(device)\n",
    "trainResNet(model_conv, batch_size= 64, learning_rate=0.001, num_epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modified Resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train err: 0.6362083333333334, Train loss: 2.4103850440979  |Validation err: 0.3955 , Validation loss:1.6493736865028503 \n",
      "Total time elapsed: 197.47 seconds\n",
      "Epoch 2: Train err: 0.3877083333333333, Train loss: 1.435254467010498  |Validation err: 0.32025 , Validation loss:1.153655659584772 \n",
      "Total time elapsed: 363.61 seconds\n",
      "Epoch 3: Train err: 0.34025, Train loss: 1.1759901294708253  |Validation err: 0.29925 , Validation loss:1.0359827298966666 \n",
      "Total time elapsed: 530.30 seconds\n",
      "Epoch 4: Train err: 0.3162083333333333, Train loss: 1.0729954274495443  |Validation err: 0.2845 , Validation loss:0.9626742431095668 \n",
      "Total time elapsed: 696.20 seconds\n",
      "Epoch 5: Train err: 0.30275, Train loss: 1.0231116800308226  |Validation err: 0.26925 , Validation loss:0.9241339852885594 \n",
      "Total time elapsed: 862.93 seconds\n",
      "Epoch 6: Train err: 0.296375, Train loss: 0.9874955123265584  |Validation err: 0.27 , Validation loss:0.8897495799594455 \n",
      "Total time elapsed: 1029.16 seconds\n",
      "Epoch 7: Train err: 0.2835, Train loss: 0.9574542078971863  |Validation err: 0.26225 , Validation loss:0.8822251123095316 \n",
      "Total time elapsed: 1196.89 seconds\n",
      "Epoch 8: Train err: 0.27879166666666666, Train loss: 0.9374438006083171  |Validation err: 0.25825 , Validation loss:0.8605770336257087 \n",
      "Total time elapsed: 1362.79 seconds\n",
      "Epoch 9: Train err: 0.2720416666666667, Train loss: 0.91420108350118  |Validation err: 0.255 , Validation loss:0.8544348908795251 \n",
      "Total time elapsed: 1528.38 seconds\n",
      "Epoch 10: Train err: 0.2685416666666667, Train loss: 0.896206229686737  |Validation err: 0.24875 , Validation loss:0.834414353446355 \n",
      "Total time elapsed: 1695.02 seconds\n",
      "Epoch 11: Train err: 0.26608333333333334, Train loss: 0.8929414238929748  |Validation err: 0.24825 , Validation loss:0.828313516245948 \n",
      "Total time elapsed: 1857.86 seconds\n",
      "Epoch 12: Train err: 0.26158333333333333, Train loss: 0.8746284919579824  |Validation err: 0.2415 , Validation loss:0.8120753443430341 \n",
      "Total time elapsed: 2022.90 seconds\n",
      "Epoch 13: Train err: 0.25958333333333333, Train loss: 0.8774002550443013  |Validation err: 0.2445 , Validation loss:0.8308319913016425 \n",
      "Total time elapsed: 2186.59 seconds\n",
      "Epoch 14: Train err: 0.25833333333333336, Train loss: 0.8627916059494019  |Validation err: 0.24375 , Validation loss:0.8223964676024422 \n",
      "Total time elapsed: 2352.52 seconds\n",
      "Epoch 15: Train err: 0.25333333333333335, Train loss: 0.8553942640622457  |Validation err: 0.247 , Validation loss:0.8253632291914925 \n",
      "Total time elapsed: 2516.97 seconds\n",
      "Epoch 16: Train err: 0.24829166666666666, Train loss: 0.8285496544043223  |Validation err: 0.243 , Validation loss:0.8182020773963322 \n",
      "Total time elapsed: 2682.75 seconds\n",
      "Epoch 17: Train err: 0.24954166666666666, Train loss: 0.8344694437980652  |Validation err: 0.24275 , Validation loss:0.8129675161270868 \n",
      "Total time elapsed: 2847.08 seconds\n",
      "Epoch 18: Train err: 0.247875, Train loss: 0.826703351577123  |Validation err: 0.24475 , Validation loss:0.8100609892890567 \n",
      "Total time elapsed: 3011.73 seconds\n",
      "Epoch 19: Train err: 0.24679166666666666, Train loss: 0.827975222269694  |Validation err: 0.238 , Validation loss:0.7947755290402306 \n",
      "Total time elapsed: 3177.34 seconds\n",
      "Epoch 20: Train err: 0.24270833333333333, Train loss: 0.8144030574162802  |Validation err: 0.24325 , Validation loss:0.7976844547286867 \n",
      "Total time elapsed: 3342.98 seconds\n",
      "Epoch 21: Train err: 0.24379166666666666, Train loss: 0.8101261975765228  |Validation err: 0.2305 , Validation loss:0.7794790258483281 \n",
      "Total time elapsed: 3506.72 seconds\n",
      "Epoch 22: Train err: 0.2385, Train loss: 0.797887768983841  |Validation err: 0.2345 , Validation loss:0.7871460829462323 \n",
      "Total time elapsed: 3671.15 seconds\n",
      "Epoch 23: Train err: 0.23729166666666668, Train loss: 0.797396785736084  |Validation err: 0.23875 , Validation loss:0.7812104944198851 \n",
      "Total time elapsed: 3836.44 seconds\n",
      "Epoch 24: Train err: 0.24020833333333333, Train loss: 0.7873470485210419  |Validation err: 0.24325 , Validation loss:0.79849516399323 \n",
      "Total time elapsed: 4001.86 seconds\n",
      "Epoch 25: Train err: 0.23879166666666668, Train loss: 0.7849618179798126  |Validation err: 0.24175 , Validation loss:0.7990735613164448 \n",
      "Total time elapsed: 4166.71 seconds\n",
      "Epoch 26: Train err: 0.23533333333333334, Train loss: 0.7822091799577077  |Validation err: 0.23525 , Validation loss:0.8039037798132215 \n",
      "Total time elapsed: 4330.30 seconds\n",
      "Epoch 27: Train err: 0.23054166666666667, Train loss: 0.7654861342112224  |Validation err: 0.23425 , Validation loss:0.7727299985431489 \n",
      "Total time elapsed: 4497.07 seconds\n",
      "Epoch 28: Train err: 0.22908333333333333, Train loss: 0.7615598466396332  |Validation err: 0.2265 , Validation loss:0.7746986505531129 \n",
      "Total time elapsed: 4661.41 seconds\n",
      "Epoch 29: Train err: 0.22908333333333333, Train loss: 0.7633946279684702  |Validation err: 0.22875 , Validation loss:0.778226185412634 \n",
      "Total time elapsed: 4826.81 seconds\n",
      "Epoch 30: Train err: 0.22491666666666665, Train loss: 0.7497433741092682  |Validation err: 0.2325 , Validation loss:0.7709433535734812 \n",
      "Total time elapsed: 4992.30 seconds\n",
      "Finished Training\n",
      "Total time elapsed: 4992.30 seconds\n"
     ]
    }
   ],
   "source": [
    "model_resmod = torchvision.models.resnet18(pretrained=True).to(device)\n",
    "num_ftrs = model_resmod.fc.in_features\n",
    "model_resmod.fc = nn.Sequential(nn.Linear(num_ftrs, 216).to(device),nn.ReLU().to(device),\n",
    "                               nn.Linear(216,128).to(device),nn.Dropout(0.5).to(device),nn.ReLU(),\n",
    ",                                nn.Linear(128,20).to(device))\n",
    "#print(model_resmod)\n",
    "trainResNetDA(model_resmod, batch_size= 64, learning_rate=0.0001, num_epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train err: 0.45945833333333336, Train loss: 1.6057487428983053  |Validation err: 0.3065 , Validation loss:1.0178756439496601 \n",
      "Total time elapsed: 165.14 seconds\n",
      "Epoch 2: Train err: 0.31075, Train loss: 1.045701271057129  |Validation err: 0.28075 , Validation loss:0.9289043119975499 \n",
      "Total time elapsed: 331.90 seconds\n",
      "Epoch 3: Train err: 0.28958333333333336, Train loss: 0.9665951420466106  |Validation err: 0.26975 , Validation loss:0.9021504578136262 \n",
      "Total time elapsed: 497.91 seconds\n",
      "Epoch 4: Train err: 0.27516666666666667, Train loss: 0.9121582752068838  |Validation err: 0.257 , Validation loss:0.8486317831372457 \n",
      "Total time elapsed: 664.08 seconds\n",
      "Epoch 5: Train err: 0.26466666666666666, Train loss: 0.8784831972122192  |Validation err: 0.25275 , Validation loss:0.8538888555670542 \n",
      "Total time elapsed: 829.22 seconds\n",
      "Epoch 6: Train err: 0.25366666666666665, Train loss: 0.8446724543571472  |Validation err: 0.24725 , Validation loss:0.8270000309225113 \n",
      "Total time elapsed: 995.57 seconds\n",
      "Epoch 7: Train err: 0.24279166666666666, Train loss: 0.8169224023024241  |Validation err: 0.234 , Validation loss:0.7991153247772701 \n",
      "Total time elapsed: 1159.76 seconds\n",
      "Epoch 8: Train err: 0.240875, Train loss: 0.7939968695640564  |Validation err: 0.23775 , Validation loss:0.8033276815263052 \n",
      "Total time elapsed: 1325.07 seconds\n",
      "Epoch 9: Train err: 0.2315, Train loss: 0.7669816928704579  |Validation err: 0.23825 , Validation loss:0.8026476488226936 \n",
      "Total time elapsed: 1489.26 seconds\n",
      "Epoch 10: Train err: 0.22545833333333334, Train loss: 0.7468232388496399  |Validation err: 0.22575 , Validation loss:0.7859306018503885 \n",
      "Total time elapsed: 1653.71 seconds\n",
      "Epoch 11: Train err: 0.22241666666666668, Train loss: 0.7327226928869883  |Validation err: 0.23075 , Validation loss:0.7769194642702738 \n",
      "Total time elapsed: 1817.70 seconds\n",
      "Epoch 12: Train err: 0.214375, Train loss: 0.7059092893600464  |Validation err: 0.234 , Validation loss:0.77947574664676 \n",
      "Total time elapsed: 1982.18 seconds\n",
      "Epoch 13: Train err: 0.21441666666666667, Train loss: 0.7078271353244782  |Validation err: 0.237 , Validation loss:0.8057430258819035 \n",
      "Total time elapsed: 2146.63 seconds\n",
      "Epoch 14: Train err: 0.20858333333333334, Train loss: 0.6799605338176091  |Validation err: 0.23675 , Validation loss:0.8188812732696533 \n",
      "Total time elapsed: 2312.78 seconds\n",
      "Epoch 15: Train err: 0.20529166666666668, Train loss: 0.6753335579236348  |Validation err: 0.2405 , Validation loss:0.8037735025087992 \n",
      "Total time elapsed: 2478.86 seconds\n",
      "Epoch 16: Train err: 0.19541666666666666, Train loss: 0.6363563475608826  |Validation err: 0.227 , Validation loss:0.7944156139615982 \n",
      "Total time elapsed: 2645.11 seconds\n",
      "Epoch 17: Train err: 0.19441666666666665, Train loss: 0.6345639164447785  |Validation err: 0.23025 , Validation loss:0.8087868936478145 \n",
      "Total time elapsed: 2811.04 seconds\n",
      "Epoch 18: Train err: 0.186125, Train loss: 0.6147669876416524  |Validation err: 0.22675 , Validation loss:0.801096594049817 \n",
      "Total time elapsed: 2976.44 seconds\n",
      "Epoch 19: Train err: 0.18679166666666666, Train loss: 0.6174625810782115  |Validation err: 0.21975 , Validation loss:0.7746162788262443 \n",
      "Total time elapsed: 3141.36 seconds\n",
      "Epoch 20: Train err: 0.185125, Train loss: 0.6029923470815023  |Validation err: 0.23625 , Validation loss:0.8164592252837287 \n",
      "Total time elapsed: 3306.48 seconds\n",
      "Epoch 21: Train err: 0.17433333333333334, Train loss: 0.5829403880437215  |Validation err: 0.22325 , Validation loss:0.7760016369441199 \n",
      "Total time elapsed: 3471.34 seconds\n",
      "Epoch 22: Train err: 0.17870833333333333, Train loss: 0.5867148704131444  |Validation err: 0.2315 , Validation loss:0.8199044995837741 \n",
      "Total time elapsed: 3637.34 seconds\n",
      "Epoch 23: Train err: 0.16754166666666667, Train loss: 0.5586668589909871  |Validation err: 0.2285 , Validation loss:0.809180115896558 \n",
      "Total time elapsed: 3802.45 seconds\n",
      "Epoch 24: Train err: 0.16820833333333332, Train loss: 0.5554275416533152  |Validation err: 0.2295 , Validation loss:0.8382309774557749 \n",
      "Total time elapsed: 3967.74 seconds\n",
      "Epoch 25: Train err: 0.16766666666666666, Train loss: 0.54869335770607  |Validation err: 0.2355 , Validation loss:0.8340498100197504 \n",
      "Total time elapsed: 4133.21 seconds\n",
      "Epoch 26: Train err: 0.165125, Train loss: 0.5402473805745442  |Validation err: 0.22975 , Validation loss:0.8194352700596764 \n",
      "Total time elapsed: 4299.00 seconds\n",
      "Epoch 27: Train err: 0.1605, Train loss: 0.5270575784047444  |Validation err: 0.2315 , Validation loss:0.8380173652891129 \n",
      "Total time elapsed: 4464.98 seconds\n",
      "Epoch 28: Train err: 0.15520833333333334, Train loss: 0.5210281577110291  |Validation err: 0.23075 , Validation loss:0.84148656470435 \n",
      "Total time elapsed: 4630.57 seconds\n",
      "Epoch 29: Train err: 0.15916666666666668, Train loss: 0.5260012015501658  |Validation err: 0.236 , Validation loss:0.8318525349336957 \n",
      "Total time elapsed: 4796.26 seconds\n",
      "Epoch 30: Train err: 0.151875, Train loss: 0.5033437372048696  |Validation err: 0.22675 , Validation loss:0.832812507947286 \n",
      "Total time elapsed: 4961.99 seconds\n",
      "Finished Training\n",
      "Total time elapsed: 4961.99 seconds\n"
     ]
    }
   ],
   "source": [
    "model_resmod = torchvision.models.resnet18(pretrained=True).to(device)\n",
    "num_ftrs = model_resmod.fc.in_features\n",
    "model_resmod.fc = nn.Sequential(nn.Linear(num_ftrs, 216).to(device),nn.ReLU().to(device),\n",
    "                               nn.Linear(216,128).to(device),nn.Dropout(0.5).to(device),nn.ReLU(),\n",
    "                                nn.Linear(128,20).to(device))\n",
    "#print(model_resmod)\n",
    "trainResNetDA(model_resmod, batch_size= 64, learning_rate=0.0005, num_epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train err: 0.83725, Train loss: 2.8770094272938183  |Validation err: 0.68375 , Validation loss:2.711021544441344 \n",
      "Total time elapsed: 89.88 seconds\n",
      "Epoch 2: Train err: 0.5739166666666666, Train loss: 2.4888841839546854  |Validation err: 0.50425 , Validation loss:2.244612799750434 \n",
      "Total time elapsed: 181.25 seconds\n",
      "Epoch 3: Train err: 0.453, Train loss: 2.018533447321425  |Validation err: 0.417 , Validation loss:1.8162292469115484 \n",
      "Total time elapsed: 272.73 seconds\n",
      "Epoch 4: Train err: 0.39158333333333334, Train loss: 1.6644351767732741  |Validation err: 0.3845 , Validation loss:1.5487727759376404 \n",
      "Total time elapsed: 363.96 seconds\n",
      "Epoch 5: Train err: 0.36441666666666667, Train loss: 1.4300847040845992  |Validation err: 0.36625 , Validation loss:1.3757558304166038 \n",
      "Total time elapsed: 454.42 seconds\n",
      "Epoch 6: Train err: 0.34358333333333335, Train loss: 1.295385741807045  |Validation err: 0.3405 , Validation loss:1.2574809638280717 \n",
      "Total time elapsed: 544.96 seconds\n",
      "Epoch 7: Train err: 0.3219166666666667, Train loss: 1.1916431778288903  |Validation err: 0.32425 , Validation loss:1.176078876805684 \n",
      "Total time elapsed: 635.25 seconds\n",
      "Epoch 8: Train err: 0.3139166666666667, Train loss: 1.1272660034134032  |Validation err: 0.31925 , Validation loss:1.1206731654348827 \n",
      "Total time elapsed: 725.65 seconds\n",
      "Epoch 9: Train err: 0.3001666666666667, Train loss: 1.0733892207450055  |Validation err: 0.30575 , Validation loss:1.068755254859016 \n",
      "Total time elapsed: 816.86 seconds\n",
      "Epoch 10: Train err: 0.2896666666666667, Train loss: 1.0232249926379386  |Validation err: 0.2965 , Validation loss:1.0325727226242187 \n",
      "Total time elapsed: 907.52 seconds\n",
      "Epoch 11: Train err: 0.28625, Train loss: 0.9902135222516162  |Validation err: 0.29525 , Validation loss:1.007802911221035 \n",
      "Total time elapsed: 998.27 seconds\n",
      "Epoch 12: Train err: 0.2755, Train loss: 0.9581105946860415  |Validation err: 0.28775 , Validation loss:0.9947681568917774 \n",
      "Total time elapsed: 1089.20 seconds\n",
      "Epoch 13: Train err: 0.26975, Train loss: 0.9320207009924218  |Validation err: 0.294 , Validation loss:0.9790703673211355 \n",
      "Total time elapsed: 1179.06 seconds\n",
      "Epoch 14: Train err: 0.26808333333333334, Train loss: 0.9216768082152021  |Validation err: 0.29075 , Validation loss:0.9663982088603671 \n",
      "Total time elapsed: 1270.50 seconds\n",
      "Epoch 15: Train err: 0.2649166666666667, Train loss: 0.8990249491118371  |Validation err: 0.27275 , Validation loss:0.916621258334508 \n",
      "Total time elapsed: 1360.61 seconds\n",
      "Epoch 16: Train err: 0.26066666666666666, Train loss: 0.8800122560338771  |Validation err: 0.27475 , Validation loss:0.9299456410937839 \n",
      "Total time elapsed: 1451.52 seconds\n",
      "Epoch 17: Train err: 0.25366666666666665, Train loss: 0.8667643059124338  |Validation err: 0.2685 , Validation loss:0.9098259768788777 \n",
      "Total time elapsed: 1542.94 seconds\n",
      "Epoch 18: Train err: 0.25225, Train loss: 0.8487512929008362  |Validation err: 0.26775 , Validation loss:0.9154440495702956 \n",
      "Total time elapsed: 1633.32 seconds\n",
      "Epoch 19: Train err: 0.25183333333333335, Train loss: 0.8451376733310679  |Validation err: 0.2655 , Validation loss:0.885882350187453 \n",
      "Total time elapsed: 1724.65 seconds\n",
      "Epoch 20: Train err: 0.24475, Train loss: 0.8235340834932124  |Validation err: 0.26725 , Validation loss:0.8890180720223321 \n",
      "Total time elapsed: 1815.09 seconds\n",
      "Epoch 21: Train err: 0.24183333333333334, Train loss: 0.807567057298853  |Validation err: 0.268 , Validation loss:0.8985252815579611 \n",
      "Total time elapsed: 1905.95 seconds\n",
      "Epoch 22: Train err: 0.24391666666666667, Train loss: 0.8088147820944481  |Validation err: 0.26475 , Validation loss:0.8864471240649148 \n",
      "Total time elapsed: 1995.95 seconds\n",
      "Epoch 23: Train err: 0.23825, Train loss: 0.7976095586064014  |Validation err: 0.27025 , Validation loss:0.8913130372289627 \n",
      "Total time elapsed: 2085.57 seconds\n",
      "Epoch 24: Train err: 0.23783333333333334, Train loss: 0.7889538087109302  |Validation err: 0.25725 , Validation loss:0.871070973456852 \n",
      "Total time elapsed: 2175.82 seconds\n",
      "Epoch 25: Train err: 0.23291666666666666, Train loss: 0.7782624887025102  |Validation err: 0.26125 , Validation loss:0.8747948067528861 \n",
      "Total time elapsed: 2266.37 seconds\n",
      "Epoch 26: Train err: 0.22975, Train loss: 0.7773430827767291  |Validation err: 0.25975 , Validation loss:0.8718955649269952 \n",
      "Total time elapsed: 2356.93 seconds\n",
      "Epoch 27: Train err: 0.23266666666666666, Train loss: 0.7692248990878146  |Validation err: 0.26025 , Validation loss:0.8567967963597131 \n",
      "Total time elapsed: 2446.92 seconds\n",
      "Epoch 28: Train err: 0.23008333333333333, Train loss: 0.7676694724471012  |Validation err: 0.25275 , Validation loss:0.8458416575477237 \n",
      "Total time elapsed: 2538.96 seconds\n",
      "Epoch 29: Train err: 0.22825, Train loss: 0.7565755322575569  |Validation err: 0.25425 , Validation loss:0.8659469863725087 \n",
      "Total time elapsed: 2629.30 seconds\n",
      "Epoch 30: Train err: 0.22291666666666668, Train loss: 0.7448702051918558  |Validation err: 0.257 , Validation loss:0.8538780912520394 \n",
      "Total time elapsed: 2718.49 seconds\n",
      "Finished Training\n",
      "Total time elapsed: 2718.49 seconds\n"
     ]
    }
   ],
   "source": [
    "model_resmod = torchvision.models.resnet18(pretrained=True).to(device)\n",
    "num_ftrs = model_resmod.fc.in_features\n",
    "model_resmod.fc = nn.Sequential(nn.Linear(num_ftrs, 216).to(device),nn.ReLU().to(device),\n",
    "                               nn.Linear(216,128).to(device),nn.Dropout(0.5).to(device),nn.ReLU(),\n",
    "                                nn.Linear(128,20).to(device))\n",
    "#print(model_resmod)\n",
    "trainResNet(model_resmod, batch_size= 64, learning_rate=0.00005, num_epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train err: 0.72975, Train loss: 2.7036956802327583  |Validation err: 0.51825 , Validation loss:2.281234934216454 \n",
      "Total time elapsed: 90.36 seconds\n",
      "Epoch 2: Train err: 0.4374166666666667, Train loss: 1.8774740905203717  |Validation err: 0.39425 , Validation loss:1.5594886560288688 \n",
      "Total time elapsed: 180.58 seconds\n",
      "Epoch 3: Train err: 0.36125, Train loss: 1.3882280721309337  |Validation err: 0.34375 , Validation loss:1.2676766770226615 \n",
      "Total time elapsed: 271.57 seconds\n",
      "Epoch 4: Train err: 0.3259166666666667, Train loss: 1.1789664280541399  |Validation err: 0.31925 , Validation loss:1.1325273778703477 \n",
      "Total time elapsed: 361.63 seconds\n",
      "Epoch 5: Train err: 0.30366666666666664, Train loss: 1.0534963227332907  |Validation err: 0.31325 , Validation loss:1.060103484562465 \n",
      "Total time elapsed: 452.56 seconds\n",
      "Epoch 6: Train err: 0.2935, Train loss: 0.9952764872540819  |Validation err: 0.289 , Validation loss:1.000165637523409 \n",
      "Total time elapsed: 544.26 seconds\n",
      "Epoch 7: Train err: 0.2743333333333333, Train loss: 0.9376899552472094  |Validation err: 0.2875 , Validation loss:0.9736455518101889 \n",
      "Total time elapsed: 634.18 seconds\n",
      "Epoch 8: Train err: 0.2695, Train loss: 0.9089199003387005  |Validation err: 0.2835 , Validation loss:0.9438493573476398 \n",
      "Total time elapsed: 725.01 seconds\n",
      "Epoch 9: Train err: 0.25516666666666665, Train loss: 0.8763803639944564  |Validation err: 0.27525 , Validation loss:0.9152476532118661 \n",
      "Total time elapsed: 815.79 seconds\n",
      "Epoch 10: Train err: 0.2509166666666667, Train loss: 0.8467950473757501  |Validation err: 0.2615 , Validation loss:0.8904576301574707 \n",
      "Total time elapsed: 905.82 seconds\n",
      "Epoch 11: Train err: 0.24475, Train loss: 0.8237518703049802  |Validation err: 0.269 , Validation loss:0.8846377758752733 \n",
      "Total time elapsed: 997.80 seconds\n",
      "Epoch 12: Train err: 0.24233333333333335, Train loss: 0.8050611095225557  |Validation err: 0.26025 , Validation loss:0.8846735083867633 \n",
      "Total time elapsed: 1087.54 seconds\n",
      "Epoch 13: Train err: 0.23558333333333334, Train loss: 0.7891656677139566  |Validation err: 0.266 , Validation loss:0.8818151203412858 \n",
      "Total time elapsed: 1177.78 seconds\n",
      "Epoch 14: Train err: 0.23283333333333334, Train loss: 0.7800414910975922  |Validation err: 0.26175 , Validation loss:0.8767500973883129 \n",
      "Total time elapsed: 1268.43 seconds\n",
      "Epoch 15: Train err: 0.2325, Train loss: 0.7688626242761917  |Validation err: 0.247 , Validation loss:0.8306028767237588 \n",
      "Total time elapsed: 1360.12 seconds\n",
      "Epoch 16: Train err: 0.22716666666666666, Train loss: 0.7490772023162944  |Validation err: 0.25475 , Validation loss:0.8584585833171058 \n",
      "Total time elapsed: 1450.48 seconds\n",
      "Epoch 17: Train err: 0.22241666666666668, Train loss: 0.7424757748525194  |Validation err: 0.2535 , Validation loss:0.8391300652708326 \n",
      "Total time elapsed: 1540.40 seconds\n",
      "Epoch 18: Train err: 0.21766666666666667, Train loss: 0.7272266041725239  |Validation err: 0.2495 , Validation loss:0.8535930665712508 \n",
      "Total time elapsed: 1632.30 seconds\n",
      "Epoch 19: Train err: 0.22258333333333333, Train loss: 0.7267814673324848  |Validation err: 0.2475 , Validation loss:0.8304494467992631 \n",
      "Total time elapsed: 1723.03 seconds\n",
      "Epoch 20: Train err: 0.21525, Train loss: 0.708481879785974  |Validation err: 0.2535 , Validation loss:0.8308204147550795 \n",
      "Total time elapsed: 1813.55 seconds\n",
      "Epoch 21: Train err: 0.20783333333333334, Train loss: 0.6927969414503017  |Validation err: 0.2515 , Validation loss:0.8490845825937059 \n",
      "Total time elapsed: 1903.54 seconds\n",
      "Epoch 22: Train err: 0.21283333333333335, Train loss: 0.693756565610145  |Validation err: 0.2485 , Validation loss:0.8416842401973785 \n",
      "Total time elapsed: 1995.03 seconds\n",
      "Epoch 23: Train err: 0.21075, Train loss: 0.6872450333643467  |Validation err: 0.256 , Validation loss:0.8511278837446182 \n",
      "Total time elapsed: 2085.23 seconds\n",
      "Epoch 24: Train err: 0.20633333333333334, Train loss: 0.6770046537226819  |Validation err: 0.24425 , Validation loss:0.8387391297590165 \n",
      "Total time elapsed: 2175.85 seconds\n",
      "Epoch 25: Train err: 0.20275, Train loss: 0.6661812888180956  |Validation err: 0.24575 , Validation loss:0.8345146827281468 \n",
      "Total time elapsed: 2265.99 seconds\n",
      "Epoch 26: Train err: 0.201, Train loss: 0.6675824383471874  |Validation err: 0.25075 , Validation loss:0.8424394949088021 \n",
      "Total time elapsed: 2355.59 seconds\n",
      "Epoch 27: Train err: 0.1995, Train loss: 0.6613645480668291  |Validation err: 0.248 , Validation loss:0.8250399827957153 \n",
      "Total time elapsed: 2445.41 seconds\n",
      "Epoch 28: Train err: 0.20033333333333334, Train loss: 0.6594782701515137  |Validation err: 0.2465 , Validation loss:0.8261713030792418 \n",
      "Total time elapsed: 2536.66 seconds\n",
      "Epoch 29: Train err: 0.19683333333333333, Train loss: 0.649186299360813  |Validation err: 0.24325 , Validation loss:0.8302812803359259 \n",
      "Total time elapsed: 2628.32 seconds\n",
      "Epoch 30: Train err: 0.19383333333333333, Train loss: 0.63388388619778  |Validation err: 0.24975 , Validation loss:0.843680721426767 \n",
      "Total time elapsed: 2719.62 seconds\n",
      "Finished Training\n",
      "Total time elapsed: 2719.62 seconds\n"
     ]
    }
   ],
   "source": [
    "model_resmod = torchvision.models.resnet18(pretrained=True).to(device)\n",
    "num_ftrs = model_resmod.fc.in_features\n",
    "model_resmod.fc = nn.Sequential(nn.Linear(num_ftrs, 216).to(device),nn.ReLU().to(device),\n",
    "                               nn.Linear(216,128).to(device),nn.Dropout(0.5).to(device),nn.ReLU(),\n",
    "                                nn.Linear(128,20).to(device))\n",
    "#print(model_resmod)\n",
    "trainResNet(model_resmod, batch_size= 64, learning_rate=0.0001, num_epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train err: 0.5026666666666667, Train loss: 1.8296967342178871  |Validation err: 0.338 , Validation loss:1.1457969811227586 \n",
      "Total time elapsed: 120.10 seconds\n",
      "Epoch 2: Train err: 0.31733333333333336, Train loss: 1.0567598025849525  |Validation err: 0.31025 , Validation loss:1.0112297393026806 \n",
      "Total time elapsed: 213.07 seconds\n",
      "Epoch 3: Train err: 0.274, Train loss: 0.9138582894142638  |Validation err: 0.28625 , Validation loss:0.9523319015427242 \n",
      "Total time elapsed: 303.69 seconds\n",
      "Epoch 4: Train err: 0.24858333333333332, Train loss: 0.835197775129308  |Validation err: 0.2695 , Validation loss:0.9064487010713608 \n",
      "Total time elapsed: 394.43 seconds\n",
      "Epoch 5: Train err: 0.23841666666666667, Train loss: 0.7809689485646308  |Validation err: 0.27175 , Validation loss:0.89109867811203 \n",
      "Total time elapsed: 484.87 seconds\n",
      "Epoch 6: Train err: 0.23016666666666666, Train loss: 0.7588379240416466  |Validation err: 0.259 , Validation loss:0.8772706871940976 \n",
      "Total time elapsed: 574.65 seconds\n",
      "Epoch 7: Train err: 0.22016666666666668, Train loss: 0.7238949226572159  |Validation err: 0.24525 , Validation loss:0.8319692124450018 \n",
      "Total time elapsed: 665.18 seconds\n",
      "Epoch 8: Train err: 0.21075, Train loss: 0.6969706600016736  |Validation err: 0.257 , Validation loss:0.8629349848580739 \n",
      "Total time elapsed: 756.64 seconds\n",
      "Epoch 9: Train err: 0.20441666666666666, Train loss: 0.6672014140702308  |Validation err: 0.25175 , Validation loss:0.8581210251838441 \n",
      "Total time elapsed: 848.03 seconds\n",
      "Epoch 10: Train err: 0.1965, Train loss: 0.6424698939031743  |Validation err: 0.261 , Validation loss:0.8843723024640765 \n",
      "Total time elapsed: 939.39 seconds\n",
      "Epoch 11: Train err: 0.18733333333333332, Train loss: 0.6166837067997202  |Validation err: 0.251 , Validation loss:0.8494037862807985 \n",
      "Total time elapsed: 1030.38 seconds\n",
      "Epoch 12: Train err: 0.18583333333333332, Train loss: 0.608562059066397  |Validation err: 0.253 , Validation loss:0.8469244024110218 \n",
      "Total time elapsed: 1121.22 seconds\n",
      "Epoch 13: Train err: 0.17491666666666666, Train loss: 0.5709494661460531  |Validation err: 0.24525 , Validation loss:0.8548933314898658 \n",
      "Total time elapsed: 1211.53 seconds\n",
      "Epoch 14: Train err: 0.18091666666666667, Train loss: 0.570581556476177  |Validation err: 0.25225 , Validation loss:0.8598855520997729 \n",
      "Total time elapsed: 1301.62 seconds\n",
      "Epoch 15: Train err: 0.17383333333333334, Train loss: 0.5491538895888531  |Validation err: 0.23825 , Validation loss:0.839897754646483 \n",
      "Total time elapsed: 1393.62 seconds\n",
      "Epoch 16: Train err: 0.15958333333333333, Train loss: 0.5024266808908037  |Validation err: 0.24275 , Validation loss:0.8493698911061363 \n",
      "Total time elapsed: 1482.90 seconds\n",
      "Epoch 17: Train err: 0.15491666666666667, Train loss: 0.4936778112611872  |Validation err: 0.24525 , Validation loss:0.829520884487364 \n",
      "Total time elapsed: 1573.43 seconds\n",
      "Epoch 18: Train err: 0.15208333333333332, Train loss: 0.4773886330584262  |Validation err: 0.2415 , Validation loss:0.8615682475150578 \n",
      "Total time elapsed: 1665.80 seconds\n",
      "Epoch 19: Train err: 0.14458333333333334, Train loss: 0.45118531402438244  |Validation err: 0.254 , Validation loss:0.9083805912070804 \n",
      "Total time elapsed: 1757.83 seconds\n",
      "Epoch 20: Train err: 0.13533333333333333, Train loss: 0.4297076492233479  |Validation err: 0.2385 , Validation loss:0.8316077987353007 \n",
      "Total time elapsed: 1847.78 seconds\n",
      "Epoch 21: Train err: 0.12833333333333333, Train loss: 0.4048188384226028  |Validation err: 0.24375 , Validation loss:0.9136797028874594 \n",
      "Total time elapsed: 1938.63 seconds\n",
      "Epoch 22: Train err: 0.13041666666666665, Train loss: 0.3979672151518629  |Validation err: 0.251 , Validation loss:0.9341272569838024 \n",
      "Total time elapsed: 2030.64 seconds\n",
      "Epoch 23: Train err: 0.12033333333333333, Train loss: 0.37725600417941174  |Validation err: 0.242 , Validation loss:0.8778127403486342 \n",
      "Total time elapsed: 2120.82 seconds\n",
      "Epoch 24: Train err: 0.11366666666666667, Train loss: 0.3538207439507576  |Validation err: 0.2415 , Validation loss:0.8835048088951717 \n",
      "Total time elapsed: 2211.34 seconds\n",
      "Epoch 25: Train err: 0.11041666666666666, Train loss: 0.33803655016929546  |Validation err: 0.2445 , Validation loss:0.8909943188939776 \n",
      "Total time elapsed: 2302.24 seconds\n",
      "Epoch 26: Train err: 0.09875, Train loss: 0.31054309731785285  |Validation err: 0.23625 , Validation loss:0.9404971135987176 \n",
      "Total time elapsed: 2393.99 seconds\n",
      "Epoch 27: Train err: 0.09866666666666667, Train loss: 0.30551320924720865  |Validation err: 0.23975 , Validation loss:0.9497970957604666 \n",
      "Total time elapsed: 2483.37 seconds\n",
      "Epoch 28: Train err: 0.09591666666666666, Train loss: 0.28491709290191214  |Validation err: 0.24175 , Validation loss:0.935400757997755 \n",
      "Total time elapsed: 2573.76 seconds\n",
      "Epoch 29: Train err: 0.09075, Train loss: 0.27443818851037227  |Validation err: 0.2435 , Validation loss:0.9634916753995986 \n",
      "Total time elapsed: 2664.31 seconds\n",
      "Epoch 30: Train err: 0.08683333333333333, Train loss: 0.25871077944782184  |Validation err: 0.2375 , Validation loss:0.9455842082462613 \n",
      "Total time elapsed: 2754.55 seconds\n",
      "Finished Training\n",
      "Total time elapsed: 2754.55 seconds\n"
     ]
    }
   ],
   "source": [
    "model_resmod = torchvision.models.resnet18(pretrained=True).to(device)\n",
    "num_ftrs = model_resmod.fc.in_features\n",
    "model_resmod.fc = nn.Sequential(nn.Linear(num_ftrs, 216).to(device),nn.ReLU().to(device),\n",
    "                               nn.Linear(216,128).to(device),nn.Dropout(0.5).to(device),nn.ReLU(),\n",
    "                                nn.Linear(128,20).to(device))\n",
    "#print(model_resmod)\n",
    "trainResNet(model_resmod, batch_size= 64, learning_rate=0.0005, num_epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train err: 0.8006666666666666, Train loss: 2.8293524853726653  |Validation err: 0.6445 , Validation loss:2.602300748229027 \n",
      "Total time elapsed: 90.60 seconds\n",
      "Epoch 2: Train err: 0.5210833333333333, Train loss: 2.2931180215896445  |Validation err: 0.45575 , Validation loss:1.985670655965805 \n",
      "Total time elapsed: 182.07 seconds\n",
      "Epoch 3: Train err: 0.40941666666666665, Train loss: 1.7400200924974807  |Validation err: 0.38875 , Validation loss:1.5587487891316414 \n",
      "Total time elapsed: 271.31 seconds\n",
      "Epoch 4: Train err: 0.35991666666666666, Train loss: 1.4129762066171525  |Validation err: 0.3515 , Validation loss:1.320157129317522 \n",
      "Total time elapsed: 362.76 seconds\n",
      "Epoch 5: Train err: 0.33, Train loss: 1.227053221235884  |Validation err: 0.332 , Validation loss:1.1917029991745949 \n",
      "Total time elapsed: 453.69 seconds\n",
      "Epoch 6: Train err: 0.31825, Train loss: 1.1217031092085736  |Validation err: 0.312 , Validation loss:1.1080899387598038 \n",
      "Total time elapsed: 544.48 seconds\n",
      "Epoch 7: Train err: 0.2975833333333333, Train loss: 1.0496781519118776  |Validation err: 0.2925 , Validation loss:1.0445854552090168 \n",
      "Total time elapsed: 633.99 seconds\n",
      "Epoch 8: Train err: 0.28458333333333335, Train loss: 0.9892330651587629  |Validation err: 0.293 , Validation loss:1.0062508545815945 \n",
      "Total time elapsed: 723.89 seconds\n",
      "Epoch 9: Train err: 0.2786666666666667, Train loss: 0.9509448960740515  |Validation err: 0.281 , Validation loss:0.9560980498790741 \n",
      "Total time elapsed: 815.16 seconds\n",
      "Epoch 10: Train err: 0.2653333333333333, Train loss: 0.9104962837188801  |Validation err: 0.274 , Validation loss:0.9465427957475185 \n",
      "Total time elapsed: 905.06 seconds\n",
      "Epoch 11: Train err: 0.26008333333333333, Train loss: 0.8815524635162759  |Validation err: 0.27125 , Validation loss:0.9127640444785357 \n",
      "Total time elapsed: 994.72 seconds\n",
      "Epoch 12: Train err: 0.25325, Train loss: 0.8655902546771029  |Validation err: 0.267 , Validation loss:0.900487631559372 \n",
      "Total time elapsed: 1086.15 seconds\n",
      "Epoch 13: Train err: 0.24808333333333332, Train loss: 0.8401407986245257  |Validation err: 0.26925 , Validation loss:0.9159817062318325 \n",
      "Total time elapsed: 1177.90 seconds\n",
      "Epoch 14: Train err: 0.248, Train loss: 0.8211682825646502  |Validation err: 0.26725 , Validation loss:0.8796484749764204 \n",
      "Total time elapsed: 1267.38 seconds\n",
      "Epoch 15: Train err: 0.23858333333333334, Train loss: 0.8092998443765843  |Validation err: 0.2625 , Validation loss:0.8723384663462639 \n",
      "Total time elapsed: 1357.48 seconds\n",
      "Epoch 16: Train err: 0.24058333333333334, Train loss: 0.7914789858016562  |Validation err: 0.25875 , Validation loss:0.8602552209049463 \n",
      "Total time elapsed: 1448.36 seconds\n",
      "Epoch 17: Train err: 0.23325, Train loss: 0.7853714842745598  |Validation err: 0.258 , Validation loss:0.8619401566684246 \n",
      "Total time elapsed: 1538.10 seconds\n",
      "Epoch 18: Train err: 0.22808333333333333, Train loss: 0.7652963989592613  |Validation err: 0.258 , Validation loss:0.8543222919106483 \n",
      "Total time elapsed: 1629.86 seconds\n",
      "Epoch 19: Train err: 0.22741666666666666, Train loss: 0.7587364290622954  |Validation err: 0.25125 , Validation loss:0.8468025699257851 \n",
      "Total time elapsed: 1720.36 seconds\n",
      "Epoch 20: Train err: 0.22175, Train loss: 0.7418501516605945  |Validation err: 0.24525 , Validation loss:0.8327343687415123 \n",
      "Total time elapsed: 1811.47 seconds\n",
      "Epoch 21: Train err: 0.21766666666666667, Train loss: 0.7369783431925672  |Validation err: 0.251 , Validation loss:0.8496519364416599 \n",
      "Total time elapsed: 1902.47 seconds\n",
      "Epoch 22: Train err: 0.22041666666666668, Train loss: 0.7349519203317926  |Validation err: 0.24825 , Validation loss:0.835718696936965 \n",
      "Total time elapsed: 1993.04 seconds\n",
      "Epoch 23: Train err: 0.21283333333333335, Train loss: 0.7197062620457183  |Validation err: 0.2545 , Validation loss:0.8536172658205032 \n",
      "Total time elapsed: 2084.44 seconds\n",
      "Epoch 24: Train err: 0.21416666666666667, Train loss: 0.7074539747009886  |Validation err: 0.247 , Validation loss:0.8249110709875822 \n",
      "Total time elapsed: 2174.09 seconds\n",
      "Epoch 25: Train err: 0.211, Train loss: 0.6999505479285058  |Validation err: 0.2515 , Validation loss:0.830805754289031 \n",
      "Total time elapsed: 2264.53 seconds\n",
      "Epoch 26: Train err: 0.20416666666666666, Train loss: 0.6960168303327358  |Validation err: 0.2475 , Validation loss:0.8335003405809402 \n",
      "Total time elapsed: 2356.19 seconds\n",
      "Epoch 27: Train err: 0.20575, Train loss: 0.6831736279294846  |Validation err: 0.25075 , Validation loss:0.840442193672061 \n",
      "Total time elapsed: 2446.79 seconds\n",
      "Epoch 28: Train err: 0.2035, Train loss: 0.674764575476342  |Validation err: 0.24275 , Validation loss:0.8212834093719721 \n",
      "Total time elapsed: 2537.94 seconds\n",
      "Epoch 29: Train err: 0.20208333333333334, Train loss: 0.6734315635676079  |Validation err: 0.24225 , Validation loss:0.834061473608017 \n",
      "Total time elapsed: 2629.06 seconds\n",
      "Epoch 30: Train err: 0.19908333333333333, Train loss: 0.6655820310115814  |Validation err: 0.249 , Validation loss:0.826890729367733 \n",
      "Total time elapsed: 2720.92 seconds\n",
      "Finished Training\n",
      "Total time elapsed: 2720.92 seconds\n"
     ]
    }
   ],
   "source": [
    "model_resmod = torchvision.models.resnet18(pretrained=True).to(device)\n",
    "num_ftrs = model_resmod.fc.in_features\n",
    "model_resmod.fc = nn.Sequential(nn.Linear(num_ftrs, 216).to(device),nn.ReLU().to(device),\n",
    "                               nn.Linear(216,128).to(device),nn.Dropout(0.5).to(device),nn.ReLU(),\n",
    "                                nn.Linear(128,20).to(device))\n",
    "#print(model_resmod)\n",
    "trainResNet(model_resmod, batch_size= 128, learning_rate=0.0001, num_epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train err: 0.9435, Train loss: 2.996792098309131  |Validation err: 0.9345 , Validation loss:2.9902035742998123 \n",
      "Total time elapsed: 113.82 seconds\n",
      "Epoch 2: Train err: 0.9305833333333333, Train loss: 2.980627658519339  |Validation err: 0.9175 , Validation loss:2.9709274917840958 \n",
      "Total time elapsed: 206.23 seconds\n",
      "Epoch 3: Train err: 0.89925, Train loss: 2.9599617795741304  |Validation err: 0.88825 , Validation loss:2.946491815149784 \n",
      "Total time elapsed: 296.02 seconds\n",
      "Epoch 4: Train err: 0.87525, Train loss: 2.9392296786003924  |Validation err: 0.8565 , Validation loss:2.925736367702484 \n",
      "Total time elapsed: 387.07 seconds\n",
      "Epoch 5: Train err: 0.8485, Train loss: 2.9149233204253178  |Validation err: 0.82725 , Validation loss:2.8978857323527336 \n",
      "Total time elapsed: 479.95 seconds\n",
      "Epoch 6: Train err: 0.8155833333333333, Train loss: 2.885659085943344  |Validation err: 0.80425 , Validation loss:2.8678610250353813 \n",
      "Total time elapsed: 569.98 seconds\n",
      "Epoch 7: Train err: 0.7891666666666667, Train loss: 2.8509891058536287  |Validation err: 0.7875 , Validation loss:2.833161883056164 \n",
      "Total time elapsed: 660.56 seconds\n",
      "Epoch 8: Train err: 0.76125, Train loss: 2.8142900111827442  |Validation err: 0.7495 , Validation loss:2.7939432710409164 \n",
      "Total time elapsed: 754.17 seconds\n",
      "Epoch 9: Train err: 0.7325833333333334, Train loss: 2.7717562710985226  |Validation err: 0.719 , Validation loss:2.7524055764079094 \n",
      "Total time elapsed: 845.95 seconds\n",
      "Epoch 10: Train err: 0.7146666666666667, Train loss: 2.72659455461705  |Validation err: 0.707 , Validation loss:2.706203557550907 \n",
      "Total time elapsed: 937.04 seconds\n",
      "Epoch 11: Train err: 0.69325, Train loss: 2.6804111181421484  |Validation err: 0.67725 , Validation loss:2.656846761703491 \n",
      "Total time elapsed: 1027.40 seconds\n",
      "Epoch 12: Train err: 0.6695833333333333, Train loss: 2.6347614197020834  |Validation err: 0.67175 , Validation loss:2.6049321815371513 \n",
      "Total time elapsed: 1119.35 seconds\n",
      "Epoch 13: Train err: 0.6538333333333334, Train loss: 2.5853106087826667  |Validation err: 0.6495 , Validation loss:2.556324765086174 \n",
      "Total time elapsed: 1209.80 seconds\n",
      "Epoch 14: Train err: 0.63825, Train loss: 2.5333983644526055  |Validation err: 0.6255 , Validation loss:2.5107953250408173 \n",
      "Total time elapsed: 1301.23 seconds\n",
      "Epoch 15: Train err: 0.6220833333333333, Train loss: 2.4829445671527943  |Validation err: 0.60975 , Validation loss:2.4600494354963303 \n",
      "Total time elapsed: 1391.85 seconds\n",
      "Epoch 16: Train err: 0.608, Train loss: 2.441674757511058  |Validation err: 0.60775 , Validation loss:2.4205564484000206 \n",
      "Total time elapsed: 1482.77 seconds\n",
      "Epoch 17: Train err: 0.5934166666666667, Train loss: 2.3930818451211806  |Validation err: 0.591 , Validation loss:2.3629708290100098 \n",
      "Total time elapsed: 1571.95 seconds\n",
      "Epoch 18: Train err: 0.5821666666666667, Train loss: 2.338041637806182  |Validation err: 0.58725 , Validation loss:2.3217697143554688 \n",
      "Total time elapsed: 1661.94 seconds\n"
     ]
    }
   ],
   "source": [
    "model_resmod = torchvision.models.resnet18(pretrained=True).to(device)\n",
    "num_ftrs = model_resmod.fc.in_features\n",
    "model_resmod.fc = nn.Sequential(nn.Linear(num_ftrs, 216).to(device),nn.ReLU().to(device),\n",
    "                               nn.Linear(216,128).to(device),nn.Dropout(0.5).to(device),nn.ReLU(),\n",
    "                                nn.Linear(128,20).to(device))\n",
    "#print(model_resmod)\n",
    "trainResNet(model_resmod, batch_size= 128, learning_rate=0.00001, num_epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "LhR8szsnT9ht",
    "DZj-htFCT9iW",
    "0d68hPcAT9ii",
    "kJL31d3yT9iw",
    "TohhgGDfT9jL"
   ],
   "name": "ARFAs Copy- 20 - Alexnet_resnet_Food_Classification_withgpu.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
