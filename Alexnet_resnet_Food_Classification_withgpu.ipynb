{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Alexnet_resnet_Food_Classification_withgpu.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "LhR8szsnT9ht",
        "DZj-htFCT9iW",
        "0d68hPcAT9ii",
        "TohhgGDfT9jL"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "hV2YkkE9T9g3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        " # Food Classification Using Deep Neural Networks and Transfer Learning"
      ]
    },
    {
      "metadata": {
        "id": "JNNjPabdT9g6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 1. Data Splitting "
      ]
    },
    {
      "metadata": {
        "id": "ksf8gEyBT9g7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Split the data into training, validation, and test sets. Justify your choice."
      ]
    },
    {
      "metadata": {
        "id": "-LdKamKAT9g9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "V1UNELltT9hB",
        "colab_type": "code",
        "outputId": "982df3fe-e364-4d1a-b759-0594fca93d90",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "#move to gpu if possible\n",
        "\n",
        "device = torch.device(\"cpu\")\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "print(\"using device\", device)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "using device cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "49JzY7d9T9hE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "import shutil\n",
        "import os\n",
        "import random\n",
        "random.seed = 2019\n",
        "split = [600,800,1000]\n",
        "\n",
        "for food in ['falafel','apple_pie','donuts','french_fries','macarons','nachos','onion_rings','oysters','pizza', 'mussels']:\n",
        "    \n",
        "    #create new empty folders\n",
        "    if not os.path.exists('train/' + food):\n",
        "        os.makedirs('train/' + food)\n",
        "    if not os.path.exists('val/' + food):\n",
        "        os.makedirs('val/' + food)\n",
        "    if not os.path.exists('test/' + food):\n",
        "        os.makedirs('test/' + food)\n",
        "    \n",
        "    #make a list of image file names, shuffle them\n",
        "    pictures = os.listdir(\"Food Chosen/\"+ food)\n",
        "    pictures.sort()\n",
        "    random.shuffle(pictures)\n",
        "    \n",
        "    #copy images into folders\n",
        "    for pic in pictures[0:600]:\n",
        "        shutil.copy(\"Food Chosen/\"+ food +\"/\"+ pic,\"train/\"+food+\"/\"+pic)\n",
        "        print(\"copy\",\"Food Chosen/\"+ food +\"/\"+ pic,\"to\",\"train/\"+food+\"/\"+pic)\n",
        "        \n",
        "    for pic in pictures[600:800]:\n",
        "        shutil.copy(\"Food Chosen/\"+ food +\"/\"+ pic,\"val/\"+food+\"/\"+pic)\n",
        "        print(\"copy\",\"Food Chosen/\"+ food +\"/\"+ pic,\"to\",\"val/\"+food+\"/\"+pic)\n",
        "        \n",
        "    for pic in pictures[800:1000]:\n",
        "        shutil.copy(\"Food Chosen/\"+ food +\"/\"+ pic,\"test/\"+food+\"/\"+pic)\n",
        "        print(\"copy\",\"Food Chosen/\"+ food +\"/\"+ pic,\"to\",\"test/\"+food+\"/\"+pic)"
      ]
    },
    {
      "metadata": {
        "id": "7PGF9YYJT9hF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Load Data"
      ]
    },
    {
      "metadata": {
        "id": "KeHo3BdtT9hG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_data_loader(target_classes, batch_size):\n",
        "    \"\"\" Returns the indices for datapoints in the dataset that\n",
        "    belongs to the desired target classes, a subset of all possible classes.\n",
        "\n",
        "    Args:\n",
        "        dataset: Dataset object\n",
        "        classes: A list of strings denoting the name of each class\n",
        "        target_classes: A list of strings denoting the name of the desired\n",
        "                        classes. Should be a subset of the argument 'classes'\n",
        "    Returns:\n",
        "        indices: list of indices that have labels corresponding to one of the\n",
        "                 target classes\n",
        "    \"\"\"\n",
        "    classes = ('falafel','apple_pie','donuts','french_fries','macarons','nachos','onion_rings','oysters','pizza', 'mussels')\n",
        "\n",
        "\n",
        "    transform = transforms.Compose(\n",
        "    [transforms.Resize((224,224), interpolation=2), transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "    ])\n",
        "\n",
        "    trainset = torchvision.datasets.ImageFolder(root = 'train',\n",
        "                                                 transform=transform)\n",
        " \n",
        "    train_loader = torch.utils.data.DataLoader(trainset, batch_size = batch_size,\n",
        "                                              num_workers = 1, shuffle = True)\n",
        "\n",
        "    \n",
        "    val_set = torchvision.datasets.ImageFolder(root='val',\n",
        "                                                 transform=transform)\n",
        "    val_loader = torch.utils.data.DataLoader(val_set, batch_size=batch_size,\n",
        "                                              num_workers=1, shuffle = True)\n",
        "\n",
        "    testset = torchvision.datasets.ImageFolder(root= 'test',\n",
        "                                            transform=transform)\n",
        "    \n",
        "    test_loader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
        "                                             num_workers=1, shuffle = True)\n",
        "\n",
        "    return train_loader, classes#, val_loader, test_loader, classes"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7tuk9Ud9UiYw",
        "colab_type": "code",
        "outputId": "e6fd51d6-b632-45aa-a170-05479645fefe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "yHu0kUNKVM0R",
        "colab_type": "code",
        "outputId": "57a7c61a-0b38-4888-d486-00c70eb73353",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "cd /gdrive/My Drive/Colab Notebooks"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/gdrive/My Drive/Colab Notebooks\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "lwLyhE9MVr8c",
        "colab_type": "code",
        "outputId": "8d5127d7-9200-4f85-e4bf-6e2c4bc18eee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        }
      },
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Alexnet_resnet_Food_Classification_withgpu.ipynb\n",
            "'Copy of Alexnet_resnet_Food_Classification_withgpu.ipynb'\n",
            "'Copy of Copy of Food_Classification_withgpu.ipynb.gdoc'\n",
            " Food_Classification.ipynb\n",
            " Hyperparameters.gsheet\n",
            " model_ResNet_bs32_lr0.0001_epoch0\n",
            " model_ResNet_bs32_lr0.0001_epoch1\n",
            " model_ResNet_bs32_lr0.0001_epoch10\n",
            " model_ResNet_bs32_lr0.0001_epoch11\n",
            " model_ResNet_bs32_lr0.0001_epoch12\n",
            " model_ResNet_bs32_lr0.0001_epoch13\n",
            " model_ResNet_bs32_lr0.0001_epoch14\n",
            " model_ResNet_bs32_lr0.0001_epoch15\n",
            " model_ResNet_bs32_lr0.0001_epoch16\n",
            " model_ResNet_bs32_lr0.0001_epoch17\n",
            " model_ResNet_bs32_lr0.0001_epoch18\n",
            " model_ResNet_bs32_lr0.0001_epoch19\n",
            " model_ResNet_bs32_lr0.0001_epoch19_train_err.csv\n",
            " model_ResNet_bs32_lr0.0001_epoch19_train_loss.csv\n",
            " model_ResNet_bs32_lr0.0001_epoch19_val_err.csv\n",
            " model_ResNet_bs32_lr0.0001_epoch19_val_loss.csv\n",
            " model_ResNet_bs32_lr0.0001_epoch2\n",
            " model_ResNet_bs32_lr0.0001_epoch3\n",
            " model_ResNet_bs32_lr0.0001_epoch4\n",
            " model_ResNet_bs32_lr0.0001_epoch5\n",
            " model_ResNet_bs32_lr0.0001_epoch6\n",
            " model_ResNet_bs32_lr0.0001_epoch7\n",
            " model_ResNet_bs32_lr0.0001_epoch8\n",
            " model_ResNet_bs32_lr0.0001_epoch9\n",
            " test\n",
            " train\n",
            " val\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "nHT5pN6iT9hK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "classes = ('falafel','apple_pie','donuts','french_fries','macarons','nachos','onion_rings','oysters','pizza', 'mussels')\n",
        "\n",
        "transform = transforms.Compose(\n",
        "    [transforms.Resize((224,224), interpolation=2), transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "    ])\n",
        "\n",
        "trainset = torchvision.datasets.ImageFolder(root = 'train',\n",
        "                                             transform=transform)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(trainset, batch_size = 64,\n",
        "                                          num_workers = 1, shuffle = True)\n",
        "val_set = torchvision.datasets.ImageFolder(root='val',\n",
        "                                                 transform=transform)\n",
        "val_loader = torch.utils.data.DataLoader(val_set, batch_size=64,\n",
        "                                              num_workers=1, shuffle = True)\n",
        "\n",
        "testset = torchvision.datasets.ImageFolder(root= 'test',\n",
        "                                            transform=transform)\n",
        "    \n",
        "test_loader = torch.utils.data.DataLoader(testset, batch_size=64,\n",
        "                                             num_workers=1, shuffle = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UI7rliTUT9hO",
        "colab_type": "code",
        "outputId": "0b049279-0de3-4a25-ba11-0b4baca510af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "cell_type": "code",
      "source": [
        "trainset.class_to_idx\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'apple_pie': 0,\n",
              " 'donuts': 1,\n",
              " 'falafel': 2,\n",
              " 'french_fries': 3,\n",
              " 'macarons': 4,\n",
              " 'mussels': 5,\n",
              " 'nachos': 6,\n",
              " 'onion_rings': 7,\n",
              " 'oysters': 8,\n",
              " 'pizza': 9}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "metadata": {
        "id": "1GU18HS4T9hS",
        "colab_type": "code",
        "outputId": "094f0fb5-c1be-444f-e287-413d5f26824b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "cell_type": "code",
      "source": [
        "train_loader_iter = iter(train_loader)\n",
        "imgs, labels = next(train_loader_iter)\n",
        "print(imgs.shape)\n",
        "print(labels)\n",
        "print(labels.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([64, 3, 224, 224])\n",
            "tensor([3, 6, 4, 5, 0, 1, 1, 0, 8, 2, 7, 6, 9, 8, 5, 0, 2, 4, 2, 7, 2, 3, 0, 9,\n",
            "        9, 9, 8, 5, 4, 2, 0, 6, 6, 0, 6, 2, 4, 7, 3, 3, 4, 8, 8, 0, 5, 3, 4, 4,\n",
            "        2, 6, 2, 4, 8, 7, 1, 4, 2, 8, 5, 7, 1, 8, 9, 1])\n",
            "torch.Size([64])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "xAECXnrlT9hV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 2. Convolutional Network - Baseline Model \n",
        "Build a convolutional neural network model that takes the (224x224 RGB) image as input."
      ]
    },
    {
      "metadata": {
        "id": "bungE51hT9hW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        " \n",
        "class LargeNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LargeNet, self).__init__()\n",
        "        self.name = \"large\"\n",
        "        self.conv1 = nn.Conv2d(3, 32, 3, padding = 1) \n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, padding = 1)\n",
        "        self.conv3 = nn.Conv2d(64, 128, 3, padding = 1)\n",
        "        \n",
        "        self.fc1 = nn.Linear(128 * 28 * 28, 128)\n",
        "        self.fc2 = nn.Linear(128, 9)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = self.pool(F.relu(self.conv3(x)))\n",
        "        x = x.view(-1, 128 * 28 * 28)  \n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x) \n",
        "        x = x.squeeze(1) # Flatten to [batch_size]\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "L5sdpmLhT9hZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 3. Training \n",
        "Train your network. Plot the training curve.\n",
        "\n",
        "Make sure that you are checkpointing frequently!"
      ]
    },
    {
      "metadata": {
        "id": "IDPxQg20T9ha",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "# convolutional neural network, \n",
        "class LargeNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LargeNet, self).__init__()\n",
        "        self.name = \"large\"\n",
        "        self.conv1 = nn.Conv2d(3, 5, 3, padding = 1) \n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(5, 10, 3, padding = 1)\n",
        "        self.conv3 = nn.Conv2d(10, 15, 3, padding = 1)\n",
        "        self.fc1 = nn.Linear(15 * 28 * 28 , 64)\n",
        "        self.fc2 = nn.Linear(64, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = self.pool(F.relu(self.conv3(x)))\n",
        "        \n",
        "        x = x.view(-1, 15 * 28 * 28)  \n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x) \n",
        "        x = x.squeeze(1) # Flatten to [batch_size]\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mcNH2X3ET9hd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_model_name(name, batch_size, learning_rate, epoch):\n",
        "    \"\"\" Generate a name for the model consisting of all the hyperparameter values\n",
        "\n",
        "    Args:\n",
        "        config: Configuration object containing the hyperparameters\n",
        "    Returns:\n",
        "        path: A string with the hyperparameter name and value concatenated\n",
        "    \"\"\"\n",
        "    path = \"model_{0}_bs{1}_lr{2}_epoch{3}\".format(name,\n",
        "                                                   batch_size,\n",
        "                                                   learning_rate,\n",
        "                                                   epoch)\n",
        "    return path\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2ZVu5ZF3T9hg",
        "colab_type": "code",
        "outputId": "c0021392-946d-4d59-f879-37e4d10e27f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        }
      },
      "cell_type": "code",
      "source": [
        "train_loader_iter = iter(train_loader)\n",
        "imgs, labels = next(train_loader_iter)\n",
        "print(imgs.shape)\n",
        "print(labels)\n",
        "print(labels.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([64, 3, 224, 224])\n",
            "tensor([0, 0, 1, 6, 9, 5, 4, 5, 1, 9, 9, 2, 8, 2, 4, 9, 1, 3, 5, 8, 1, 3, 3, 9,\n",
            "        4, 0, 9, 3, 5, 8, 4, 3, 8, 7, 7, 8, 5, 6, 3, 8, 5, 7, 3, 9, 4, 7, 5, 7,\n",
            "        4, 3, 8, 2, 4, 1, 2, 7, 5, 4, 8, 0, 3, 2, 9, 1])\n",
            "torch.Size([64])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "qMG640AKT9hk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Training - Baseline Model"
      ]
    },
    {
      "metadata": {
        "id": "JokNNQZRT9hl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def evaluate(net, loader, criterion):\n",
        "    \"\"\" Evaluate the network on the validation set.\n",
        "\n",
        "     Args:\n",
        "         net: PyTorch neural network object\n",
        "         loader: PyTorch data loader for the validation set\n",
        "         criterion: The loss function\n",
        "     Returns:\n",
        "         err: A scalar for the avg classification error over the validation set\n",
        "         loss: A scalar for the average loss function over the validation set\n",
        "     \"\"\"\n",
        "    total_loss = 0.0\n",
        "    total_err = 0.0\n",
        "    total_epoch = 0\n",
        "\n",
        "    for i, data in enumerate(loader, 0):\n",
        "        inputs, labels = data\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "        \n",
        "\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        pred = outputs.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
        "        total_err += pred.ne(labels.view_as(pred)).sum().item()\n",
        "        total_loss += loss.item()\n",
        "        total_epoch += len(labels)\n",
        "\n",
        "    err = float(total_err) / total_epoch\n",
        "    loss = float(total_loss) / (i + 1)\n",
        "\n",
        "    return err, loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ELngXxjuT9hn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train(net, batch_size=1, learning_rate=0.0001, num_epochs=30):\n",
        "    transform = transforms.Compose(\n",
        "        [transforms.Resize((224,224)), transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "    trainset = torchvision.datasets.ImageFolder(root='train',\n",
        "                                                 transform=transform)\n",
        "    train_loader = torch.utils.data.DataLoader(trainset, batch_size=64,\n",
        "                                          num_workers=1, shuffle = True)\n",
        "    \n",
        "    train_loader_iter = iter(train_loader)\n",
        "    imgs, labels = next(train_loader_iter)\n",
        "    \n",
        "    \n",
        "    torch.manual_seed(1000)\n",
        "\n",
        "   \n",
        "  \n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
        "    \n",
        "    train_err = np.zeros(num_epochs)\n",
        "    train_loss = np.zeros(num_epochs)\n",
        "    val_err = np.zeros(num_epochs)\n",
        "    val_loss = np.zeros(num_epochs)\n",
        "    \n",
        "    \n",
        "    iters, losses, train_acc, val_acc = [], [], [], []\n",
        "    start_time = time.time()\n",
        "    # training\n",
        "    n=0\n",
        "    for epoch in range(num_epochs):  # loop over the dataset multiple times \n",
        "      \n",
        "        total_train_loss = 0.0\n",
        "        total_train_err = 0.0\n",
        "\n",
        "        total_epoch = 0\n",
        "        for i, data in enumerate(train_loader, 0):\n",
        "            \n",
        "            inputs, labels = data\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "                      \n",
        "            optimizer.zero_grad()\n",
        "            outputs = net(inputs)\n",
        "           \n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            iters.append(n)\n",
        "           \n",
        "            pred = outputs.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
        "            total_train_err += pred.ne(labels.view_as(pred)).sum().item()\n",
        "            total_train_loss += loss.item()\n",
        "            total_epoch += len(labels)\n",
        "           \n",
        "            losses.append(float(loss)/batch_size)  \n",
        "            n += 1# compute *average* loss\n",
        "        \n",
        "        train_err[epoch] = float(total_train_err) / total_epoch\n",
        "     \n",
        "        train_loss[epoch] = float(total_train_loss) / (i+1)\n",
        "        \n",
        "        val_err[epoch], val_loss[epoch] = evaluate(net, val_loader, criterion)\n",
        "    \n",
        "        print((\"Epoch {}: Train err: {}, Train loss: {}  |\" +\n",
        "               \"Validation err: {} , Validation loss:{} \").format(\n",
        "                   epoch + 1,\n",
        "            \n",
        "                   train_err[epoch],\n",
        "                   train_loss[epoch],\n",
        "                   val_err[epoch],\n",
        "                   val_loss[epoch]\n",
        "                        ))\n",
        "\n",
        "        # Save the current model (checkpoint) to a file\n",
        "        model_path = get_model_name(net.name, batch_size, learning_rate, epoch)\n",
        "        torch.save(net.state_dict(), model_path)\n",
        "\n",
        "    print('Finished Training')\n",
        "    end_time = time.time()\n",
        "    elapsed_time = end_time - start_time\n",
        "    print(\"Total time elapsed: {:.2f} seconds\".format(elapsed_time))\n",
        "\n",
        "    # Write the train/test loss/err into CSV file for plotting later\n",
        "    epochs = np.arange(1, num_epochs + 1)\n",
        "\n",
        "    np.savetxt(\"{}_train_err.csv\".format(model_path), train_err)\n",
        "    np.savetxt(\"{}_train_loss.csv\".format(model_path), train_loss)\n",
        "    np.savetxt(\"{}_val_err.csv\".format(model_path), val_err)\n",
        "    np.savetxt(\"{}_val_loss.csv\".format(model_path), val_loss)\n",
        "        \n",
        "          "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JFA7StZPT9hq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Training Curve\n",
        "import matplotlib.pyplot as plt\n",
        "def plot_training_curve(path):\n",
        "    \"\"\" Plots the training curve for a model run, given the csv files\n",
        "    containing the train/validation error/loss.\n",
        "\n",
        "    Args:\n",
        "        path: The base path of the csv files produced during training\n",
        "    \"\"\"\n",
        "\n",
        "    train_err = np.loadtxt(\"{}_train_err.csv\".format(path))\n",
        "    val_err = np.loadtxt(\"{}_val_err.csv\".format(path))\n",
        "    train_loss = np.loadtxt(\"{}_train_loss.csv\".format(path))\n",
        "    val_loss = np.loadtxt(\"{}_val_loss.csv\".format(path))\n",
        "    plt.title(\"Train vs Validation Error\")\n",
        "    plt.plot(range(1,16), train_err, label=\"Train\")\n",
        "    plt.plot(range(1,16), val_err, label=\"Validation\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Error\")\n",
        "    plt.legend(loc='best')\n",
        "    plt.show()\n",
        "    plt.title(\"Train vs Validation Loss\")\n",
        "    plt.plot(range(1,16), train_loss, label=\"Train\")\n",
        "    plt.plot(range(1,16), val_loss, label=\"Validation\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend(loc='best')\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LhR8szsnT9ht",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Hyperparameter Seach - Baseline Model"
      ]
    },
    {
      "metadata": {
        "id": "qyMPArHuT9hv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "ne3 = LargeNet()\n",
        "ne3.to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2hmLiEa9T9h1",
        "colab_type": "code",
        "outputId": "ace26a8f-a718-4b75-d8ea-b7820cb033bd",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train(ne3, batch_size = 64, learning_rate=0.01, num_epochs = 15)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1: Train err: 0.8581666666666666, Train loss: 2.292994164405985  |Validation err: 0.8265 , Validation loss:2.2168687283992767 \n",
            "Epoch 2: Train err: 0.8133333333333334, Train loss: 2.1898040238847125  |Validation err: 0.8125 , Validation loss:2.1898394525051117 \n",
            "Epoch 3: Train err: 0.7908333333333334, Train loss: 2.150301992893219  |Validation err: 0.8325 , Validation loss:2.231725499033928 \n",
            "Epoch 4: Train err: 0.7218333333333333, Train loss: 2.024061899235908  |Validation err: 0.7735 , Validation loss:2.1689374707639217 \n",
            "Epoch 5: Train err: 0.6418333333333334, Train loss: 1.8138254690677562  |Validation err: 0.7865 , Validation loss:2.2966490015387535 \n",
            "Epoch 6: Train err: 0.5516666666666666, Train loss: 1.5930405474723655  |Validation err: 0.806 , Validation loss:2.587955057621002 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-28-4f2dc5f4ef1d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mne3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.01\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m15\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[1;32m<ipython-input-11-0253fd649bf5>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(net, batch_size, learning_rate, num_epochs)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\Anaconda3\\envs\\aps3601\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    488\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 489\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    490\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m<ipython-input-7-77d54b61f7c3>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\Anaconda3\\envs\\aps3601\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    488\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 489\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    490\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\Anaconda3\\envs\\aps3601\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    318\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    319\u001b[0m         return F.conv2d(input, self.weight, self.bias, self.stride,\n\u001b[1;32m--> 320\u001b[1;33m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[0;32m    321\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    322\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "k1kENXYFT9h_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "ne3 = LargeNet()\n",
        "ne3.to(device)\n",
        "train(ne3, batch_size = 64, learning_rate=0.1, num_epochs = 15)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hPM9889qT9iG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "ne3 = LargeNet()\n",
        "ne3.to(device)\n",
        "train(ne3, batch_size = 32, learning_rate=0.001, num_epochs = 15)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UhyyR4gzT9iJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "ne3 = LargeNet()\n",
        "ne3.to(device)\n",
        "train(ne3, batch_size = 32, learning_rate=0.01, num_epochs = 15)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2_TC2OaNT9iM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "ne3 = LargeNet()\n",
        "ne3.to(device)\n",
        "train(ne3, batch_size = 32, learning_rate=0.1, num_epochs = 15)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sAq5ICKTT9iR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "D9s09g7cT9iT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "path = 'model_large_bs64_lr0.001_epoch14'\n",
        "plot_training_curve(path)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DZj-htFCT9iW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Test Classification Error"
      ]
    },
    {
      "metadata": {
        "id": "lUt5BuEgT9iX",
        "colab_type": "code",
        "outputId": "86c651fc-377c-4813-aaea-bbe4f51bb78a",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "net = LargeNet()\n",
        "net.to(device)\n",
        "model_path = get_model_name(net.name, batch_size=64, learning_rate=0.001, epoch=5)\n",
        "state = torch.load(model_path)\n",
        "net.load_state_dict(state)\n",
        "\n",
        "evaluate(net, test_loader, nn.CrossEntropyLoss())\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.581625, 1.6759631481170654)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "metadata": {
        "id": "Yep6nFfJT9ib",
        "colab_type": "code",
        "outputId": "16d8cce6-3dd6-4950-afde-cd5c1ff9c298",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "The test classification error was about 0.37."
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-42-3ddd3359581b>, line 1)",
          "traceback": [
            "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-42-3ddd3359581b>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    The test classification error was about 0.37.\u001b[0m\n\u001b[1;37m           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "kiC1d-89T9ie",
        "colab_type": "code",
        "outputId": "87a61043-7e56-4090-c00e-d649161fd349",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "len(testset)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "metadata": {
        "id": "0d68hPcAT9ii",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Test Accuracy"
      ]
    },
    {
      "metadata": {
        "id": "uOc3Ym-oT9ij",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_accuracy(model):\n",
        "    data = testset\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    for i, data in enumerate(test_loader, 0):\n",
        "        inputs, labels = data\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "        \n",
        "        outputs = net(inputs)\n",
        "        # We don't need to run F.softmax\n",
        "        pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
        "        correct += pred.eq(labels.view_as(pred)).sum().item()\n",
        "        total += imgs.shape[0]\n",
        "    return correct / total"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5kXdkJ_5T9im",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "net = LargeNet()\n",
        "net.to(device)\n",
        "model_path = get_model_name(net.name, batch_size=64, learning_rate=0.001, epoch=5)\n",
        "state = torch.load(model_path)\n",
        "net.load_state_dict(state)\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WEUI8c2yT9ir",
        "colab_type": "code",
        "outputId": "9821f3ed-8f57-4661-e504-b2f30b74daa5",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "get_accuracy(net)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "conv2d(): argument 'input' (position 1) must be Tensor, not int",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-35-f1638336344e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mget_accuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[1;32m<ipython-input-33-718f2e7d6d4c>\u001b[0m in \u001b[0;36mget_accuracy\u001b[1;34m(model)\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mcorrect\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mimgs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# We don't need to run F.softmax\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m         \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m# get the index of the max log-probability\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mcorrect\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mpred\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meq\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview_as\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\Anaconda3\\envs\\aps3601\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    488\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 489\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    490\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m<ipython-input-27-77d54b61f7c3>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\Anaconda3\\envs\\aps3601\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    488\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 489\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    490\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\Anaconda3\\envs\\aps3601\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    318\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    319\u001b[0m         return F.conv2d(input, self.weight, self.bias, self.stride,\n\u001b[1;32m--> 320\u001b[1;33m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[0;32m    321\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    322\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mTypeError\u001b[0m: conv2d(): argument 'input' (position 1) must be Tensor, not int"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "s8I6THosT9iv",
        "colab_type": "raw"
      },
      "cell_type": "markdown",
      "source": [
        "Test accuracy was about 62 %."
      ]
    },
    {
      "metadata": {
        "id": "kJL31d3yT9iw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 5. Transfer Learning [16 pt]\n",
        "For many image classification tasks, it is generally not a good idea to train a very large Deep Neural Network model from scratch due to the enormous compute requirements and lack of sufficient amounts of training data. One of the better option is to try using an existing model that performs a similar task to the one you need to solve. This method of utilizing a pre-trained network for other similar tasks is broadly termed â€œTransfer Learningâ€. In this assignment, we will use Transfer Learning to extract features from the hand gesture images. Then, train a smaller network to use these features as input and classify the hand gestures.\n",
        "\n",
        "As you have learned from the CNN lecture, Convolution layers extract various features from the images which get utilized by the fully connected layers for correct classification. AlexNet architecture played a pivotal role in establishing Deep Neural Nets as a go-to tool for Image classification problems and we will use an imagenet pre-trained AlexNet model to extract features in this assignment."
      ]
    },
    {
      "metadata": {
        "id": "6PrndgSVT9iy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from torchvision import transforms\n",
        "\n",
        "# Set up a transform that scales and crops an image so it has the dimensions\n",
        "# of the input layer of alexnet\n",
        "scale_crop = transforms.Compose([\n",
        "   transforms.Resize(256),\n",
        "   transforms.CenterCrop(224)\n",
        "])\n",
        "\n",
        "# The normalization that was applied to the data when alexnet was trained\n",
        "normalize = transforms.Normalize(\n",
        "   mean=[0.485, 0.456, 0.406],\n",
        "   std=[0.229, 0.224, 0.225]\n",
        ")\n",
        "\n",
        "# Put scaling, cropping, normalzing and converting from PIL image to pytorch into one package\n",
        "preprocess = transforms.Compose([\n",
        "   scale_crop,\n",
        "   transforms.ToTensor(),\n",
        "   normalize\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6IRYcWeOT9i2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "trainset = torchvision.datasets.ImageFolder(root = 'train',\n",
        "                                             transform=preprocess)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(trainset, batch_size = 32,\n",
        "                                          num_workers = 1, shuffle = True)\n",
        "val_set = torchvision.datasets.ImageFolder(root='val',\n",
        "                                                 transform=preprocess)\n",
        "val_loader = torch.utils.data.DataLoader(val_set, batch_size=32,\n",
        "                                              num_workers=1, shuffle = True)\n",
        "\n",
        "testset = torchvision.datasets.ImageFolder(root= 'test',\n",
        "                                           transform=transform)\n",
        "    \n",
        "test_loader = torch.utils.data.DataLoader(testset, batch_size=32,\n",
        "                                             num_workers=1, shuffle = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5bEOwrIiT9i5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Importing relevant Libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import time\n",
        "import os\n",
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from torchvision import datasets, models, transforms\n",
        "from torch.nn import functional as F\n",
        "import copy\n",
        "from torch.autograd import Variable\n",
        "\n",
        "\n",
        "###############################################################################\n",
        "# Feature Extraction using AlexNet pretrained model\n",
        "\n",
        "class AlexNet(nn.Module):\n",
        "    '''\n",
        "    Class that loads AlexNet Feature Model ('Convolution layers') with imagenet trained weights\n",
        "    \n",
        "    input : image tensors with dimension Lx3x224x224\n",
        "    \n",
        "    output : feature tensor with dimension Lx256x6x6\n",
        "    \n",
        "    *L - Batch size\n",
        "    \n",
        "    '''\n",
        "    \n",
        "    def load_weights(self):\n",
        "        an_builtin = torchvision.models.alexnet(pretrained=True) # Loads the pretrained model weights\n",
        "        \n",
        "        features_weight_i = [0, 3, 6, 8, 10]\n",
        "        for i in features_weight_i:\n",
        "            self.features[i].weight = an_builtin.features[i].weight\n",
        "            self.features[i].bias = an_builtin.features[i].bias\n",
        "\n",
        "    def __init__(self):\n",
        "        super(AlexNet, self).__init__()\n",
        "        self.name = \"AlexNet\"\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(256 * 6 * 6, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(4096, 10),\n",
        "        )\n",
        "        self.load_weights() # Copies the weights to AlexNetFeatures model layers\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = x.view(x.size(0), 256 * 6 * 6)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "        \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fL3JWAtWb3Mv",
        "colab_type": "code",
        "outputId": "67b6d976-4b5c-4062-c388-bc2134ac29db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        }
      },
      "cell_type": "code",
      "source": [
        "import torchvision.models\n",
        "\n",
        "alexNet = torchvision.models.alexnet(pretrained=False)\n",
        "alexNet"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AlexNet(\n",
              "  (features): Sequential(\n",
              "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
              "    (1): ReLU(inplace)\n",
              "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
              "    (4): ReLU(inplace)\n",
              "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (7): ReLU(inplace)\n",
              "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (9): ReLU(inplace)\n",
              "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (11): ReLU(inplace)\n",
              "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
              "  (classifier): Sequential(\n",
              "    (0): Dropout(p=0.5)\n",
              "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
              "    (2): ReLU(inplace)\n",
              "    (3): Dropout(p=0.5)\n",
              "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "    (5): ReLU(inplace)\n",
              "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "metadata": {
        "id": "LXL0wV9bbhRn",
        "colab_type": "code",
        "outputId": "acdc704e-e37e-41c1-c1a2-cdb9292cd13c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1560
        }
      },
      "cell_type": "code",
      "source": [
        "import torchvision.models\n",
        "\n",
        "resNet18 = torchvision.models.resnet18(pretrained=False)\n",
        "resNet18"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "metadata": {
        "id": "UwGDVZ5ba9qe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "91DGE92aT9i9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "netalex = AlexNet()\n",
        "netalex.to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5U2LXww4T9jA",
        "colab_type": "code",
        "outputId": "33e1ba85-ef10-448c-8d87-1670ff684a64",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 592
        }
      },
      "cell_type": "code",
      "source": [
        "train(netalex, batch_size = 64, learning_rate = 0.00001, num_epochs = 15)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1: Train err: 0.5908333333333333, Train loss: 1.8192532404940178  |Validation err: 0.4129353233830846 , Validation loss:1.2098208628594875 \n",
            "Epoch 2: Train err: 0.336, Train loss: 1.0047019105008308  |Validation err: 0.36517412935323385 , Validation loss:1.1387120466679335 \n",
            "Epoch 3: Train err: 0.25916666666666666, Train loss: 0.8018852922510593  |Validation err: 0.3402985074626866 , Validation loss:1.1026504393666983 \n",
            "Epoch 4: Train err: 0.23766666666666666, Train loss: 0.7051939694805348  |Validation err: 0.3353233830845771 , Validation loss:1.1032313834875822 \n",
            "Epoch 5: Train err: 0.20016666666666666, Train loss: 0.6161762878615805  |Validation err: 0.3407960199004975 , Validation loss:1.0975917130708694 \n",
            "Epoch 6: Train err: 0.18283333333333332, Train loss: 0.5571522214945327  |Validation err: 0.31144278606965176 , Validation loss:1.0427784509956837 \n",
            "Epoch 7: Train err: 0.16216666666666665, Train loss: 0.4949508328387078  |Validation err: 0.2900497512437811 , Validation loss:0.9826078210026026 \n",
            "Epoch 8: Train err: 0.15433333333333332, Train loss: 0.4525228252119206  |Validation err: 0.29203980099502486 , Validation loss:1.0085347164422274 \n",
            "Epoch 9: Train err: 0.129, Train loss: 0.3990353765956899  |Validation err: 0.30597014925373134 , Validation loss:1.0812860783189535 \n",
            "Epoch 10: Train err: 0.12316666666666666, Train loss: 0.3659418786459781  |Validation err: 0.2990049751243781 , Validation loss:1.0787921603769064 \n",
            "Epoch 11: Train err: 0.10483333333333333, Train loss: 0.33061716816526776  |Validation err: 0.3263681592039801 , Validation loss:1.241654297336936 \n",
            "Epoch 12: Train err: 0.096, Train loss: 0.2953158182666657  |Validation err: 0.30597014925373134 , Validation loss:1.2001926600933075 \n",
            "Epoch 13: Train err: 0.08316666666666667, Train loss: 0.2625973953053038  |Validation err: 0.29203980099502486 , Validation loss:1.2129251677542925 \n",
            "Epoch 14: Train err: 0.0695, Train loss: 0.22683768188382716  |Validation err: 0.3099502487562189 , Validation loss:1.2990973629057407 \n",
            "Epoch 15: Train err: 0.071, Train loss: 0.2144617684819597  |Validation err: 0.2955223880597015 , Validation loss:1.2849179729819298 \n",
            "Epoch 16: Train err: 0.0605, Train loss: 0.1898078126634689  |Validation err: 0.28308457711442786 , Validation loss:1.214936539530754 \n",
            "Epoch 17: Train err: 0.051333333333333335, Train loss: 0.16149577482583674  |Validation err: 0.30149253731343284 , Validation loss:1.3302013203501701 \n",
            "Epoch 18: Train err: 0.051166666666666666, Train loss: 0.1551061255183626  |Validation err: 0.2840796019900497 , Validation loss:1.289955697953701 \n",
            "Epoch 19: Train err: 0.03833333333333333, Train loss: 0.12579736153179027  |Validation err: 0.2915422885572139 , Validation loss:1.2557972446084023 \n",
            "Epoch 20: Train err: 0.035166666666666666, Train loss: 0.116690692947583  |Validation err: 0.2935323383084577 , Validation loss:1.3451756909489632 \n",
            "Epoch 21: Train err: 0.031166666666666665, Train loss: 0.1081245502734438  |Validation err: 0.282089552238806 , Validation loss:1.3504174500703812 \n",
            "Epoch 22: Train err: 0.0305, Train loss: 0.10032496621158529  |Validation err: 0.27810945273631843 , Validation loss:1.40424769744277 \n",
            "Epoch 23: Train err: 0.0285, Train loss: 0.08804659676203068  |Validation err: 0.30895522388059704 , Validation loss:1.6362245231866837 \n",
            "Epoch 24: Train err: 0.023666666666666666, Train loss: 0.07946090431923562  |Validation err: 0.3009950248756219 , Validation loss:1.6046176943928003 \n",
            "Epoch 25: Train err: 0.021, Train loss: 0.07102988489923324  |Validation err: 0.2945273631840796 , Validation loss:1.5195213239639997 \n",
            "Epoch 26: Train err: 0.017833333333333333, Train loss: 0.06223929966700838  |Validation err: 0.32288557213930347 , Validation loss:1.7384257968515158 \n",
            "Epoch 27: Train err: 0.018, Train loss: 0.06262569293934614  |Validation err: 0.3144278606965174 , Validation loss:1.7899679662659764 \n",
            "Epoch 28: Train err: 0.017333333333333333, Train loss: 0.059460304558594176  |Validation err: 0.2975124378109453 , Validation loss:1.5772949326783419 \n",
            "Epoch 29: Train err: 0.012666666666666666, Train loss: 0.04992265396929802  |Validation err: 0.2945273631840796 , Validation loss:1.664762070402503 \n",
            "Epoch 30: Train err: 0.012833333333333334, Train loss: 0.047656319401365645  |Validation err: 0.26865671641791045 , Validation loss:1.478234812617302 \n",
            "Finished Training\n",
            "Total time elapsed: 5172.03 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Jvj-kk8BT9jC",
        "colab_type": "code",
        "outputId": "dfb24e14-78fb-496e-b89b-302f736437d7",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train(netalex, batch_size = 32, learning_rate = 0.001, num_epochs = 15)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1: Train err: 0.8636666666666667, Train loss: 6.696951688604152  |Validation err: 0.8235 , Validation loss:2.198826481425573 \n",
            "Epoch 2: Train err: 0.7846666666666666, Train loss: 2.0924417465291123  |Validation err: 0.7975 , Validation loss:2.510080871127901 \n",
            "Epoch 3: Train err: 0.7626666666666667, Train loss: 2.0622327657456094  |Validation err: 0.7625 , Validation loss:2.4051466385523477 \n",
            "Epoch 4: Train err: 0.751, Train loss: 2.019769406065028  |Validation err: 0.7625 , Validation loss:2.2512343421814935 \n",
            "Epoch 5: Train err: 0.7235, Train loss: 1.9731543495299968  |Validation err: 0.7465 , Validation loss:2.336838175380041 \n",
            "Epoch 6: Train err: 0.6973333333333334, Train loss: 1.9247440936717581  |Validation err: 0.7205 , Validation loss:2.2657625448136103 \n",
            "Epoch 7: Train err: 0.6743333333333333, Train loss: 1.8526354774515679  |Validation err: 0.6925 , Validation loss:2.011690378189087 \n",
            "Epoch 8: Train err: 0.6598333333333334, Train loss: 1.827751729082554  |Validation err: 0.7235 , Validation loss:2.3836690925416493 \n",
            "Epoch 9: Train err: 0.6433333333333333, Train loss: 1.7885482438067173  |Validation err: 0.719 , Validation loss:2.336203590271965 \n",
            "Epoch 10: Train err: 0.6193333333333333, Train loss: 1.743459827088295  |Validation err: 0.704 , Validation loss:2.3837664978844777 \n",
            "Epoch 11: Train err: 0.6101666666666666, Train loss: 1.6957772919472227  |Validation err: 0.7005 , Validation loss:2.4929724647885276 \n",
            "Epoch 12: Train err: 0.5885, Train loss: 1.6535703374984416  |Validation err: 0.645 , Validation loss:2.0353128645155163 \n",
            "Epoch 13: Train err: 0.5641666666666667, Train loss: 1.596399836083676  |Validation err: 0.6535 , Validation loss:2.2003054013327947 \n",
            "Epoch 14: Train err: 0.5616666666666666, Train loss: 1.578344371724636  |Validation err: 0.671 , Validation loss:2.3120113478766546 \n",
            "Epoch 15: Train err: 0.5436666666666666, Train loss: 1.5411840436306405  |Validation err: 0.6545 , Validation loss:2.978955346440512 \n",
            "Finished Training\n",
            "Total time elapsed: 57385.94 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "DdWaCx5WwSTD",
        "colab_type": "code",
        "outputId": "9f208fba-93f3-480e-b9ef-a90624ef5d72",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "cell_type": "code",
      "source": [
        "netalex = AlexNet()\n",
        "netalex.to(device)\n",
        "train(netalex, batch_size = 16, learning_rate = 0.0001, num_epochs = 20)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1: Train err: 0.3865, Train loss: 1.1424561700922378  |Validation err: 0.2865671641791045 , Validation loss:0.9071825702512075 \n",
            "Epoch 2: Train err: 0.197, Train loss: 0.6006449099550856  |Validation err: 0.3686567164179104 , Validation loss:1.1458514368250257 \n",
            "Epoch 3: Train err: 0.11216666666666666, Train loss: 0.35121220500862343  |Validation err: 0.30895522388059704 , Validation loss:1.091131821038231 \n",
            "Epoch 4: Train err: 0.07083333333333333, Train loss: 0.21470273650707083  |Validation err: 0.309452736318408 , Validation loss:1.3678297010206042 \n",
            "Epoch 5: Train err: 0.0425, Train loss: 0.1261425785958133  |Validation err: 0.31840796019900497 , Validation loss:1.5226205663075523 \n",
            "Epoch 6: Train err: 0.03866666666666667, Train loss: 0.11121613052772715  |Validation err: 0.32487562189054725 , Validation loss:1.636424587359504 \n",
            "Epoch 7: Train err: 0.024333333333333332, Train loss: 0.06661355166517674  |Validation err: 0.3238805970149254 , Validation loss:1.791774418618944 \n",
            "Epoch 8: Train err: 0.018833333333333334, Train loss: 0.06231192800592869  |Validation err: 0.27611940298507465 , Validation loss:1.5329324697691298 \n",
            "Epoch 9: Train err: 0.021666666666666667, Train loss: 0.05928395157481762  |Validation err: 0.36069651741293535 , Validation loss:2.203834565622466 \n",
            "Epoch 10: Train err: 0.021166666666666667, Train loss: 0.06568412256843233  |Validation err: 0.27412935323383086 , Validation loss:1.44787502312471 \n",
            "Epoch 11: Train err: 0.0225, Train loss: 0.06399956492191934  |Validation err: 0.26318407960199003 , Validation loss:1.444032204056543 \n",
            "Epoch 12: Train err: 0.012833333333333334, Train loss: 0.03390684236395866  |Validation err: 0.2870646766169154 , Validation loss:1.8885924461342039 \n",
            "Epoch 13: Train err: 0.0205, Train loss: 0.0589857573145406  |Validation err: 0.3482587064676617 , Validation loss:2.278676712560275 \n",
            "Epoch 14: Train err: 0.015, Train loss: 0.045627714085214315  |Validation err: 0.29701492537313434 , Validation loss:1.6853867343493871 \n",
            "Epoch 15: Train err: 0.008666666666666666, Train loss: 0.027287409938079245  |Validation err: 0.33134328358208953 , Validation loss:2.273909136889473 \n",
            "Epoch 16: Train err: 0.017333333333333333, Train loss: 0.047755037672183615  |Validation err: 0.31791044776119404 , Validation loss:1.919470149846304 \n",
            "Epoch 17: Train err: 0.013833333333333333, Train loss: 0.045271610246693836  |Validation err: 0.2796019900497512 , Validation loss:1.6636495147905652 \n",
            "Epoch 18: Train err: 0.007166666666666667, Train loss: 0.025918921138694945  |Validation err: 0.354726368159204 , Validation loss:2.498535850691417 \n",
            "Epoch 19: Train err: 0.0095, Train loss: 0.030174928472099946  |Validation err: 0.2845771144278607 , Validation loss:1.7138834666638147 \n",
            "Epoch 20: Train err: 0.007, Train loss: 0.01943804448510104  |Validation err: 0.3009950248756219 , Validation loss:2.257768635948499 \n",
            "Finished Training\n",
            "Total time elapsed: 2251.39 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "IMPgfAFwBDaV",
        "colab_type": "code",
        "outputId": "35024ada-7cfa-4673-fee2-36236a811045",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        }
      },
      "cell_type": "code",
      "source": [
        "netalex = AlexNet()\n",
        "netalex.to(device)\n",
        "train(netalex, batch_size = 32, learning_rate = 0.0005, num_epochs = 20)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/alexnet-owt-4df8aa71.pth\" to /root/.torch/models/alexnet-owt-4df8aa71.pth\n",
            "244418560it [00:04, 51536996.70it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1: Train err: 0.8931666666666667, Train loss: 2.3129352813071393  |Validation err: 0.900497512437811 , Validation loss:2.3032867908477783 \n",
            "Epoch 2: Train err: 0.878, Train loss: 2.2840839675132263  |Validation err: 0.8766169154228856 , Validation loss:2.2816719819629 \n",
            "Epoch 3: Train err: 0.8566666666666667, Train loss: 2.2538155393397554  |Validation err: 0.8671641791044776 , Validation loss:2.270226516420879 \n",
            "Epoch 4: Train err: 0.8348333333333333, Train loss: 2.218134271337631  |Validation err: 0.836318407960199 , Validation loss:2.219892628609188 \n",
            "Epoch 5: Train err: 0.8061666666666667, Train loss: 2.1489719976770116  |Validation err: 0.8238805970149253 , Validation loss:2.668194231532869 \n",
            "Epoch 6: Train err: 0.7178333333333333, Train loss: 1.9751741493001898  |Validation err: 0.7079601990049751 , Validation loss:2.018897395285349 \n",
            "Epoch 7: Train err: 0.601, Train loss: 1.6993810927614252  |Validation err: 0.6975124378109453 , Validation loss:2.4539715827457487 \n",
            "Epoch 8: Train err: 0.5066666666666667, Train loss: 1.4586954433867272  |Validation err: 0.6059701492537314 , Validation loss:2.1201243608716935 \n",
            "Epoch 9: Train err: 0.39266666666666666, Train loss: 1.163744403326765  |Validation err: 0.5736318407960199 , Validation loss:2.1874502178222412 \n",
            "Epoch 10: Train err: 0.30533333333333335, Train loss: 0.9140308575427278  |Validation err: 0.5373134328358209 , Validation loss:1.9353769364811124 \n",
            "Epoch 11: Train err: 0.24533333333333332, Train loss: 0.7295319923061006  |Validation err: 0.5109452736318408 , Validation loss:2.230195787217882 \n",
            "Epoch 12: Train err: 0.19833333333333333, Train loss: 0.6054298928443421  |Validation err: 0.5447761194029851 , Validation loss:2.7820765820760576 \n",
            "Epoch 13: Train err: 0.141, Train loss: 0.41309425948148076  |Validation err: 0.563681592039801 , Validation loss:2.841241520548624 \n",
            "Epoch 14: Train err: 0.10166666666666667, Train loss: 0.3064268806830366  |Validation err: 0.49850746268656715 , Validation loss:2.3925273740102373 \n",
            "Epoch 15: Train err: 0.09583333333333334, Train loss: 0.28892292637140193  |Validation err: 0.5054726368159204 , Validation loss:2.4206373142817665 \n",
            "Epoch 16: Train err: 0.09133333333333334, Train loss: 0.2855696577895829  |Validation err: 0.554726368159204 , Validation loss:3.927305338874696 \n",
            "Epoch 17: Train err: 0.06366666666666666, Train loss: 0.20071197994687456  |Validation err: 0.5293532338308458 , Validation loss:3.3920124390768627 \n",
            "Epoch 18: Train err: 0.06116666666666667, Train loss: 0.1806436878173275  |Validation err: 0.4766169154228856 , Validation loss:2.89788341900659 \n",
            "Epoch 19: Train err: 0.05616666666666666, Train loss: 0.17415978815010252  |Validation err: 0.4925373134328358 , Validation loss:3.1044607276008245 \n",
            "Epoch 20: Train err: 0.052, Train loss: 0.15994947593896947  |Validation err: 0.4701492537313433 , Validation loss:3.193839967250824 \n",
            "Finished Training\n",
            "Total time elapsed: 3364.05 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "jFCR_AXLR4fZ",
        "colab_type": "code",
        "outputId": "67884d5a-cb82-42c9-82bb-93cd33019d65",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "cell_type": "code",
      "source": [
        "netalex = AlexNet()\n",
        "netalex.to(device)\n",
        "train(netalex, batch_size = 64, learning_rate = 0.0005, num_epochs = 20)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1: Train err: 0.8835, Train loss: 2.320931708559077  |Validation err: 0.8791044776119403 , Validation loss:2.2924960075862826 \n",
            "Epoch 2: Train err: 0.8613333333333333, Train loss: 2.2638895029717303  |Validation err: 0.8771144278606965 , Validation loss:2.285294078645252 \n",
            "Epoch 3: Train err: 0.828, Train loss: 2.179639473874518  |Validation err: 0.818407960199005 , Validation loss:2.2222018563558184 \n",
            "Epoch 4: Train err: 0.7263333333333334, Train loss: 1.9586204531344962  |Validation err: 0.6567164179104478 , Validation loss:1.811238551896716 \n",
            "Epoch 5: Train err: 0.6023333333333334, Train loss: 1.652762827721048  |Validation err: 0.6343283582089553 , Validation loss:1.8037584414557806 \n",
            "Epoch 6: Train err: 0.473, Train loss: 1.3626352485190047  |Validation err: 0.508955223880597 , Validation loss:1.6825155871255058 \n",
            "Epoch 7: Train err: 0.3635, Train loss: 1.0854070890457073  |Validation err: 0.5084577114427861 , Validation loss:1.517351762642936 \n",
            "Epoch 8: Train err: 0.30366666666666664, Train loss: 0.9126868083121928  |Validation err: 0.5348258706467661 , Validation loss:1.8191457002881974 \n",
            "Epoch 9: Train err: 0.24583333333333332, Train loss: 0.7387305396668454  |Validation err: 0.5069651741293533 , Validation loss:1.7135927942064073 \n",
            "Epoch 10: Train err: 0.20366666666666666, Train loss: 0.6041051482266568  |Validation err: 0.4412935323383085 , Validation loss:1.665384529128907 \n",
            "Epoch 11: Train err: 0.156, Train loss: 0.47282060790569225  |Validation err: 0.4417910447761194 , Validation loss:1.9173926029886519 \n",
            "Epoch 12: Train err: 0.11416666666666667, Train loss: 0.3487686586189777  |Validation err: 0.4621890547263682 , Validation loss:1.9606360367366247 \n",
            "Epoch 13: Train err: 0.09033333333333333, Train loss: 0.2803966020967098  |Validation err: 0.49402985074626865 , Validation loss:2.516180313768841 \n",
            "Epoch 14: Train err: 0.08266666666666667, Train loss: 0.25980742299493326  |Validation err: 0.46616915422885574 , Validation loss:2.1155306233300104 \n",
            "Epoch 15: Train err: 0.07216666666666667, Train loss: 0.22145704477232822  |Validation err: 0.4616915422885572 , Validation loss:2.203220488533141 \n",
            "Epoch 16: Train err: 0.057166666666666664, Train loss: 0.17086679072297634  |Validation err: 0.47512437810945274 , Validation loss:3.607003338753231 \n",
            "Epoch 17: Train err: 0.07183333333333333, Train loss: 0.23357124115399858  |Validation err: 0.42338308457711443 , Validation loss:1.8753071361117892 \n",
            "Epoch 18: Train err: 0.05516666666666667, Train loss: 0.16776240161283218  |Validation err: 0.4691542288557214 , Validation loss:2.6052251410862755 \n",
            "Epoch 19: Train err: 0.041666666666666664, Train loss: 0.12279086260798763  |Validation err: 0.4577114427860697 , Validation loss:3.5813440917030213 \n",
            "Epoch 20: Train err: 0.025666666666666667, Train loss: 0.07824255193167544  |Validation err: 0.4462686567164179 , Validation loss:3.038092255592346 \n",
            "Finished Training\n",
            "Total time elapsed: 2039.96 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "igsqV0AeSBu5",
        "colab_type": "code",
        "outputId": "a6465f56-c80c-4c0a-8a83-1d9828c9dfbd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "cell_type": "code",
      "source": [
        "netalex = AlexNet()\n",
        "netalex.to(device)\n",
        "train(netalex, batch_size = 32, learning_rate = 0.00005, num_epochs = 20)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1: Train err: 0.4196666666666667, Train loss: 1.2426844055348254  |Validation err: 0.2990049751243781 , Validation loss:0.920150447459448 \n",
            "Epoch 2: Train err: 0.219, Train loss: 0.6540741188094971  |Validation err: 0.3567164179104478 , Validation loss:1.1191261514784798 \n",
            "Epoch 3: Train err: 0.156, Train loss: 0.473246028765719  |Validation err: 0.2860696517412935 , Validation loss:0.926016146228427 \n",
            "Epoch 4: Train err: 0.10583333333333333, Train loss: 0.3268153307602761  |Validation err: 0.3119402985074627 , Validation loss:1.1126495798428853 \n",
            "Epoch 5: Train err: 0.06833333333333333, Train loss: 0.21071445062122446  |Validation err: 0.31144278606965176 , Validation loss:1.370934506257375 \n",
            "Epoch 6: Train err: 0.0505, Train loss: 0.15983863476108998  |Validation err: 0.2716417910447761 , Validation loss:1.151049964957767 \n",
            "Epoch 7: Train err: 0.031, Train loss: 0.09941372552767713  |Validation err: 0.2796019900497512 , Validation loss:1.213928859385233 \n",
            "Epoch 8: Train err: 0.028166666666666666, Train loss: 0.08551554068764473  |Validation err: 0.27761194029850744 , Validation loss:1.2897675836843157 \n",
            "Epoch 9: Train err: 0.02266666666666667, Train loss: 0.06321876139399853  |Validation err: 0.2840796019900497 , Validation loss:1.3508467986470176 \n",
            "Epoch 10: Train err: 0.015, Train loss: 0.04599705211659695  |Validation err: 0.30895522388059704 , Validation loss:1.6540270286893088 \n",
            "Epoch 11: Train err: 0.019666666666666666, Train loss: 0.05216296407532819  |Validation err: 0.2766169154228856 , Validation loss:1.5321917420341855 \n",
            "Epoch 12: Train err: 0.0115, Train loss: 0.036615117116177334  |Validation err: 0.3288557213930348 , Validation loss:1.9861294542040144 \n",
            "Epoch 13: Train err: 0.008666666666666666, Train loss: 0.03117739159534586  |Validation err: 0.2945273631840796 , Validation loss:1.6232731451117803 \n",
            "Epoch 14: Train err: 0.012833333333333334, Train loss: 0.039541525716715036  |Validation err: 0.27412935323383086 , Validation loss:1.5511488337365409 \n",
            "Epoch 15: Train err: 0.012666666666666666, Train loss: 0.038029459721230445  |Validation err: 0.34129353233830845 , Validation loss:2.2117845444452193 \n",
            "Epoch 16: Train err: 0.0095, Train loss: 0.0312893685211051  |Validation err: 0.2865671641791045 , Validation loss:1.7389576340478563 \n",
            "Epoch 17: Train err: 0.0065, Train loss: 0.021654769476067197  |Validation err: 0.2915422885572139 , Validation loss:1.929986574347057 \n",
            "Epoch 18: Train err: 0.011166666666666667, Train loss: 0.03174256542548934  |Validation err: 0.2721393034825871 , Validation loss:1.5763731438016135 \n",
            "Epoch 19: Train err: 0.012166666666666666, Train loss: 0.035264190776828436  |Validation err: 0.30646766169154227 , Validation loss:1.904491576883528 \n",
            "Epoch 20: Train err: 0.006333333333333333, Train loss: 0.01975536217140232  |Validation err: 0.3044776119402985 , Validation loss:2.049412723571535 \n",
            "Finished Training\n",
            "Total time elapsed: 2066.40 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ES9xsuzFniNJ",
        "colab_type": "code",
        "outputId": "6c896984-9dc6-48b1-a15c-8e26e522e925",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "cell_type": "code",
      "source": [
        "netalex = AlexNet()\n",
        "netalex.to(device)\n",
        "train(netalex, batch_size = 64, learning_rate = 0.00005, num_epochs = 20)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1: Train err: 0.41883333333333334, Train loss: 1.2419434785842896  |Validation err: 0.3034825870646766 , Validation loss:0.9177590764704204 \n",
            "Epoch 2: Train err: 0.22016666666666668, Train loss: 0.657989907454937  |Validation err: 0.35124378109452736 , Validation loss:1.1080554543979584 \n",
            "Epoch 3: Train err: 0.15733333333333333, Train loss: 0.47340591885942096  |Validation err: 0.291044776119403 , Validation loss:0.9185986149878729 \n",
            "Epoch 4: Train err: 0.103, Train loss: 0.32332505919831867  |Validation err: 0.31044776119402984 , Validation loss:1.113845521022403 \n",
            "Epoch 5: Train err: 0.0685, Train loss: 0.21185640697466565  |Validation err: 0.3054726368159204 , Validation loss:1.3256247895104545 \n",
            "Epoch 6: Train err: 0.051, Train loss: 0.15742263427757203  |Validation err: 0.2845771144278607 , Validation loss:1.2137668506493644 \n",
            "Epoch 7: Train err: 0.029833333333333333, Train loss: 0.09628963585388153  |Validation err: 0.2860696517412935 , Validation loss:1.2755623246942247 \n",
            "Epoch 8: Train err: 0.03283333333333333, Train loss: 0.09294345099082653  |Validation err: 0.27810945273631843 , Validation loss:1.2632609028664847 \n",
            "Epoch 9: Train err: 0.0265, Train loss: 0.0734409973659414  |Validation err: 0.2880597014925373 , Validation loss:1.3908424524087755 \n",
            "Epoch 10: Train err: 0.009, Train loss: 0.03698920553669016  |Validation err: 0.30199004975124377 , Validation loss:1.5953040969750238 \n",
            "Epoch 11: Train err: 0.015, Train loss: 0.04418945296647701  |Validation err: 0.29850746268656714 , Validation loss:1.7095538945425124 \n",
            "Epoch 12: Train err: 0.01, Train loss: 0.031203043845264202  |Validation err: 0.32238805970149254 , Validation loss:1.9997996843996502 \n",
            "Epoch 13: Train err: 0.012, Train loss: 0.03817845912689858  |Validation err: 0.33930348258706466 , Validation loss:2.1710558495824297 \n",
            "Epoch 14: Train err: 0.009833333333333333, Train loss: 0.030824665668757356  |Validation err: 0.2716417910447761 , Validation loss:1.5995147076864091 \n",
            "Epoch 15: Train err: 0.013166666666666667, Train loss: 0.03718106028564433  |Validation err: 0.34378109452736316 , Validation loss:1.9989472873627194 \n",
            "Epoch 16: Train err: 0.0095, Train loss: 0.027039019603955935  |Validation err: 0.28955223880597014 , Validation loss:1.81055862562997 \n",
            "Epoch 17: Train err: 0.008833333333333334, Train loss: 0.03018204118818679  |Validation err: 0.3288557213930348 , Validation loss:2.166573155493963 \n",
            "Epoch 18: Train err: 0.014833333333333334, Train loss: 0.04339582761105625  |Validation err: 0.32686567164179103 , Validation loss:1.9392749922616142 \n",
            "Epoch 19: Train err: 0.009333333333333334, Train loss: 0.03032981389340885  |Validation err: 0.3492537313432836 , Validation loss:2.3371407768082997 \n",
            "Epoch 20: Train err: 0.007333333333333333, Train loss: 0.02515120333694714  |Validation err: 0.2810945273631841 , Validation loss:1.5031843038778456 \n",
            "Finished Training\n",
            "Total time elapsed: 2069.56 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "OH8PelmRB4TP",
        "colab_type": "code",
        "outputId": "afef8229-e002-4b05-bdd2-c4a79148fb62",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "cell_type": "code",
      "source": [
        "netalex2 = AlexNet()\n",
        "netalex2.to(device)\n",
        "train(netalex2, batch_size = 32, learning_rate = 0.00001, num_epochs = 20)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1: Train err: 0.5936666666666667, Train loss: 1.809698923471126  |Validation err: 0.41691542288557215 , Validation loss:1.226113047864702 \n",
            "Epoch 2: Train err: 0.3368333333333333, Train loss: 1.013751449103051  |Validation err: 0.3497512437810945 , Validation loss:1.0828766529522245 \n",
            "Epoch 3: Train err: 0.26616666666666666, Train loss: 0.8230058829835121  |Validation err: 0.345273631840796 , Validation loss:1.0834999680519104 \n",
            "Epoch 4: Train err: 0.23733333333333334, Train loss: 0.7154985222410648  |Validation err: 0.3338308457711443 , Validation loss:1.059084074837821 \n",
            "Epoch 5: Train err: 0.207, Train loss: 0.6225019794829348  |Validation err: 0.3174129353233831 , Validation loss:1.0316127169699896 \n",
            "Epoch 6: Train err: 0.18166666666666667, Train loss: 0.5609518210304544  |Validation err: 0.3074626865671642 , Validation loss:1.0131385222313896 \n",
            "Epoch 7: Train err: 0.17233333333333334, Train loss: 0.5021994854224489  |Validation err: 0.29701492537313434 , Validation loss:0.9666019645002153 \n",
            "Epoch 8: Train err: 0.14466666666666667, Train loss: 0.4493910616382639  |Validation err: 0.2935323383084577 , Validation loss:0.9925519947021727 \n",
            "Epoch 9: Train err: 0.128, Train loss: 0.40545543362485603  |Validation err: 0.30696517412935326 , Validation loss:1.1310163159219047 \n",
            "Epoch 10: Train err: 0.12433333333333334, Train loss: 0.3651692119050533  |Validation err: 0.2955223880597015 , Validation loss:1.065392796009306 \n",
            "Epoch 11: Train err: 0.10516666666666667, Train loss: 0.3297262526255973  |Validation err: 0.30149253731343284 , Validation loss:1.114138564420125 \n",
            "Epoch 12: Train err: 0.098, Train loss: 0.2951604720759899  |Validation err: 0.31840796019900497 , Validation loss:1.209966254612756 \n",
            "Epoch 13: Train err: 0.086, Train loss: 0.2583678243483635  |Validation err: 0.30398009950248756 , Validation loss:1.230717172698369 \n",
            "Epoch 14: Train err: 0.0775, Train loss: 0.22981409752305518  |Validation err: 0.30597014925373134 , Validation loss:1.2703235977225833 \n",
            "Epoch 15: Train err: 0.06383333333333334, Train loss: 0.20499081925508825  |Validation err: 0.2975124378109453 , Validation loss:1.1992925736639235 \n",
            "Epoch 16: Train err: 0.054, Train loss: 0.17403551841035803  |Validation err: 0.28756218905472636 , Validation loss:1.1508500017817058 \n",
            "Epoch 17: Train err: 0.0535, Train loss: 0.16133089545876422  |Validation err: 0.2835820895522388 , Validation loss:1.247922380765279 \n",
            "Epoch 18: Train err: 0.042833333333333334, Train loss: 0.13750680924413053  |Validation err: 0.29203980099502486 , Validation loss:1.3094454923319438 \n",
            "Epoch 19: Train err: 0.042333333333333334, Train loss: 0.13140984917891788  |Validation err: 0.3044776119402985 , Validation loss:1.4734993463470822 \n",
            "Epoch 20: Train err: 0.03716666666666667, Train loss: 0.1164837407859716  |Validation err: 0.2885572139303483 , Validation loss:1.3569671776559618 \n",
            "Finished Training\n",
            "Total time elapsed: 2193.26 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "WG-dQfq230r5",
        "colab_type": "code",
        "outputId": "e826b442-2310-47bf-8409-84a331c46eb7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "cell_type": "code",
      "source": [
        "netalex2 = AlexNet()\n",
        "netalex2.to(device)\n",
        "train(netalex2, batch_size = 32, learning_rate = 0.000005, num_epochs = 20)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1: Train err: 0.7033333333333334, Train loss: 2.0962643889670676  |Validation err: 0.5358208955223881 , Validation loss:1.6299511988957722 \n",
            "Epoch 2: Train err: 0.4156666666666667, Train loss: 1.342432473568206  |Validation err: 0.4223880597014925 , Validation loss:1.2120010058085124 \n",
            "Epoch 3: Train err: 0.33916666666666667, Train loss: 1.0346959842012284  |Validation err: 0.3925373134328358 , Validation loss:1.2089395097323827 \n",
            "Epoch 4: Train err: 0.29483333333333334, Train loss: 0.9154414857955689  |Validation err: 0.354228855721393 , Validation loss:1.0923599704863534 \n",
            "Epoch 5: Train err: 0.27, Train loss: 0.8197406967903705  |Validation err: 0.33880597014925373 , Validation loss:1.0811677045292325 \n",
            "Epoch 6: Train err: 0.25066666666666665, Train loss: 0.767062412931564  |Validation err: 0.3497512437810945 , Validation loss:1.0846690213869488 \n",
            "Epoch 7: Train err: 0.23166666666666666, Train loss: 0.7002718274263625  |Validation err: 0.336318407960199 , Validation loss:1.087264839618925 \n",
            "Epoch 8: Train err: 0.21816666666666668, Train loss: 0.6669354302451965  |Validation err: 0.32338308457711445 , Validation loss:1.0256766227502672 \n",
            "Epoch 9: Train err: 0.20433333333333334, Train loss: 0.6224860631405039  |Validation err: 0.3144278606965174 , Validation loss:1.0188905284518288 \n",
            "Epoch 10: Train err: 0.19016666666666668, Train loss: 0.581495648368876  |Validation err: 0.2980099502487562 , Validation loss:1.0008825848972986 \n",
            "Epoch 11: Train err: 0.17283333333333334, Train loss: 0.5359968175279334  |Validation err: 0.3054726368159204 , Validation loss:1.0438260795578125 \n",
            "Epoch 12: Train err: 0.171, Train loss: 0.5187038519281022  |Validation err: 0.32686567164179103 , Validation loss:1.0790726050497994 \n",
            "Epoch 13: Train err: 0.15483333333333332, Train loss: 0.47921800074425147  |Validation err: 0.30895522388059704 , Validation loss:1.0721513822911277 \n",
            "Epoch 14: Train err: 0.15016666666666667, Train loss: 0.457094687889231  |Validation err: 0.3194029850746269 , Validation loss:1.1331724041984195 \n",
            "Epoch 15: Train err: 0.1435, Train loss: 0.4359391599576524  |Validation err: 0.3049751243781095 , Validation loss:1.0948609227225894 \n",
            "Epoch 16: Train err: 0.13083333333333333, Train loss: 0.40745716113993463  |Validation err: 0.32487562189054725 , Validation loss:1.1847838466129605 \n",
            "Epoch 17: Train err: 0.126, Train loss: 0.37976542891974147  |Validation err: 0.29253731343283584 , Validation loss:1.0505517811056166 \n",
            "Epoch 18: Train err: 0.11983333333333333, Train loss: 0.36004361320049205  |Validation err: 0.30597014925373134 , Validation loss:1.134976575298915 \n",
            "Epoch 19: Train err: 0.11016666666666666, Train loss: 0.33718803461561814  |Validation err: 0.2845771144278607 , Validation loss:1.0858088755418385 \n",
            "Epoch 20: Train err: 0.1015, Train loss: 0.31902012355784154  |Validation err: 0.2840796019900497 , Validation loss:1.0709192128408522 \n",
            "Finished Training\n",
            "Total time elapsed: 2064.58 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "TohhgGDfT9jL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Test Accuracy-Alex Net\n",
        "Report the test accuracy of your best model. How does the test accuracy compare to part 4(d)?"
      ]
    },
    {
      "metadata": {
        "id": "fX3IqNMNT9jM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_accuracy(model):\n",
        "    data = testset\n",
        "    total = 0\n",
        "    correct =0\n",
        "    for imgs, labels in torch.utils.data.DataLoader(data, batch_size=32):\n",
        "        features = myfeature_model(imgs)\n",
        "        output = net(features) # We don't need to run F.softmax\n",
        "        pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
        "        correct += pred.eq(labels.view_as(pred)).sum().item()\n",
        "        total += imgs.shape[0]\n",
        "    return correct / total"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "X8h6ndabT9jY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "get_accuracy(net)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "p7NvjfYEiXIa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **ResNet18 **"
      ]
    },
    {
      "metadata": {
        "id": "I5pjyDPQk39-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from torchvision import transforms\n",
        "\n",
        "# Set up a transform that scales and crops an image so it has the dimensions\n",
        "# of the input layer of alexnet\n",
        "scale_crop = transforms.Compose([\n",
        "   transforms.Resize(256),\n",
        "   transforms.CenterCrop(224)\n",
        "])\n",
        "\n",
        "# The normalization that was applied to the data when alexnet was trained\n",
        "normalize = transforms.Normalize(\n",
        "   mean=[0.485, 0.456, 0.406],\n",
        "   std=[0.229, 0.224, 0.225]\n",
        ")\n",
        "\n",
        "# Put scaling, cropping, normalzing and converting from PIL image to pytorch into one package\n",
        "preprocess = transforms.Compose([\n",
        "   scale_crop,\n",
        "   transforms.ToTensor(),\n",
        "   normalize\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5FWLZcbvk-EF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "trainset = torchvision.datasets.ImageFolder(root = 'train',\n",
        "                                             transform=preprocess)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(trainset, batch_size = 32,\n",
        "                                          num_workers = 1, shuffle = True)\n",
        "val_set = torchvision.datasets.ImageFolder(root='val',\n",
        "                                                 transform=preprocess)\n",
        "val_loader = torch.utils.data.DataLoader(val_set, batch_size=32,\n",
        "                                              num_workers=1, shuffle = True)\n",
        "\n",
        "testset = torchvision.datasets.ImageFolder(root= 'test',\n",
        "                                           transform=transform)\n",
        "    \n",
        "test_loader = torch.utils.data.DataLoader(testset, batch_size=32,\n",
        "                                             num_workers=1, shuffle = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cju5qMKUieyQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import math\n",
        "import torch.utils.model_zoo as model_zoo\n",
        "\n",
        "\n",
        "__all__ = ['ResNet', 'resnet18', 'resnet34', 'resnet50', 'resnet101',\n",
        "           'resnet152']\n",
        "\n",
        "\n",
        "model_urls = {\n",
        "    'resnet18': 'https://download.pytorch.org/models/resnet18-5c106cde.pth',\n",
        "    'resnet34': 'https://download.pytorch.org/models/resnet34-333f7ec4.pth',\n",
        "    'resnet50': 'https://download.pytorch.org/models/resnet50-19c8e357.pth',\n",
        "    'resnet101': 'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth',\n",
        "    'resnet152': 'https://download.pytorch.org/models/resnet152-b121ed2d.pth',\n",
        "}\n",
        "\n",
        "\n",
        "def conv3x3(in_planes, out_planes, stride=1):\n",
        "    \"3x3 convolution with padding\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
        "                     padding=1, bias=False)\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = conv3x3(planes, planes)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n",
        "                               padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(planes * 4)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "\n",
        "    def __init__(self, block, layers, num_classes=10):\n",
        "        self.inplanes = 64\n",
        "        super(ResNet, self).__init__()\n",
        "        self.name = \"resNet\"\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n",
        "                               bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
        "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
        "        self.avgpool = nn.AvgPool2d(7)\n",
        "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, stride=1):\n",
        "        downsample = None\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(planes * block.expansion),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
        "        self.inplanes = planes * block.expansion\n",
        "        for i in range(1, blocks):\n",
        "            layers.append(block(self.inplanes, planes))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "def resnet18(pretrained=False, **kwargs):\n",
        "    \"\"\"Constructs a ResNet-18 model.\n",
        "\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "    \"\"\"\n",
        "    model = ResNet(BasicBlock, [2, 2, 2, 2], **kwargs)\n",
        "    if pretrained:\n",
        "        model.load_state_dict(model_zoo.load_url(model_urls['resnet18']))\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "def resnet34(pretrained=False, **kwargs):\n",
        "    \"\"\"Constructs a ResNet-34 model.\n",
        "\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "    \"\"\"\n",
        "    model = ResNet(BasicBlock, [3, 4, 6, 3], **kwargs)\n",
        "    if pretrained:\n",
        "        model.load_state_dict(model_zoo.load_url(model_urls['resnet34']))\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def resnet50(pretrained=False, **kwargs):\n",
        "    \"\"\"Constructs a ResNet-50 model.\n",
        "\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "    \"\"\"\n",
        "    model = ResNet(Bottleneck, [3, 4, 6, 3], **kwargs)\n",
        "    if pretrained:\n",
        "        model.load_state_dict(model_zoo.load_url(model_urls['resnet50']))\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def resnet101(pretrained=False, **kwargs):\n",
        "    \"\"\"Constructs a ResNet-101 model.\n",
        "\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "    \"\"\"\n",
        "    model = ResNet(Bottleneck, [3, 4, 23, 3], **kwargs)\n",
        "    if pretrained:\n",
        "        model.load_state_dict(model_zoo.load_url(model_urls['resnet101']))\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def resnet152(pretrained=False, **kwargs):\n",
        "    \"\"\"Constructs a ResNet-152 model.\n",
        "\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "    \"\"\"\n",
        "    model = ResNet(Bottleneck, [3, 8, 36, 3], **kwargs)\n",
        "    if pretrained:\n",
        "        model.load_state_dict(model_zoo.load_url(model_urls['resnet152']))\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Z2_pYqJPio91",
        "colab_type": "code",
        "outputId": "1014bd67-7478-4722-d149-f84564027f25",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        }
      },
      "cell_type": "code",
      "source": [
        "res18net = resnet18(num_classes=10)\n",
        "res18net.to(device)\n",
        "train(res18net, batch_size = 32, learning_rate = 0.0001, num_epochs = 20)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1: Train err: 0.661, Train loss: 1.8924471507681178  |Validation err: 0.5726368159203981 , Validation loss:1.689476768175761 \n",
            "Epoch 2: Train err: 0.523, Train loss: 1.5235189364311543  |Validation err: 0.5482587064676617 , Validation loss:1.6152919493024311 \n",
            "Epoch 3: Train err: 0.414, Train loss: 1.2366426790014227  |Validation err: 0.5114427860696518 , Validation loss:1.5224968308494204 \n",
            "Epoch 4: Train err: 0.2713333333333333, Train loss: 0.8496165484824079  |Validation err: 0.49900497512437814 , Validation loss:1.542760585981702 \n",
            "Epoch 5: Train err: 0.102, Train loss: 0.40856025478941327  |Validation err: 0.5208955223880597 , Validation loss:1.6961960016735016 \n",
            "Epoch 6: Train err: 0.0375, Train loss: 0.180067962471475  |Validation err: 0.5129353233830846 , Validation loss:1.7391987906561956 \n",
            "Epoch 7: Train err: 0.011, Train loss: 0.08696742569829555  |Validation err: 0.5144278606965174 , Validation loss:1.834957066036406 \n",
            "Epoch 8: Train err: 0.0021666666666666666, Train loss: 0.03521799393236003  |Validation err: 0.5124378109452736 , Validation loss:1.8467240863376193 \n",
            "Epoch 9: Train err: 0.0005, Train loss: 0.015204935017576877  |Validation err: 0.49402985074626865 , Validation loss:1.8503709444924006 \n",
            "Epoch 10: Train err: 0.00016666666666666666, Train loss: 0.008861232350798363  |Validation err: 0.5064676616915423 , Validation loss:1.8718297443692646 \n",
            "Epoch 11: Train err: 0.0, Train loss: 0.008333456207146036  |Validation err: 0.5019900497512437 , Validation loss:1.8556372315164595 \n",
            "Epoch 12: Train err: 0.0, Train loss: 0.0053771689454925825  |Validation err: 0.48706467661691544 , Validation loss:1.8692023356755574 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "GkDPxbhfYaVA",
        "colab_type": "code",
        "outputId": "7c63a2ff-0c41-46e5-d1fe-6a5ec5834ec3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "cell_type": "code",
      "source": [
        "res18net = resnet18(num_classes=10)\n",
        "res18net.to(device)\n",
        "train(res18net, batch_size = 32, learning_rate = 0.00001, num_epochs = 20)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1: Train err: 0.8385, Train loss: 2.2299749686362897  |Validation err: 0.7328358208955223 , Validation loss:2.0489529863236444 \n",
            "Epoch 2: Train err: 0.707, Train loss: 1.978893595807096  |Validation err: 0.6676616915422886 , Validation loss:1.9167451347623552 \n",
            "Epoch 3: Train err: 0.6308333333333334, Train loss: 1.8412276011832216  |Validation err: 0.6283582089552239 , Validation loss:1.8465531685995678 \n",
            "Epoch 4: Train err: 0.5855, Train loss: 1.7314548530477158  |Validation err: 0.6014925373134329 , Validation loss:1.7689531227898976 \n",
            "Epoch 5: Train err: 0.5445, Train loss: 1.626549836168898  |Validation err: 0.5825870646766169 , Validation loss:1.7276400762891013 \n",
            "Epoch 6: Train err: 0.5155, Train loss: 1.5440139364688954  |Validation err: 0.5761194029850746 , Validation loss:1.6973343046884688 \n",
            "Epoch 7: Train err: 0.4866666666666667, Train loss: 1.4642740944598585  |Validation err: 0.5671641791044776 , Validation loss:1.6498697277099368 \n",
            "Epoch 8: Train err: 0.457, Train loss: 1.381351706829477  |Validation err: 0.5507462686567164 , Validation loss:1.6193241429707361 \n",
            "Epoch 9: Train err: 0.42283333333333334, Train loss: 1.2974385587458914  |Validation err: 0.5328358208955224 , Validation loss:1.5974820208927942 \n",
            "Epoch 10: Train err: 0.38333333333333336, Train loss: 1.2113910634466942  |Validation err: 0.5348258706467661 , Validation loss:1.5767280487787156 \n",
            "Epoch 11: Train err: 0.3525, Train loss: 1.1384583489691957  |Validation err: 0.5199004975124378 , Validation loss:1.5532439947128296 \n",
            "Epoch 12: Train err: 0.32083333333333336, Train loss: 1.0482798947932872  |Validation err: 0.5243781094527363 , Validation loss:1.5563516351911757 \n",
            "Epoch 13: Train err: 0.2755, Train loss: 0.9527531944690867  |Validation err: 0.5084577114427861 , Validation loss:1.5243884419638014 \n",
            "Epoch 14: Train err: 0.24383333333333335, Train loss: 0.8736649846777003  |Validation err: 0.5194029850746269 , Validation loss:1.525976192383539 \n",
            "Epoch 15: Train err: 0.207, Train loss: 0.7806422824555255  |Validation err: 0.5318407960199005 , Validation loss:1.5686074684536646 \n",
            "Epoch 16: Train err: 0.16683333333333333, Train loss: 0.6968187417121644  |Validation err: 0.5184079601990049 , Validation loss:1.5498980283737183 \n",
            "Epoch 17: Train err: 0.14183333333333334, Train loss: 0.6191945450103029  |Validation err: 0.5199004975124378 , Validation loss:1.5540566274097987 \n",
            "Epoch 18: Train err: 0.1115, Train loss: 0.540587676332352  |Validation err: 0.5149253731343284 , Validation loss:1.5613504345454867 \n",
            "Epoch 19: Train err: 0.08683333333333333, Train loss: 0.46897771447262865  |Validation err: 0.5084577114427861 , Validation loss:1.5625593728489346 \n",
            "Epoch 20: Train err: 0.0605, Train loss: 0.3984356205514137  |Validation err: 0.5208955223880597 , Validation loss:1.6195300997249664 \n",
            "Finished Training\n",
            "Total time elapsed: 2996.10 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "HG9YYKGpjzkK",
        "colab_type": "code",
        "outputId": "c578049c-9ed0-4c34-d6d6-d5b012a73d9e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "cell_type": "code",
      "source": [
        "res18net = resnet18(num_classes=10)\n",
        "res18net.to(device)\n",
        "train(res18net, batch_size = 32, learning_rate = 0.00005, num_epochs = 20)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1: Train err: 0.6908333333333333, Train loss: 1.9662996327623408  |Validation err: 0.5990049751243781 , Validation loss:1.7568683662111797 \n",
            "Epoch 2: Train err: 0.5445, Train loss: 1.61964606097404  |Validation err: 0.5686567164179105 , Validation loss:1.6615479522281222 \n",
            "Epoch 3: Train err: 0.45916666666666667, Train loss: 1.381170998228357  |Validation err: 0.5308457711442786 , Validation loss:1.5866614920752389 \n",
            "Epoch 4: Train err: 0.37566666666666665, Train loss: 1.1511684104483177  |Validation err: 0.5268656716417911 , Validation loss:1.5478026677691747 \n",
            "Epoch 5: Train err: 0.25416666666666665, Train loss: 0.8416564229955065  |Validation err: 0.4925373134328358 , Validation loss:1.5212908396645197 \n",
            "Epoch 6: Train err: 0.13583333333333333, Train loss: 0.547311969576998  |Validation err: 0.5144278606965174 , Validation loss:1.603037300563994 \n",
            "Epoch 7: Train err: 0.052833333333333336, Train loss: 0.30130988724054175  |Validation err: 0.5213930348258706 , Validation loss:1.6833469546030437 \n",
            "Epoch 8: Train err: 0.024333333333333332, Train loss: 0.16607478039061768  |Validation err: 0.554228855721393 , Validation loss:1.8256035986400785 \n",
            "Epoch 9: Train err: 0.009833333333333333, Train loss: 0.0919919316359657  |Validation err: 0.5213930348258706 , Validation loss:1.771414005567157 \n",
            "Epoch 10: Train err: 0.0018333333333333333, Train loss: 0.04655535705387592  |Validation err: 0.5238805970149254 , Validation loss:1.8657179200460041 \n",
            "Epoch 11: Train err: 0.0028333333333333335, Train loss: 0.04834800201686139  |Validation err: 0.545771144278607 , Validation loss:1.9651631457465035 \n",
            "Epoch 12: Train err: 0.0013333333333333333, Train loss: 0.03116945874817828  |Validation err: 0.5228855721393035 , Validation loss:1.8672488322333685 \n",
            "Epoch 13: Train err: 0.0008333333333333334, Train loss: 0.01984282256044606  |Validation err: 0.5313432835820896 , Validation loss:1.8998045940247794 \n",
            "Epoch 14: Train err: 0.0008333333333333334, Train loss: 0.01870184249066292  |Validation err: 0.5169154228855721 , Validation loss:1.8768815331988864 \n",
            "Epoch 15: Train err: 0.00016666666666666666, Train loss: 0.013820126256410112  |Validation err: 0.5184079601990049 , Validation loss:1.9277978265096272 \n",
            "Epoch 16: Train err: 0.0, Train loss: 0.011171344489334746  |Validation err: 0.5109452736318408 , Validation loss:1.9404576316712394 \n",
            "Epoch 17: Train err: 0.0, Train loss: 0.008980891939767815  |Validation err: 0.527860696517413 , Validation loss:1.9588597899391538 \n",
            "Epoch 18: Train err: 0.0, Train loss: 0.006851703577496587  |Validation err: 0.508955223880597 , Validation loss:1.9543822871314154 \n",
            "Epoch 19: Train err: 0.0, Train loss: 0.005993045945750906  |Validation err: 0.5114427860696518 , Validation loss:1.9728037043223305 \n",
            "Epoch 20: Train err: 0.0, Train loss: 0.005856075502456503  |Validation err: 0.5154228855721393 , Validation loss:1.9934155846398973 \n",
            "Finished Training\n",
            "Total time elapsed: 2092.80 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "xEBbqZsFux_5",
        "colab_type": "code",
        "outputId": "5157797c-4fd7-4aa6-83c7-d4fd83b1ac7c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "cell_type": "code",
      "source": [
        "res18net = resnet18(num_classes=10)\n",
        "res18net.to(device)\n",
        "train(res18net, batch_size = 16, learning_rate = 0.00005, num_epochs = 20)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1: Train err: 0.6883333333333334, Train loss: 1.9665132228364335  |Validation err: 0.5940298507462687 , Validation loss:1.7574835353427463 \n",
            "Epoch 2: Train err: 0.5445, Train loss: 1.6201823670813378  |Validation err: 0.5706467661691542 , Validation loss:1.6646455602040366 \n",
            "Epoch 3: Train err: 0.4608333333333333, Train loss: 1.3846958178154967  |Validation err: 0.5333333333333333 , Validation loss:1.579527665698339 \n",
            "Epoch 4: Train err: 0.37316666666666665, Train loss: 1.1520917707301201  |Validation err: 0.517910447761194 , Validation loss:1.531101735811385 \n",
            "Epoch 5: Train err: 0.251, Train loss: 0.8413121446650079  |Validation err: 0.4945273631840796 , Validation loss:1.527711444430881 \n",
            "Epoch 6: Train err: 0.14033333333333334, Train loss: 0.5439721184208038  |Validation err: 0.5199004975124378 , Validation loss:1.619893953913734 \n",
            "Epoch 7: Train err: 0.0565, Train loss: 0.301407719704699  |Validation err: 0.53681592039801 , Validation loss:1.7141874820467025 \n",
            "Epoch 8: Train err: 0.02266666666666667, Train loss: 0.16099951923527617  |Validation err: 0.5393034825870647 , Validation loss:1.768407867068336 \n",
            "Epoch 9: Train err: 0.009, Train loss: 0.08974331684727618  |Validation err: 0.5194029850746269 , Validation loss:1.7939876090912592 \n",
            "Epoch 10: Train err: 0.002, Train loss: 0.04579854225541683  |Validation err: 0.5238805970149254 , Validation loss:1.8559010009917 \n",
            "Epoch 11: Train err: 0.0021666666666666666, Train loss: 0.042655417537118524  |Validation err: 0.5288557213930348 , Validation loss:1.8902773705739824 \n",
            "Epoch 12: Train err: 0.0018333333333333333, Train loss: 0.03438615872267079  |Validation err: 0.5263681592039801 , Validation loss:1.917941278881497 \n",
            "Epoch 13: Train err: 0.0013333333333333333, Train loss: 0.025112972574982236  |Validation err: 0.5129353233830846 , Validation loss:1.8790982545368256 \n",
            "Epoch 14: Train err: 0.0, Train loss: 0.016388565836910236  |Validation err: 0.5144278606965174 , Validation loss:1.8730406619253612 \n",
            "Epoch 15: Train err: 0.0006666666666666666, Train loss: 0.016425232779472432  |Validation err: 0.5169154228855721 , Validation loss:1.93165213531918 \n",
            "Epoch 16: Train err: 0.0, Train loss: 0.010871493574628171  |Validation err: 0.5134328358208955 , Validation loss:1.918090504313272 \n",
            "Epoch 17: Train err: 0.0, Train loss: 0.008428482933247343  |Validation err: 0.5084577114427861 , Validation loss:1.928566578834776 \n",
            "Epoch 18: Train err: 0.0, Train loss: 0.00684706230172293  |Validation err: 0.5134328358208955 , Validation loss:1.9600126894693526 \n",
            "Epoch 19: Train err: 0.0, Train loss: 0.00607967551084275  |Validation err: 0.5139303482587064 , Validation loss:1.9592952501206171 \n",
            "Epoch 20: Train err: 0.0, Train loss: 0.006042983937770762  |Validation err: 0.5208955223880597 , Validation loss:1.9729985604210505 \n",
            "Finished Training\n",
            "Total time elapsed: 2106.28 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "aioBCbl1wG-9",
        "colab_type": "code",
        "outputId": "f445cc9e-323d-4412-ba70-907d5a9cb0b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "cell_type": "code",
      "source": [
        "res18net = resnet18(num_classes=10)\n",
        "res18net.to(device)\n",
        "train(res18net, batch_size = 16, learning_rate = 0.00001, num_epochs = 20)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1: Train err: 0.8006666666666666, Train loss: 2.184048390134852  |Validation err: 0.6970149253731344 , Validation loss:2.0182409343265353 \n",
            "Epoch 2: Train err: 0.6726666666666666, Train loss: 1.9546241646117353  |Validation err: 0.6427860696517413 , Validation loss:1.8825933914335946 \n",
            "Epoch 3: Train err: 0.6145, Train loss: 1.8165169028525656  |Validation err: 0.6303482587064677 , Validation loss:1.8197147070415436 \n",
            "Epoch 4: Train err: 0.5763333333333334, Train loss: 1.708015207280504  |Validation err: 0.5965174129353233 , Validation loss:1.7441735721769787 \n",
            "Epoch 5: Train err: 0.5291666666666667, Train loss: 1.6083150721610862  |Validation err: 0.5741293532338309 , Validation loss:1.7038453079405285 \n",
            "Epoch 6: Train err: 0.5006666666666667, Train loss: 1.5287060737609863  |Validation err: 0.5502487562189055 , Validation loss:1.6645165606150552 \n",
            "Epoch 7: Train err: 0.4706666666666667, Train loss: 1.4452538414204374  |Validation err: 0.5597014925373134 , Validation loss:1.6417683627870348 \n",
            "Epoch 8: Train err: 0.44166666666666665, Train loss: 1.3632465448785336  |Validation err: 0.5497512437810945 , Validation loss:1.6167049275504217 \n",
            "Epoch 9: Train err: 0.4083333333333333, Train loss: 1.2710836409254278  |Validation err: 0.5258706467661691 , Validation loss:1.5699507433270652 \n",
            "Epoch 10: Train err: 0.37416666666666665, Train loss: 1.1852110877950142  |Validation err: 0.5208955223880597 , Validation loss:1.553950245418246 \n",
            "Epoch 11: Train err: 0.3436666666666667, Train loss: 1.1118180840573413  |Validation err: 0.5343283582089552 , Validation loss:1.54874372860742 \n",
            "Epoch 12: Train err: 0.30883333333333335, Train loss: 1.0195685162189159  |Validation err: 0.5039800995024876 , Validation loss:1.5254985964487469 \n",
            "Epoch 13: Train err: 0.2693333333333333, Train loss: 0.9320272086782658  |Validation err: 0.5004975124378109 , Validation loss:1.523388662035503 \n",
            "Epoch 14: Train err: 0.22983333333333333, Train loss: 0.837093268303161  |Validation err: 0.5094527363184079 , Validation loss:1.5237777384500655 \n",
            "Epoch 15: Train err: 0.19733333333333333, Train loss: 0.746426561411391  |Validation err: 0.5243781094527363 , Validation loss:1.5791839455801344 \n",
            "Epoch 16: Train err: 0.15733333333333333, Train loss: 0.6612972731285907  |Validation err: 0.5094527363184079 , Validation loss:1.5389964826523312 \n",
            "Epoch 17: Train err: 0.12216666666666667, Train loss: 0.5768866757763192  |Validation err: 0.5159203980099503 , Validation loss:1.5397961574887473 \n",
            "Epoch 18: Train err: 0.09316666666666666, Train loss: 0.4963217751776918  |Validation err: 0.5034825870646766 , Validation loss:1.560954589692373 \n",
            "Epoch 19: Train err: 0.07216666666666667, Train loss: 0.4240840239093659  |Validation err: 0.5079601990049751 , Validation loss:1.5722493046805972 \n",
            "Epoch 20: Train err: 0.052833333333333336, Train loss: 0.3640622211263535  |Validation err: 0.517910447761194 , Validation loss:1.620893987398299 \n",
            "Finished Training\n",
            "Total time elapsed: 2092.06 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "eXaM2et4A01m",
        "colab_type": "code",
        "outputId": "a07d4178-bc10-4c50-9039-b734817b0d10",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "cell_type": "code",
      "source": [
        "res18net = resnet18(num_classes=10)\n",
        "res18net.to(device)\n",
        "train(res18net, batch_size = 16, learning_rate = 0.0001, num_epochs = 20)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1: Train err: 0.6645, Train loss: 1.9102689243377524  |Validation err: 0.5895522388059702 , Validation loss:1.7171572617122106 \n",
            "Epoch 2: Train err: 0.5263333333333333, Train loss: 1.5602751602517797  |Validation err: 0.563681592039801 , Validation loss:1.6372319005784535 \n",
            "Epoch 3: Train err: 0.4445, Train loss: 1.3073043455468847  |Validation err: 0.5159203980099503 , Validation loss:1.5392789642016094 \n",
            "Epoch 4: Train err: 0.32166666666666666, Train loss: 0.9735555934145096  |Validation err: 0.5184079601990049 , Validation loss:1.5942716257912772 \n",
            "Epoch 5: Train err: 0.15866666666666668, Train loss: 0.5589618502145118  |Validation err: 0.5024875621890548 , Validation loss:1.6181088212936643 \n",
            "Epoch 6: Train err: 0.0555, Train loss: 0.25397126820493254  |Validation err: 0.5373134328358209 , Validation loss:1.772815401591952 \n",
            "Epoch 7: Train err: 0.017833333333333333, Train loss: 0.10649809312630207  |Validation err: 0.5174129353233831 , Validation loss:1.8311938586689176 \n",
            "Epoch 8: Train err: 0.008, Train loss: 0.05945683195394404  |Validation err: 0.5323383084577115 , Validation loss:1.9734253656296503 \n",
            "Epoch 9: Train err: 0.0011666666666666668, Train loss: 0.025444492519377394  |Validation err: 0.5124378109452736 , Validation loss:1.9187160597907171 \n",
            "Epoch 10: Train err: 0.0, Train loss: 0.01111113861639132  |Validation err: 0.5208955223880597 , Validation loss:1.9371342185943845 \n",
            "Epoch 11: Train err: 0.00016666666666666666, Train loss: 0.010091459487584676  |Validation err: 0.545771144278607 , Validation loss:1.998337355871049 \n",
            "Epoch 12: Train err: 0.0, Train loss: 0.006069513632262007  |Validation err: 0.5119402985074627 , Validation loss:1.9681518777968392 \n",
            "Epoch 13: Train err: 0.0, Train loss: 0.004643066023337714  |Validation err: 0.5054726368159204 , Validation loss:1.9323464859099615 \n",
            "Epoch 14: Train err: 0.00016666666666666666, Train loss: 0.0044864871727421565  |Validation err: 0.5169154228855721 , Validation loss:2.0385894283415777 \n",
            "Epoch 15: Train err: 0.0, Train loss: 0.003770281610257448  |Validation err: 0.5054726368159204 , Validation loss:2.0141857048821827 \n",
            "Epoch 16: Train err: 0.0, Train loss: 0.0032992566678118197  |Validation err: 0.5104477611940299 , Validation loss:2.0460308279309953 \n",
            "Epoch 17: Train err: 0.0, Train loss: 0.0026902059722255837  |Validation err: 0.5034825870646766 , Validation loss:2.0520342077527727 \n",
            "Epoch 18: Train err: 0.0, Train loss: 0.002377278256933502  |Validation err: 0.5029850746268657 , Validation loss:2.0705727887532066 \n",
            "Epoch 19: Train err: 0.0, Train loss: 0.002158531297414385  |Validation err: 0.49701492537313435 , Validation loss:2.032361492278084 \n",
            "Epoch 20: Train err: 0.0, Train loss: 0.0019512397913321695  |Validation err: 0.5069651741293533 , Validation loss:2.0751059887901184 \n",
            "Finished Training\n",
            "Total time elapsed: 2143.83 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}